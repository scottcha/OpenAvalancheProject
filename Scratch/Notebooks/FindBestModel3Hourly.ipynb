{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook demonstrating the use of Timeseriesai for generating predictions\n",
    "https://github.com/timeseriesAI/timeseriesAI\n",
    "\n",
    "## The below is done using the tutorial data set (one state, one season) and is a very small dataset so the results are only intended for use on how to use this, not necessarily what the ML capability is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### uses env timeseriesai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.all import *\n",
    "from joblib import Parallel, delayed\n",
    "from openavalancheproject.tsai_utilities import *\n",
    "import numpy as np\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_seed(42,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/media/scottcha/Data2/OAPMLData/'\n",
    "data_root2 = '/media/scottcha/E1/Data/OAPMLData/'\n",
    "\n",
    "ml_path = data_root + '/5.MLData/'\n",
    "num_features = 774\n",
    "interpolation = 1\n",
    "label = 'Day1DangerAboveTreeline'\n",
    "#file_label = 'ca_co_day1above_3h_large_rechunked'\n",
    "days = '84d'\n",
    "file_label = 'ca_co_day1above_3h_large'\n",
    "#file_label = 'day1above'\n",
    "#label = 'Day1DangerAboveTreeline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = zarr.open(ml_path + 'X_' + file_label + '.zarr', mode='r')\n",
    "#X2 = zarr.open(ml_path + 'co_ca_above.zarr', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the full X datafile which has both Train and Test concated\n",
    "#X = np.load(ml_path + '/X_all_' + file_label + '.npy', mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = TSAIUtilities(X, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = pd.read_csv(ml_path + '/FeatureLabels_' + 'ca_co_day1above_3h_large' + '.csv')\n",
    "feature_names = feature_names['0'].sort_values().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_feature_list = utils.filter_features(set(feature_names), only_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.read_csv('TopFeatures.csv')['Feature'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_top_features = False\n",
    "if filter_top_features:\n",
    "    filtered_feature_list = pd.read_csv('TopFeatures.csv')['Feature'].values\n",
    "    #filtered_feature_list = [x for x in filtered_feature_list if x in pd.read_csv('TopFeatures.csv')['Feature'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_feature_indexes = [list(feature_names).index(x) for x in filtered_feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_feature_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_feature_indexes[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small_84 = X.get_orthogonal_selection((slice(None), filtered_feature_indexes, slice(1442-8*84, 1441)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_small_28 = X.get_orthogonal_selection((slice(None), filtered_feature_indexes, slice(1441-8*28, 1441)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small_84.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_small_84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature distribution, some are gausian many are not, we'll use normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "calc_train = False\n",
    "if calc_train == True:\n",
    "    feature_stds, feature_means = calc_std_and_mean(X, 0, 180000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "calc_test = False\n",
    "if calc_test == True:\n",
    "    feature_stds_test, feature_means_test = calc_std_and_mean(X, 180000, X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "calc_min_train = False\n",
    "if calc_min_train == True:\n",
    "    feature_mins, feature_maxs = utils.calc_min_max(X, 0, 180000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "calc_min_test = False\n",
    "if calc_min_test == True:\n",
    "    feature_mins_test, feature_maxs_test = calc_min_max(X, 180000, X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(feature_stds).to_csv(data_root2 +  'std_ca_co_day1above_3h_large_84d.csv', index=False)\n",
    "#pd.Series(feature_means).to_csv(data_root2 + 'mean_ca_co_day1above_3h_large_84d.csv', index=False)\n",
    "#pd.Series(feature_stds_test).to_csv(data_root2 +  'std_test_ca_co_day1above_3h_large_84d.csv', index=False)\n",
    "#pd.Series(feature_means_test).to_csv(data_root2 + 'mean_test_ca_co_day1above_3h_large_84d.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_stds = pd.read_csv(data_root2 + '/std_' + file_label + '_' + days + '.csv' )['0'].values\n",
    "#feature_means = pd.read_csv(data_root2 + '/mean_' + file_label + '_' + days + '.csv' )['0'].values\n",
    "#feature_stds_test = pd.read_csv(data_root2 + '/std_test_' + file_label + '_' + days + '.csv' )['0'].values\n",
    "#feature_means_test = pd.read_csv(data_root2 + '/mean_test_' + file_label + '_' + days + '.csv' )['0'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(feature_mins).to_csv(data_root2 +  'min_ca_co_day1above_3h_large_84d.csv', index=False)\n",
    "#pd.Series(feature_maxs).to_csv(data_root2 + 'max_ca_co_day1above_3h_large_84d.csv', index=False)\n",
    "#pd.Series(feature_mins_test).to_csv(data_root2 +  'min_test_ca_co_day1above_3h_large_84d.csv', index=False)\n",
    "#pd.Series(feature_maxs_test).to_csv(data_root2 + 'max_test_ca_co_day1above_3h_large_84d.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_mins = pd.read_csv(data_root2 + '/min_' + 'ca_co_day1above_3h_large' + '_' + days + '.csv' )['0'].values\n",
    "feature_maxs = pd.read_csv(data_root2 + '/max_' + 'ca_co_day1above_3h_large' + '_' + days + '.csv' )['0'].values\n",
    "\n",
    "#feature_mins_test = pd.read_csv(data_root2 + '/min_test_' + 'ca_co_day1above_3h_large' + '_' + days + '.csv' )['0'].values\n",
    "#feature_maxs_test = pd.read_csv(data_root2 + '/max_test_' + 'ca_co_day1above_3h_large' + '_' + days + '.csv' )['0'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the corresponding label files and concat them\n",
    "#can get the right values here based on the contents of hte ml_path directory\n",
    "\n",
    "#num_train_files = 80\n",
    "num_train_files = 60\n",
    "file_list = []\n",
    "for i in range(num_train_files):\n",
    "    file_list.append(pd.read_parquet(ml_path + '/y_train_batch_' + str(i) + '_' + file_label + '.parquet'))\n",
    "\n",
    "    \n",
    "num_test_files = 12\n",
    "#num_test_files = 4\n",
    "for i in range(num_test_files):\n",
    "    file_list.append(pd.read_parquet(ml_path + '/y_test_batch_' + str(i + num_train_files) + '_' + file_label + '.parquet'))\n",
    "    #file_list.append(pd.read_parquet(ml_path + '/y_test_batch_' + str(i) + '_' + file_label + '.parquet'))\n",
    "    \n",
    "y_df = pd.concat(file_list).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = TSAIUtilities(X, 'Day1DangerAboveTreeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.augment_labels_with_trends(data_root + 'CleanedForecastsNWAC_CAIC_UAC_CAC.V1.2013-2021.csv', y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = y_df['Day1DangerAboveTreeline'].unique()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = y_df[y_df['Day1DangerAboveTreeline'].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#augmenting lables removes some so we need to ensure X has the same index\n",
    "X = X[y_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df['Day1DangerAboveTreeline'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, cat_dict = utils.get_y_as_cat(y_df)\n",
    "cat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits2 = (L(list(y_df[y_df['season']!='20-21'].index.values)).shuffle(), L(list(y_df[y_df['season']=='20-21'].index.values)).shuffle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, dls = utils.create_dls(X, y, feature_mins=feature_mins[filtered_feature_indexes], feature_maxs=feature_maxs[filtered_feature_indexes], sample_frac=1, splits=splits2)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check both the train and test sets to ensure that there aren't full rows with all nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#num_workers:12, 7.5s\n",
    "#num_workers:6, 7.75s\n",
    "#3, 7.56s\n",
    "#1, 7.6\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.valid.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, dls = utils.create_dls(X, y, feature_mins=np.asarray(feature_mins[filtered_feature_indexes]), \n",
    "                                   feature_maxs=np.asarray(feature_maxs[filtered_feature_indexes]), \n",
    "                                   splits=splits, sample_frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trial 183 finished with value: 0.6043333333333333 and parameters: \n",
    "#{'model': 'LSTM', 'hidden_size': 100, 'lstm_n_layers': 1, 'rnn_dropout': 0.8, \n",
    "#'bidirectional': True, 'lstm_fc_dropout': 0.2, 'lstm_cell_type': 'LSTM', \n",
    "#'loss_func': 'None', 'learning_rate': 0.0076197040198174755, 'learner_callback': 'None', 'optimizer': 'AdamWD'}\n",
    "model = LSTMPlus(dls.vars, dls.c, hidden_size=100, n_layers=1, rnn_dropout=.8, bidirectional=True, \n",
    "                 fc_dropout=.2)\n",
    "model.load_state_dict(torch.load(\"./models/\" + 'lstm' + '.pth'))\n",
    "learn = Learner(dls, model, loss_func=None, opt_func=partial(Adam, decouple_wd=True),  metrics=accuracy, cbs=None)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'model': 'LSTM', 'hidden_size': 150, 'lstm_n_layers': 3, \n",
    "#'rnn_dropout': 0.8, 'bidirectional': False, 'lstm_fc_dropout': 0.6000000000000001, \n",
    "#'lstm_cell_type': 'GRU', 'loss_func': 'None', 'learning_rate': 0.001038746221962459,\n",
    "#'learner_callback': 'CutMix1d', 'optimizer': 'AdamWD'}\n",
    "model = GRUPlus(dls.vars, dls.c, hidden_size=150, n_layers=3, rnn_dropout=.8, bidirectional=False, \n",
    "                fc_dropout=.6)\n",
    "model.load_state_dict(torch.load(\"./models/\" + 'gru' + '.pth'))\n",
    "learn = Learner(dls, model, loss_func=None, opt_func=partial(Adam, decouple_wd=True),  metrics=accuracy, cbs=CutMix1d())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, lr=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(200, lr=.001, cbs=[EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=5), SaveModelCallback(monitor='accuracy', comp=np.greater, min_delta=.001, fname='lstm')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "conf_matrix = interp.plot_confusion_matrix(normalize=True)\n",
    "conf_matrix = interp.plot_confusion_matrix(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "conf_matrix = interp.plot_confusion_matrix(normalize=True)\n",
    "conf_matrix = interp.plot_confusion_matrix(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[splits[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??learn.feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./models/\" + 'gru' + '.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[splits[1]].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_names[filtered_feature_indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_names = feature_names[filtered_feature_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_importance = learn.feature_importance(X[splits[1]], y[splits[1]], feature_names=list(feature_names[filtered_feature_indexes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance[-40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance[df_importance['accuracy_change'] > 0].to_csv('TopFeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = InceptionTimePlus(dls.vars, dls.c)\n",
    "learn = Learner(dls, model, metrics=[accuracy, BalancedAccuracy()], loss_func=None, \n",
    "                cbs=[ShowGraphCallback2(), \n",
    "                     PredictionDynamics(), \n",
    "                     SaveModelCallback(monitor='accuracy', comp=np.greater, min_delta=.001, fname='default'), \n",
    "                     EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=5)]) \n",
    "#learn.lr_find()\n",
    "learn.fit(50, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, dls = utils.create_dls(X, y, feature_mins=np.asarray(feature_mins[filtered_feature_indexes]), \n",
    "                                   feature_maxs=np.asarray(feature_maxs[filtered_feature_indexes]), \n",
    "                                   splits=splits, sample_frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#'model': 'InceptionTimePlus', 'itp_num_filters': 64, 'itp_depth': 6, 'itp_dropout_rate': 0.2, 'itp_ks': 50, 'loss_func': 'LabelSmoothingCrossEntropyFlat', 'learning_rate': 0.00010360777693535245, 'learner_callback': 'IntraClassCutMix1d', 'optimizer': 'AdamNoWD'\n",
    "model = InceptionTimePlus(dls.vars, dls.c, nf=64, depth=6, ks=50, conv_dropout=.2)\n",
    "learn = Learner(dls, model, metrics=[accuracy, BalancedAccuracy()], loss_func=LabelSmoothingCrossEntropyFlat(), \n",
    "                opt_func=partial(Adam, decouple_wd=False), \n",
    "                cbs=[ShowGraphCallback2(), \n",
    "                     PredictionDynamics(),\n",
    "                     CutMix1d(),\n",
    "                     SaveModelCallback(monitor='accuracy', comp=np.greater, min_delta=.001, fname='tuned'), \n",
    "                     EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=5)]) \n",
    "learn.fit_one_cycle(50, lr_max=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#above here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = InceptionTimePlus(dls.vars, dls.c)\n",
    "learn = Learner(dls, model, metrics=[accuracy, BalancedAccuracy()], loss_func=None, cbs=[ShowGraphCallback2(), PredictionDynamics()]) \n",
    "learn.fit(20, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionTimePlus(dls.vars, dls.c, depth=8)\n",
    "learn = Learner(dls, model, metrics=[accuracy, BalancedAccuracy()], loss_func=None, cbs=[ShowGraphCallback2(), PredictionDynamics()]) \n",
    "learn.fit(20, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionTimePlus(dls.vars, dls.c, depth=12)\n",
    "learn = Learner(dls, model, metrics=[accuracy, BalancedAccuracy()], loss_func=None, cbs=[ShowGraphCallback2(), PredictionDynamics()]) \n",
    "learn.fit(20, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionTimePlus(dls.vars, dls.c, depth=3)\n",
    "learn = Learner(dls, model, metrics=[accuracy, BalancedAccuracy()], loss_func=None, cbs=[ShowGraphCallback2(), PredictionDynamics()]) \n",
    "learn.fit(20, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionTimePlus(dls.vars, dls.c, depth=1)\n",
    "learn = Learner(dls, model, metrics=[accuracy, BalancedAccuracy()], loss_func=None, cbs=[ShowGraphCallback2(), PredictionDynamics()]) \n",
    "learn.fit(40, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionTimePlus(dls.vars, dls.c, depth=2)\n",
    "learn = Learner(dls, model, metrics=[accuracy, BalancedAccuracy()], loss_func=None, cbs=[ShowGraphCallback2(), PredictionDynamics()]) \n",
    "learn.fit(40, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionTimePlus(dls.vars, dls.c, depth=2)\n",
    "learn = Learner(dls, model, metrics=[accuracy, BalancedAccuracy()], loss_func=None, opt_func = partial(Adam, decouple_wd=False), cbs=[ShowGraphCallback2(), PredictionDynamics()]) \n",
    "learn.fit(40, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionTimePlus(dls.vars, dls.c, depth=6)\n",
    "learn = Learner(dls, model, metrics=[accuracy, BalancedAccuracy()], loss_func=None, opt_func = partial(Adam, decouple_wd=False), cbs=[ShowGraphCallback2(), PredictionDynamics()]) \n",
    "learn.fit(100, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionTimePlus(dls.vars, dls.c, depth=6)\n",
    "learn = Learner(dls, model, metrics=[accuracy, BalancedAccuracy()], loss_func=None, opt_func = partial(Adam, decouple_wd=False, eps=.001), cbs=[ShowGraphCallback2(), PredictionDynamics()]) \n",
    "learn.fit(100, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionTimePlus(dls.vars, dls.c, depth=6)\n",
    "learn = Learner(dls, model, metrics=[accuracy, BalancedAccuracy()], loss_func=None, opt_func = partial(Adam, decouple_wd=False), cbs=[ShowGraphCallback2(), PredictionDynamics()]) \n",
    "learn.fit(100, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets try and train a model, in the tutorial case we are using an extremely small dataset so no conclusions should be drawn from these results they are for illustrative purposes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top values from model zoo\n",
    "#Trial 289 finished with value: 0.617 and parameters: {'num_days': 28, 'batch_size': 8, 'model': 'InceptionTimePlus', 'itp_num_filters': 32, 'itp_depth': 6, 'itp_dropout_rate': 0.0, 'loss_func': 'LabelSmoothingCrossEntropyFlat', 'learning_rate': 0.0024792334740007253}\n",
    "\n",
    "#Trial 454 finished with value: 0.616 and parameters: {'num_days': 28, 'batch_size': 8, 'model': 'InceptionTimePlus', 'itp_num_filters': 32, 'itp_depth': 9, 'itp_dropout_rate': 0.0, 'loss_func': 'None', 'learning_rate': 0.0024792334740007253}\n",
    "\n",
    "#Trial 161 finished with value: 0.609 and parameters: {'num_days': 84, 'batch_size': 8, 'model': 'InceptionTimePlus', 'itp_num_filters': 32, 'itp_depth': 9, 'itp_dropout_rate': 0.0, 'loss_func': 'None', 'learning_rate': 0.0006723398746247245}\n",
    "\n",
    "#Trial 26 finished with value: 0.611 and parameters: {'num_days': 84, 'batch_size': 8, 'model': 'InceptionTimePlus', 'itp_num_filters': 32, 'itp_depth': 6, 'itp_dropout_rate': 0.2, 'loss_func': 'None', 'learning_rate': 1.339568695002037e-05}\n",
    "\n",
    "#Trial 352 finished with value: 0.604 and parameters: {'num_days': 28, 'batch_size': 64, 'model': 'InceptionTimePlus', 'itp_num_filters': 64, 'itp_depth': 6, 'itp_dropout_rate': 0.0, 'loss_func': 'None', 'learning_rate': 0.0024792334740007253}\n",
    "\n",
    "#good learning curve\n",
    "#Trial 268 finished with value: 0.565 and parameters: {'num_days': 28, 'batch_size': 8, 'model': 'TCN', 'TCN_fc_dropout': 0.6000000000000001, 'TCN_conv_dropout': 0.0, 'loss_func': 'LabelSmoothingCrossEntropy', 'learning_rate': 1.339568695002037e-05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionTimePlus(dls.vars, dls.c,  conv_dropout=.2, depth=6, nf=32)\n",
    "learn = Learner(dls, model, metrics=[accuracy, BalancedAccuracy()], loss_func=LabelSmoothingCrossEntropyFlat(), cbs=[ShowGraphCallback2(), PredictionDynamics(),SaveModelCallback(monitor='accuracy', comp=np.greater, min_delta=.001)] )#.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.fit(200, lr=1e-5, cbs=EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionTimePlus(dls.vars, dls.c,  conv_dropout=.2, depth=6, nf=32)\n",
    "learn = Learner(dls, model, metrics=[accuracy, BalancedAccuracy()], loss_func=LabelSmoothingCrossEntropyFlat(), cbs=[ShowGraphCallback2(), PredictionDynamics(),SaveModelCallback(monitor='accuracy', comp=np.greater, min_delta=.001), CutMix1D()] )#.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(200, lr=1e-5, cbs=EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionTimePlus(dls.vars, dls.c,  conv_dropout=.2, depth=6, nf=32)\n",
    "learn = Learner(dls, model, metrics=[accuracy, BalancedAccuracy()], loss_func=LabelSmoothingCrossEntropyFlat(), cbs=[ShowGraphCallback2(), PredictionDynamics(),SaveModelCallback(monitor='accuracy', comp=np.greater, min_delta=.001), CutMix1D()] ).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(200, lr=1e-5, cbs=EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TCN(dls.vars, dls.c, conv_dropout=0, fc_dropout=.6)\n",
    "learn = Learner(dls, model, metrics=[accuracy, BalancedAccuracy()], loss_func=LabelSmoothingCrossEntropyFlat(), cbs=[ShowGraphCallback2(), PredictionDynamics(),SaveModelCallback(monitor='accuracy', comp=np.greater, min_delta=.001), CutMix1D()] )#.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(200, lr=1e-5, cbs=EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, dls = create_dls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'num_days': 112, 'batch_size': 8, 'model': 'XCM', 'window_perc': 0.2, 'xcm_nf': 192, 'xcm_fc_dropout': 0.8, 'loss_func': 'LabelSmoothingCrossEntropyFlat', 'learning_rate': 0.0002725629442830844}  \n",
    "model = XCM(dls.vars, dls.c, dls.len, window_perc=.2, fc_dropout=.8, nf=192)\n",
    "learn = Learner(dls, model, metrics=accuracy, loss_func=LabelSmoothingCrossEntropyFlat(), cbs=[ShowGraphCallback2(), SaveModelCallback(monitor='accuracy', comp=np.greater, min_delta=.001)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??LabelSmoothingCrossEntropyFlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?TST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try TST\n",
    "#{'d_ff': 512, 'd_k': 64, 'd_model': 128, 'd_v': 64, 'dropout': 0.7, 'fc_dropout': 0.1, 'loss': 'LabelSmoothingCrossEntropyFlat', 'n_heads': 16, 'n_layers': 4}\n",
    "model = TSTPlus(dls.vars, dls.c, dls.len, d_ff=512, d_k=64, d_model=128, d_v=64, dropout=.7, fc_dropout=.1, n_heads=16, n_layers=4)\n",
    "learn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropyFlat(), metrics=[accuracy, BalancedAccuracy()],  cbs=[ShowGraphCallback2(), PredictionDynamics(), SaveModelCallback(monitor='accuracy', comp=np.greater, min_delta=.001)]).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiTSTPlus(dls.vars, dls.c, dls.len)\n",
    "learn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropyFlat(), metrics=accuracy,  cbs=ShowGraphCallback2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try rocket\n",
    "model = build_ts_model(ROCKET, dls=dls) \n",
    "X_train, y_train = create_rocket_features(dls.train, model)\n",
    "X_valid, y_valid = create_rocket_features(dls.valid, model)\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "ridge = RidgeClassifierCV(alphas=np.logspace(-8, 8, 17), normalize=True)\n",
    "ridge.fit(X_train, y_train)\n",
    "print(f'alpha: {ridge.alpha_:.2E}  train: {ridge.score(X_train, y_train):.5f}  valid: {ridge.score(X_valid, y_valid):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = InceptionTimePlus(dls.vars, dls.c)\n",
    "model = InceptionTimePlus(dls.vars, dls.c, bottleneck=True, coord=False, ks=5, conv_dropout=.8, depth=6, nf=32)\n",
    "learn = Learner(dls, model, metrics=accuracy, loss_func=LabelSmoothingCrossEntropy(), cbs=ShowGraphCallback2()).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XCM\n",
    "learn.fit(200, lr=1e-2, cbs=EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TSTPlus\n",
    "learn.fit(20, lr=1e-4, cbs=EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TST\n",
    "learn.fit(20, lr=1e-4, cbs=EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(200, lr_max=1e-4, cbs=EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A100\n",
    "learn.fit_one_cycle(25, lr_max=1e-3, cbs=EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fp32\n",
    "learn.fit_one_cycle(25, lr_max=1e-3, cbs=EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(splits_2[1].sorted())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.feature_importance(X.get_orthogonal_selection((list(splits_2[1].sorted()), slice(None), slice(None))), list(y[splits_2[1].sorted()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??learn.feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(feature_names)[923]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners = {}\n",
    "#learners['ITP'] = []\n",
    "learners['TST'] = []\n",
    "#learners['XCM'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./models/\" + 'model' + '.pth'))\n",
    "learn = Learner(dls, model, metrics=accuracy, loss_func=LabelSmoothingCrossEntropyFlat(), cbs=[ShowGraphCallback2(), SaveModelCallback(monitor='accuracy', comp=np.greater, min_delta=.001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, dls = utils.create_dls(X, y, feature_mins=feature_mins[filtered_feature_indexes], feature_maxs=feature_maxs[filtered_feature_indexes], sample_frac=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionTimePlus(dls.vars, dls.c)#, nf=64, depth=6, ks=50)\n",
    "from tsai.inference import load_learner\n",
    "model.load_state_dict(torch.load(\"./models/\" + 'default' + '.pth'))\n",
    "#learn = Learner(dls, model, metrics=[accuracy, BalancedAccuracy()], loss_func=LabelSmoothingCrossEntropyFlat(), cbs=[ShowGraphCallback2(), PredictionDynamics(),SaveModelCallback(monitor='accuracy', comp=np.greater, min_delta=.001)] )#.to_fp16()\n",
    "learn = Learner(dls, model, metrics=[accuracy, BalancedAccuracy()], loss_func=None, \n",
    "                cbs=[ShowGraphCallback2(), \n",
    "                     PredictionDynamics(), \n",
    "                     SaveModelCallback(monitor='accuracy', comp=np.greater, min_delta=.001, fname='default'), \n",
    "                     EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=5)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas, target, preds = learn.get_preds(dl=dls.valid, with_input=False, with_loss=False, with_decoded=True, act=None, reorder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skm.accuracy_score(target, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d,t = flatten_check(interp.decoded, interp.targs)\n",
    "#print(str(skm.accuracy_score(t, d)))\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "conf_matrix = interp.plot_confusion_matrix(normalize=False)\n",
    "conf_matrix = interp.plot_confusion_matrix(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.most_confused()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(abs(target-preds) >= 9)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.iloc[np.where(abs(target-preds) == 10)[0]]['parsed_date'].dt.month.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid mismatches\n",
    "y_df.iloc[splits[1][np.where(abs(target-preds) == 10)[0]]][['Day1Date', 'UnifiedRegion']].groupby(['Day1Date', 'UnifiedRegion']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train mismatches\n",
    "pd.set_option('display.max_rows', 1500)\n",
    "y_df.iloc[splits[1][np.where(abs(target-preds) == 3)[0]]][['Day1Date', 'UnifiedRegion']].groupby(['Day1Date','UnifiedRegion']).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.iloc[splits[1][np.where(abs(target-preds) == 3)[0]]]['Day1Date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = y_df[ y_df['Day1Date'] == 20201217.0 ]\n",
    "t = t[t['UnifiedRegion'] == 'Northwest Coastal']\n",
    "t.sort_values(['latitude', 'longitude'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = y_df[ y_df['Day1Date'] == 20201218.0 ]\n",
    "t = t[t['UnifiedRegion'] == 'Northwest Coastal']\n",
    "t.sort_values(['latitude', 'longitude'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(x, x2=None):\n",
    "    for i in range(x.shape[0]):\n",
    "        print(filtered_feature_list[i])\n",
    "        pd.Series(x[i]).plot()\n",
    "        plt.show()\n",
    "        if x2 is not None:\n",
    "            pd.Series(x2[i]).plot()\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_features(X[210237], X[189293])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[210237][list(filtered_feature_list).index('SNOD_surface_avg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = y_df[ y_df['Day1Date'] == 20201210.0 ]\n",
    "t = t[t['UnifiedRegion'] == 'Purcells']\n",
    "t['Day1DangerAboveTreeline'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "180000-179968"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(abs(target-preds) == 3)[0][8:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.iloc[179944]['Day1DangerAboveTreeline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_2[0][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1500)\n",
    "y_df.iloc[splits[0][48]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#predict high when forecast is low \n",
    "for i in np.where(abs(target-preds) >= 3)[0][0:8]:\n",
    "    print(i)\n",
    "    print('split ' + str(splits[1][i]))\n",
    "    print('y ' + str(y[splits[1][i]]))\n",
    "    print(y_df.iloc[splits[1][i]]['Day1DangerAboveTreeline'])\n",
    "    pd.DataFrame(X[splits[1]][i]).T.plot(legend=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#sample of correct preds\n",
    "for i in np.where(abs(target-preds) == 0)[0][:8]:\n",
    "    print(i)\n",
    "    print('split ' + str(splits[1][i]))\n",
    "    print('y ' + str(y[splits[1][i]]))\n",
    "    print(y_df.iloc[splits[1][i]]['Day1DangerAboveTreeline'])\n",
    "    pd.DataFrame(X[splits[1]][i]).T.plot(legend=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0,19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[187699].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(filtered_feature_indexes)):\n",
    "    \n",
    "    low = i\n",
    "    high = i+1\n",
    "               \n",
    "    print('Incorrect Low Pred')\n",
    "\n",
    "    pd.DataFrame(X[187699][low:high].T, columns=filtered_feature_list[low:high]).plot(legend=False)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "\n",
    "    print('Correct Low Pred')\n",
    "    pd.DataFrame(X[202437][low:high].T, columns=filtered_feature_list[low:high]).plot(legend=False)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X[1001][low:high].T, columns=feature_names[filtered_feature_indexes][low:high]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_num = 0\n",
    "for i in range(X.shape[0]):\n",
    "    x = np.copy(X[i][low:high])\n",
    "    x = np.nan_to_num(x)\n",
    "    np.mod(x, 1, out=x)\n",
    "    mask = (x == 0)\n",
    "    if np.all(mask):\n",
    "        incorrect_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X[144769][low:high].T, columns=feature_names[filtered_feature_indexes][low:high]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X[0][low:high].T, columns=feature_names[filtered_feature_indexes][low:high])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_feature_names = feature_names[filtered_feature_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?learn.feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tmp_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.feature_importance(X[:256], y[:256])\n",
    "#learn.feature_importance(X[np.where(abs(target-preds) >= 9)], y[np.where(abs(target-preds) >= 9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_feature_names[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.feature_importance(X[np.where(abs(target-preds) == 0)[0][:1000]], y[np.where(abs(target-preds) == 0)[0][:1000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = [31, 42, 20, 1, 19, 51, 36, 24, 18, 4, 5, 32, 53, 16, 12, 26, 30, 17, 0, 10, 48, 55, 23, 29, 8, 14, 35, 9, 43, 7, 3, 52, 33]\n",
    "pd.Series(feature_names[filtered_feature_indexes][top_features]).to_csv('top_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most important features for incorrect\n",
    "tmp_feature_names[[13, 42, 1, 20, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most important features for correct\n",
    "tmp_feature_names[[13, 42, 20, 1, 19]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??learn.feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y[np.where(abs(target-preds) == 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_names[filtered_feature_indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[np.where(abs(target-preds) == 3)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.iloc[splits_2[1][1]]['Day1DangerAboveTreeline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.targs - np.argmax(interp.preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.iloc[splits_2[1]]['Day1DangerAboveTreeline'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d,t = flatten_check(interp.decoded, interp.targs)\n",
    "#print(str(skm.accuracy_score(t, d)))\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "conf_matrix = interp.plot_confusion_matrix(normalize=False)\n",
    "conf_matrix = interp.plot_confusion_matrix(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top values from model zoo\n",
    "#ITP1 Trial 289 finished with value: 0.617 and parameters: {'num_days': 28, 'batch_size': 8, 'model': 'InceptionTimePlus', 'itp_num_filters': 32, 'itp_depth': 6, 'itp_dropout_rate': 0.0, 'loss_func': 'LabelSmoothingCrossEntropyFlat', 'learning_rate': 0.0024792334740007253}\n",
    "\n",
    "#ITP2 Trial 454 finished with value: 0.616 and parameters: {'num_days': 28, 'batch_size': 8, 'model': 'InceptionTimePlus', 'itp_num_filters': 32, 'itp_depth': 9, 'itp_dropout_rate': 0.0, 'loss_func': 'None', 'learning_rate': 0.0024792334740007253}\n",
    "\n",
    "#ITP4 Trial 161 finished with value: 0.609 and parameters: {'num_days': 84, 'batch_size': 8, 'model': 'InceptionTimePlus', 'itp_num_filters': 32, 'itp_depth': 9, 'itp_dropout_rate': 0.0, 'loss_func': 'None', 'learning_rate': 0.0006723398746247245}\n",
    "\n",
    "#ITP5 Trial 26 finished with value: 0.611 and parameters: {'num_days': 84, 'batch_size': 8, 'model': 'InceptionTimePlus', 'itp_num_filters': 32, 'itp_depth': 6, 'itp_dropout_rate': 0.2, 'loss_func': 'None', 'learning_rate': 1.339568695002037e-05}\n",
    "\n",
    "#ITP3 Trial 352 finished with value: 0.604 and parameters: {'num_days': 28, 'batch_size': 64, 'model': 'InceptionTimePlus', 'itp_num_filters': 64, 'itp_depth': 6, 'itp_dropout_rate': 0.0, 'loss_func': 'None', 'learning_rate': 0.0024792334740007253}\n",
    "\n",
    "#good learning curve\n",
    "#TCN 1 Trial 268 finished with value: 0.565 and parameters: {'num_days': 28, 'batch_size': 8, 'model': 'TCN', 'TCN_fc_dropout': 0.6000000000000001, 'TCN_conv_dropout': 0.0, 'loss_func': 'LabelSmoothingCrossEntropy', 'learning_rate': 1.339568695002037e-05}\n",
    "learners = {'ITP1': [], 'ITP2': [], 'ITP3': [], 'ITP4': [], 'ITP5': [], 'TCN1': []}\n",
    "def get_model_and_lr(model_name, splits=None):\n",
    "    model = None\n",
    "    lr = 1e-3\n",
    "    days = 28\n",
    "    dls = None\n",
    "    if k == 'XCM':\n",
    "        lr = 1e-3\n",
    "        splits, dls = create_dls(filtered_feature_indexes, days, splits=splits)\n",
    "        model = XCM(dls.vars, dls.c, dls.len, fc_dropout=0, window_perc=.5, nf=128)\n",
    "    elif k == 'TST':\n",
    "        lr = 1e-4\n",
    "        splits, dls = create_dls(filtered_feature_indexes, days, splits=splits)\n",
    "        model = TST(dls.vars, dls.c, dls.len, d_ff=512, d_k=64, d_model=128, d_v=64, dropout=.7, fc_dropout=.1, n_heads=16, n_layers=4)\n",
    "    elif k == 'ITP':\n",
    "        lr = 1e-3\n",
    "        splits, dls = create_dls(filtered_feature_indexes, days, splits=splits)\n",
    "        model = InceptionTimePlus(dls.vars, dls.c, bottleneck=True, coord=False, ks=5, conv_dropout=.8, depth=6, nf=32)\n",
    "    elif k == 'ITP1':\n",
    "        lr = 0.0024\n",
    "        days = 28\n",
    "        X = X_small_28\n",
    "        splits, dls = create_dls(filtered_feature_indexes, days, splits=splits)\n",
    "        model = InceptionTimePlus(dls.vars, dls.c,  conv_dropout=0, depth=6, nf=32)\n",
    "    elif k == 'ITP2':\n",
    "        lr = 0.0024\n",
    "        days = 28\n",
    "        X = X_small_28\n",
    "        splits, dls = create_dls(filtered_feature_indexes, days, splits=splits)\n",
    "        model = InceptionTimePlus(dls.vars, dls.c,  conv_dropout=0, depth=9, nf=32)\n",
    "    elif k == 'ITP3':\n",
    "        lr = 0.0024\n",
    "        days = 28\n",
    "        X = X_small_28\n",
    "        splits, dls = create_dls(filtered_feature_indexes, days, splits=splits)\n",
    "        model = InceptionTimePlus(dls.vars, dls.c,  conv_dropout=0, depth=6, nf=64)\n",
    "    elif k == 'ITP4':\n",
    "        lr = 0.00067\n",
    "        days = 84\n",
    "        X = X_small_84\n",
    "        splits, dls = create_dls(filtered_feature_indexes, days, splits=splits)\n",
    "        model = InceptionTimePlus(dls.vars, dls.c,  conv_dropout=0, depth=9, nf=32)\n",
    "    elif k == 'ITP5':\n",
    "        lr = 1e-5\n",
    "        days = 84\n",
    "        X = X_small_84\n",
    "        splits, dls = create_dls(filtered_feature_indexes, days, splits=splits)\n",
    "        model = InceptionTimePlus(dls.vars, dls.c,  conv_dropout=.2, depth=6, nf=32)\n",
    "    elif k == 'TCN1':\n",
    "        lr = 1e-5\n",
    "        days = 28\n",
    "        X = X_small_28\n",
    "        splits, dls = create_dls(filtered_feature_indexes, days, splits=splits)\n",
    "        model = TCN(dls.vars, dls.c, conv_dropout=0, fc_dropout=.6)\n",
    "    return model, lr, days, dls, splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in learners.keys():\n",
    "    for i in range(5):        \n",
    "        model, lr, days, dls, splits = get_model_and_lr(k)\n",
    "        learn = Learner(dls, model, metrics=accuracy, loss_func=LabelSmoothingCrossEntropy(), cbs=[ShowGraphCallback2(), PredictionDynamics(), SaveModelCallback(monitor='accuracy', comp=np.greater, min_delta=.001, fname=k + str(i))])\n",
    "        learn.fit_one_cycle(200, lr_max=lr, cbs=EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=15))\n",
    "        learners[k].append(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in learners.keys():\n",
    "    for i in range(1):\n",
    "        learners[k][i].export(\"./models/\" + k + str(i) + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionTimePlus(dls.vars, dls.c, bottleneck=True, coord=False, ks=5, conv_dropout=.8, depth=6, nf=32)\n",
    "\n",
    "model.load_state_dict(torch.load(\"./models/\" + 'ITP' + '0.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, metrics=accuracy, loss_func=LabelSmoothingCrossEntropy(), cbs=[ShowGraphCallback2(), SaveModelCallback(monitor='accuracy', comp=np.greater, min_delta=.001, fname=k + str(i))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.get_preds(dl=dls.valid, with_input=False, with_loss=False, with_decoded=True, act=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = {}\n",
    "target = {}\n",
    "preds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, lr, days, dls, splits = get_model_and_lr('ITP1')\n",
    "for k in learners.keys():    \n",
    "    probas[k] = [None] * 5\n",
    "    target[k] = [None] * 5\n",
    "    preds[k] = [None] * 5\n",
    "    \n",
    "    for i in range(5):                \n",
    "        model, lr, days, dls, splits_new = get_model_and_lr(k, splits=splits)        \n",
    "        assert splits_new == splits\n",
    "        model.load_state_dict(torch.load(\"./models/\" + k + str(i) + '.pth'))\n",
    "        learn = Learner(dls, model, metrics=accuracy, loss_func=LabelSmoothingCrossEntropy(), cbs=[ShowGraphCallback2(), PredictionDynamics(), SaveModelCallback(monitor='accuracy', comp=np.greater, min_delta=.001, fname=k + str(i))])\n",
    "        probas[k][i], target[k][i], preds[k][i] = learn.get_preds(dl=dls.valid, with_input=False, with_loss=False, with_decoded=True, act=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.zeros(probas['TCN1'][0].shape, dtype=torch.float64, device = 'cuda')\n",
    "a\n",
    "#torch.empty_like(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.zeros(probas['TCN1'][0].shape, dtype=torch.float64, device = 'cpu')\n",
    "\n",
    "for k in learners.keys():    \n",
    "    for i in range(5):    \n",
    "        t = skm.accuracy_score(target[k][i], preds[k][i])\n",
    "        t2 = skm.accuracy_score(target[k][i], np.argmax(probas[k][i], axis=1))\n",
    "        if t > .55:\n",
    "            p += probas[k][i]\n",
    "        \n",
    "            print(k + str(i) + str(t) + ' argmax: ' + str(t2))\n",
    "    print('tmp: ' + str(skm.accuracy_score(np.argmax(p, axis=1), target['ITP1'][i])))\n",
    "unified=np.argmax(p, axis=1)\n",
    "print(skm.accuracy_score(unified, target['ITP1'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas[k], target[k], preds[k] = learners[k].get_X_preds(X[splits_2[1]], y[splits_2[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skm.confusion_matrix(unified, target['ITP1'][i], normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb1 = X[np.where(abs(target-preds) == 0)[0][:64]]\n",
    "yb1 = y[np.where(abs(target-preds) == 0)[0][:64]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = TSTensor(xb1).to(torch.device('cuda'))\n",
    "yb = TensorCategory(yb1).to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detach=True\n",
    "cpu=True\n",
    "apply_relu=True\n",
    "cmap='inferno'\n",
    "figsize=None\n",
    "att_maps = get_attribution_map(model, [model.conv2dblock, model.conv1dblock], xb, y=yb, detach=detach, cpu=cpu, apply_relu=apply_relu)\n",
    "att_maps[0] = ((att_maps[0] - att_maps[0].min()) / (att_maps[0].max() - att_maps[0].min())).mean(dim=0)\n",
    "att_maps[1] = ((att_maps[1] - att_maps[1].min()) / (att_maps[1].max() - att_maps[1].min())).mean(dim=0)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_maps[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_maps[0].mean(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_maps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = ifnone(figsize, (10, 10))\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = plt.axes()\n",
    "\n",
    "plt.title('Observed variables')\n",
    "im = ax.imshow(att_maps[0], cmap=cmap, aspect='auto')\n",
    "plt.yticks(range(len(filtered_feature_list)), filtered_feature_list)\n",
    "#plt.yticks(range(len(feature_names)), feature_names)\n",
    "cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "plt.colorbar(im, cax=cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=figsize)\n",
    "ax = plt.axes()\n",
    "plt.title('Time')\n",
    "im = ax.imshow(att_maps[1], cmap=cmap, aspect='auto')\n",
    "cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "plt.colorbar(im, cax=cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(zip(filtered_feature_list, list(att_maps[0].sum(dim=0).numpy()))).sort_values(1, ascending=False)#.plot.barh()#.to_csv('/home/scottcha/Desktop/feature_importance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_45_features = list(df.head(45)[0].values)\n",
    "top_45_features.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df, pd.DataFrame.from_records(df[0].str.split('_', 1))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns = ['feature', 'importance', 'var', 'level']\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sort_values(by='importance', ascending=False)[:20].plot.bar(x='feature', y='importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby('var').mean('importance').sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby('var').mean('importance').sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.set_index(0)\n",
    "tmp[tmp[1] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#top 30 features\n",
    "list(df.set_index(0).head(250).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bottom 30 features\n",
    "list(df.set_index(0).tail(250).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(X[220140][filtered_feature_list.index('TMP_20mb_avg')]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.classification import LearningWithNoisyLabels\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tfms = [TSStandardize(by_var=True), Nan2Value()]\n",
    "clf = TSClassifier(X, y, splits=splits_2, arch=InceptionTimePlus, batch_size=[64, 128], batch_tfms=batch_tfms, metrics=accuracy, cbs=ShowGraph(), verbose=True, inplace=False)\n",
    "clf.fit_one_cycle(10, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probas, test_targets, test_preds = clf.get_X_preds(X[splits_2[0]], with_decoded=True)\n",
    "test_probas, test_targets, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.pruning import get_noise_indices\n",
    "\n",
    "ordered_label_errors = get_noise_indices(\n",
    "    s=y[splits_2[0]],\n",
    "    psx=test_probas.numpy(),\n",
    "    sorted_index_method='normalized_margin', # Orders label errors\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ordered_label_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_3=[[x for x in splits_2[0] if x not in ordered_label_errors], splits_2[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import tpe, hp, fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_list = ['InceptionTimePlus']#, 'XceptionTimePlus', 'InceptionTimeXLPlus']#, 'mWDN', 'FCN', 'ResNet', 'xresnet1d34', 'ResCNN', 'LSTM', 'LSTM_FCN']\n",
    "inception_arch_list = ['InceptionTimePlus', 'InceptionTimePlus17x17', 'InceptionTimePlus32x32', 'InceptionTimePlus47x47', 'InceptionTimePlus62x62', 'InceptionTimeXLPlus']\n",
    "tst_arch_list = ['TST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xcm(params):\n",
    "    print(params)\n",
    "    batch_size, window_perc, loss, fc_dropout, nf = params['batch_size'], params['window_perc'], params['loss'], params['fc_dropout'], params['nf']\n",
    "    dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[batch_size], batch_tfms=batch_tfms, num_workers=12, inplace=False)\n",
    "    model = XCM(dls.vars, dls.c, dls.len, window_perc=window_perc, nf=nf, fc_dropout=fc_dropout)\n",
    "    \n",
    "    loss_func = None\n",
    "    if loss == 'LabelSmoothingCrossEntropy':\n",
    "        loss_func = LabelSmoothingCrossEntropy()\n",
    "    elif loss == 'LabelSmoothingCrossEntropyFlat':\n",
    "        loss_func = LabelSmoothingCrossEntropyFlat()\n",
    "        \n",
    "    learn = Learner(dls, model, metrics=accuracy, loss_func=loss_func)   \n",
    "    \n",
    "    learn.fit_one_cycle(25, lr_max=1e-3, cbs=EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=3))\n",
    "    learn.recorder.plot_metrics()\n",
    "    interp = ClassificationInterpretation.from_learner(learn)\n",
    "    conf_matrix = interp.plot_confusion_matrix(normalize=True)\n",
    "    #enable us to use fmin\n",
    "    #return learn.recorder.values[-1][1]\n",
    "    return 1.0 - accuracy_local(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_tst(params):\n",
    "    print(params)\n",
    "    d_model, n_heads, d_k, d_v, d_ff, dropout, n_layers, fc_dropout, loss = params['d_model'], params['n_heads'], params['d_k'], params['d_v'], params['d_ff'], params['dropout'], params['n_layers'], params['fc_dropout'], params['loss']\n",
    "    dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64], batch_tfms=batch_tfms, num_workers=12, inplace=False)\n",
    "    model = TST(dls.vars, dls.c, dls.len, d_model=d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, dropout=dropout, n_layers=n_layers, fc_dropout=fc_dropout)\n",
    "    \n",
    "    loss_func = None\n",
    "    if loss == 'LabelSmoothingCrossEntropy':\n",
    "        loss_func = LabelSmoothingCrossEntropy()\n",
    "    elif loss == 'LabelSmoothingCrossEntropyFlat':\n",
    "        loss_func = LabelSmoothingCrossEntropyFlat()\n",
    "        \n",
    "    learn = Learner(dls, model, metrics=accuracy, loss_func=loss_func)   \n",
    "    \n",
    "    learn.fit_one_cycle(25, lr_max=1e-4, cbs=EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=3))\n",
    "    learn.recorder.plot_metrics()\n",
    "    interp = ClassificationInterpretation.from_learner(learn)\n",
    "    conf_matrix = interp.plot_confusion_matrix(normalize=True)\n",
    "    #enable us to use fmin\n",
    "    #return learn.recorder.values[-1][1]\n",
    "    return 1.0 - accuracy_local(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    #arch, levels = params['arch'], params['levels']\n",
    "    print(params)\n",
    "    arch, ks, bottleneck, coord, dropout, lr, depth, num_filters, loss = params['arch'], params['ks'], params['bottleneck'], params['coord'], params['dropout'], params['lr'], params['depth'], params['num_filters'], params['loss']\n",
    "    #create the dataloader\n",
    "    #shouldn't be necessary, but there must be a bug which causes teh tfms to stop being executed on the validation set after a run\n",
    "    dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64], batch_tfms=batch_tfms, num_workers=12, inplace=False)\n",
    "\n",
    "    model = None\n",
    "    if arch == 'InceptionTime':\n",
    "        model = InceptionTime(dls.vars, dls.c)\n",
    "    elif arch == 'InceptionTimePlus':\n",
    "        model = InceptionTimePlus(dls.vars, dls.c, bottleneck=bottleneck, coord=coord, conv_dropout=dropout, depth=depth, nf=num_filters)\n",
    "    elif arch == 'InceptionTimePlus17x17':\n",
    "        model = InceptionTimePlus(dls.vars, dls.c, bottleneck=bottleneck, coord=coord, conv_dropout=dropout, depth=depth, nf=num_filters)\n",
    "    elif arch == 'InceptionTimePlus32x32':\n",
    "        model = InceptionTimePlus(dls.vars, dls.c, bottleneck=bottleneck, coord=coord, conv_dropout=dropout, depth=depth, nf=num_filters)\n",
    "    elif arch == 'InceptionTimePlus47x47':\n",
    "        model = InceptionTimePlus(dls.vars, dls.c, bottleneck=bottleneck, coord=coord, conv_dropout=dropout, depth=depth, nf=num_filters)\n",
    "    elif arch == 'InceptionTimePlus62x62':\n",
    "        model = InceptionTimePlus(dls.vars, dls.c, bottleneck=bottleneck, coord=coord, conv_dropout=dropout, depth=depth, nf=num_filters)\n",
    "    elif arch == 'InceptionTimeXLPlus':\n",
    "        model = InceptionTimeXLPlus(dls.vars, dls.c, bottleneck=bottleneck, coord=coord, conv_dropout=dropout, depth=depth, nf=num_filters)\n",
    "    elif arch == 'XceptionTime':\n",
    "        model = XceptionTime(dls.vars, dls.c)\n",
    "    elif arch == 'XceptionTimePlus':\n",
    "        model = XceptionTimePlus(dls.vars, dls.c,  bottleneck=bottleneck, coord=coord)\n",
    "    elif arch == 'mWDN':\n",
    "        model = mWDN(dls.vars, dls.c, seq_len=180, levels=4)\n",
    "    elif arch == 'FCN':\n",
    "        model = FCN(dls.vars, dls.c)\n",
    "    elif arch == 'ResNet':\n",
    "        model = ResNet(dls.vars, dls.c)\n",
    "    elif arch == 'xresnet1d34':\n",
    "        model = xresnet1d34(dls.vars, dls.c)\n",
    "    elif arch == 'ResCNN':\n",
    "        model = ResCNN(dls.vars, dls.c)    \n",
    "    elif arch == 'LSTM':\n",
    "        model = LSTM(dls.vars, dls.c)\n",
    "    elif arch == 'LSTM_FCN':\n",
    "        model = LSTM_FCN(dls.vars, dls.c)    \n",
    "        \n",
    "    loss_func = None\n",
    "    if loss == 'LabelSmoothingCrossEntropy':\n",
    "        loss_func = LabelSmoothingCrossEntropy()\n",
    "    elif loss == 'LabelSmoothingCrossEntropyFlat':\n",
    "        loss_func = LabelSmoothingCrossEntropyFlat()\n",
    "        \n",
    "    learn = Learner(dls, model, metrics=accuracy, loss_func=loss_func)   \n",
    "    #without oversampling\n",
    "    learn.fit_one_cycle(25, lr_max=lr, cbs=EarlyStoppingCallback(monitor='accuracy', min_delta=0.01, patience=3))\n",
    "    learn.recorder.plot_metrics()\n",
    "    interp = ClassificationInterpretation.from_learner(learn)\n",
    "    conf_matrix = interp.plot_confusion_matrix(normalize=True)\n",
    "    #enable us to use fmin\n",
    "    #return learn.recorder.values[-1][1]\n",
    "    return 1.0 - accuracy_local(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lstm(params):\n",
    "   \n",
    "    print(params)\n",
    "    hidden_size, n_layers, bias, rnn_dropout, bidirectional, fc_dropout, plus, lr, loss = params['hidden_size'], params['n_layers'], params['bias'], params['rnn_dropout'], params['bidirectional'], params['fc_dropout'], params['plus'], params['lr'], params['loss']\n",
    "    #create the dataloader\n",
    "    #shouldn't be necessary, but there must be a bug which causes teh tfms to stop being executed on the validation set after a run\n",
    "    dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[32], batch_tfms=batch_tfms, num_workers=12, inplace=False)\n",
    "    model = None\n",
    "    if plus:\n",
    "        model = LSTMPlus(dls.vars, dls.c, hidden_size=hidden_size, n_layers=n_layers, bias=bias, rnn_dropout=rnn_dropout, bidirectional=bidirectional, fc_dropout=fc_dropout)\n",
    "    else:\n",
    "        model = LSTM(dls.vars, dls.c, hidden_size=hidden_size, n_layers=n_layers, bias=bias, rnn_dropout=rnn_dropout, bidirectional=bidirectional, fc_dropout=fc_dropout)\n",
    "\n",
    "    loss_func = None\n",
    "    if loss == 'LabelSmoothingCrossEntropy':\n",
    "        loss_func = LabelSmoothingCrossEntropy()\n",
    "    elif loss == 'LabelSmoothingCrossEntropyFlat':\n",
    "        loss_func = LabelSmoothingCrossEntropyFlat()\n",
    "    learn = Learner(dls, model, metrics=accuracy, loss_func=loss_func)   \n",
    "    #without oversampling\n",
    "    learn.fit_one_cycle(25, lr_max=lr, cbs=EarlyStoppingCallback(monitor='accuracy', min_delta=0.01, patience=3))\n",
    "    learn.recorder.plot_metrics()\n",
    "    interp = ClassificationInterpretation.from_learner(learn)\n",
    "    conf_matrix = interp.plot_confusion_matrix(normalize=True)\n",
    "    #enable us to use fmin\n",
    "    return 1.0 - accuracy_local(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'arch': hp.choice('arch', arch_list),\n",
    "    'ks': hp.choice('ks', [5, 10, 40, 60, 80]),\n",
    "    'bottleneck': hp.choice('bottleneck', [True]),\n",
    "    'coord': hp.choice('coord', [False]),\n",
    "    #'stride': hp.choice('stride', [1,3,5,7,9]),\n",
    "    'dropout': hp.choice('dropout', [0., .2, .5, .8]),\n",
    "    'lr': hp.choice('lr', [1e-2, 1e-3, 1e-4]),\n",
    "    'depth': hp.choice('depth', [3, 6, 9]),\n",
    "    'num_filters': hp.choice('num_filters', [32, 64, 96]),\n",
    "    'loss': hp.choice('loss', ['LabelSmoothingCrossEntropy', 'LabelSmoothingCrossEntropyFlat', 'None'])\n",
    "}\n",
    "\n",
    "space_lstm = {\n",
    "    'hidden_size': hp.choice('hidden_size', [100, 150, 200, 250]),\n",
    "    'n_layers': hp.choice('n_layers', [1,5,8,10]),\n",
    "    'bias': hp.choice('bias', [True, False]),\n",
    "    'rnn_dropout': hp.choice('rnn_dropout', [0, .2, .4, .6, .8]),\n",
    "    'bidirectional': hp.choice('bidirectional', [True, False]),\n",
    "    'fc_dropout': hp.choice('fc_dropout', [0, .2, .4, .6, .8]),\n",
    "    'plus': hp.choice('plus', [True, False]),\n",
    "    'lr': hp.choice('lr', [1e-2, 1e-3, 1e-4]),\n",
    "    'loss': hp.choice('loss', ['LabelSmoothingCrossEntropy', 'LabelSmoothingCrossEntropyFlat', 'None'])\n",
    "}\n",
    "\n",
    "space_tst = {\n",
    "    'd_model': hp.choice('d_model', [64, 128, 256]),\n",
    "    'n_heads': hp.choice('n_heads', [8, 16, 24]), \n",
    "    'd_k': hp.choice('d_k', [16, 32, 64, None]), \n",
    "    'd_v': hp.choice('d_v', [16, 32, 64, None]), \n",
    "    'd_ff': hp.choice('d_ff', [128, 256, 512]), \n",
    "    'dropout': hp.choice('dropout', [0., .1, .5, .7]) ,\n",
    "    'n_layers': hp.choice('n_layers', [2,3,4,5]), \n",
    "    'fc_dropout': hp.choice('fc_dropout', [0, .1, .5, .7]),\n",
    "    'loss': hp.choice('loss', ['LabelSmoothingCrossEntropy', 'LabelSmoothingCrossEntropyFlat', 'None'])\n",
    "}\n",
    "\n",
    "space_xcm = {\n",
    "    'batch_size': hp.choice('batch_size', [1, 16, 32, 64]),\n",
    "    'window_perc': hp.choice('window_perc', [.3, .5, .7, 1]),\n",
    "    'nf': hp.choice('nf', [64, 128, 256]),\n",
    "    'fc_dropout': hp.choice('fc_dropout', [0, .1, .5, .8]),\n",
    "    'loss': hp.choice('loss', ['LabelSmoothingCrossEntropy', 'LabelSmoothingCrossEntropyFlat', 'None'])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = fmin(\n",
    "        fn=objective_xcm,\n",
    "        space=space_xcm,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=25\n",
    ")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=50\n",
    ")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#large\n",
    "best = fmin(\n",
    "        fn=objective_lstm,\n",
    "        space=space_lstm,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best = fmin(\n",
    "        fn=objective_tst,\n",
    "        space=space_tst,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimum for 30k samples {'arch': 'InceptionTimePlus', 'bottleneck': True, 'coord': False, 'depth': 3, 'dropout': 0.0, 'ks': 60, 'lr': 0.001, 'num_filters': 64}\n",
    "#for 180k samples 72 features {'arch': 'InceptionTimePlus', 'bottleneck': True, 'coord': False, 'depth': 6, 'dropout': 0.8, 'ks': 5, 'loss': 'LabelSmoothingCrossEntropy', 'lr': 0.01, 'num_filters': 32}\n",
    "#{'arch': 'InceptionTimePlus', 'bottleneck': True, 'coord': False, 'depth': 6, 'dropout': 0.8, 'ks': 5, 'loss': 'LabelSmoothingCrossEntropy', 'lr': 0.001, 'num_filters': 64}\n",
    "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64], batch_tfms=batch_tfms, num_workers=12, inplace=False)\n",
    "model = InceptionTimePlus(dls.vars, dls.c, bottleneck=True, coord=False, ks=5, conv_dropout=.8, depth=6, nf=32)\n",
    "learn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropy(), metrics=accuracy)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learn.fit_one_cycle(25, lr_max=1e-3, cbs=[EarlyStoppingCallback(monitor='accuracy', min_delta=1e-2, patience=3), ShowGraphCallback2()])#, SaveModelCallback(every_epoch=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TST 30 days, 200k samples, 74 features\n",
    "learn.fit_one_cycle(25, lr_max=1e-4, cbs=[EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=5)])#, SaveModelCallback(every_epoch=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "conf_matrix = interp.plot_confusion_matrix(normalize=True)\n",
    "conf_matrix = interp.plot_confusion_matrix(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_local(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(5):\n",
    "    model = None\n",
    "    dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64], batch_tfms=batch_tfms, num_workers=12, inplace=False)\n",
    "    model = TST(dls.vars, dls.c, dls.len, d_ff=512, d_k=64, d_model=128, d_v=64, dropout=.7, fc_dropout=.1, n_heads=16, n_layers=4)\n",
    "    learn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropyFlat(), metrics=accuracy,  cbs=ShowGraphCallback2())\n",
    "    learn.fit_one_cycle(25, lr_max=1e-4, cbs=[EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=3)])\n",
    "    interp = ClassificationInterpretation.from_learner(learn)\n",
    "    conf_matrix = interp.plot_confusion_matrix(normalize=True)\n",
    "    print(str(accuracy_local(interp)))\n",
    "    models.append(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[splits_2[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = []\n",
    "ys[0] = models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "conf_matrix = interp.plot_confusion_matrix(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#30 days, 50% samples , 74 features\n",
    "learn.fit_one_cycle(25, lr_max=1e-4, cbs=[EarlyStoppingCallback(monitor='accuracy', min_delta=0.001, patience=5)])#, SaveModelCallback(every_epoch=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "conf_matrix = interp.plot_confusion_matrix(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = learn.dls.valid.dataset[:100][0].data\n",
    "y_valid = learn.dls.valid.dataset[:100][1].data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.dls.valid.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [mn for mn in learn.recorder.metric_names if mn not in ['epoch', 'train_loss', 'valid_loss', 'time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_metric_idx=0\n",
    "if len(metrics) == 0 or key_metric_idx is None:\n",
    "    metric_name = learner.loss_func.__class__.__name__\n",
    "    key_metric_idx = None\n",
    "else:\n",
    "    metric_name = metrics[key_metric_idx]\n",
    "    metric = learn.recorder.metrics[key_metric_idx].func\n",
    "print(f'Selected metric: {metric_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = pd.read_csv(ml_path + '/FeatureLabels_' + file_label + '.csv')\n",
    "feature_names = feature_names['0'].sort_values().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if feature_names is None:\n",
    "    feature_names = [f\"var_{i}\" for i in range(X_valid.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "print('Computing feature importance...')\n",
    "\n",
    "COLS = ['BASELINE'] + list(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dls.valid.dataset.tls[0]._splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.valid.dataset[0:10][0].data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid[:, k-1].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "random_state=23\n",
    "if k>0:\n",
    "    save_feat = X_valid[:, k-1].clone()\n",
    "    X_valid[:, k-1] = random_shuffle(X_valid[:, k-1].flatten(), random_state=random_state).reshape(X_valid[:, k-1].shape)\n",
    "if key_metric_idx is None:\n",
    "    value = learn.get_X_preds(X_valid, y_valid, with_loss=True)[-1].mean().item()\n",
    "else:\n",
    "    output = learn.get_X_preds(X_valid, y_valid)\n",
    "    value = metric(output[0], output[1]).item()\n",
    "    print(f\"{k:3} feature: {COLS[k]:20} {metric_name}: {value:8.6f}\")\n",
    "    results.append([COLS[k], value])\n",
    "if k>0:\n",
    "    X_valid[:, k-1] = save_feat\n",
    "    del save_feat; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??learn.dls.valid.new_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sys import getsizeof, stderr\n",
    "from itertools import chain\n",
    "from collections import deque\n",
    "try:\n",
    "    from reprlib import repr\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "def total_size(o, handlers={}, verbose=False):\n",
    "    \"\"\" Returns the approximate memory footprint an object and all of its contents.\n",
    "\n",
    "    Automatically finds the contents of the following builtin containers and\n",
    "    their subclasses:  tuple, list, deque, dict, set and frozenset.\n",
    "    To search other containers, add handlers to iterate over their contents:\n",
    "\n",
    "        handlers = {SomeContainerClass: iter,\n",
    "                    OtherContainerClass: OtherContainerClass.get_elements}\n",
    "\n",
    "    \"\"\"\n",
    "    dict_handler = lambda d: chain.from_iterable(d.items())\n",
    "    all_handlers = {tuple: iter,\n",
    "                    list: iter,\n",
    "                    deque: iter,\n",
    "                    dict: dict_handler,\n",
    "                    set: iter,\n",
    "                    frozenset: iter,\n",
    "                   }\n",
    "    all_handlers.update(handlers)     # user handlers take precedence\n",
    "    seen = set()                      # track which object id's have already been seen\n",
    "    default_size = getsizeof(0)       # estimate sizeof object without __sizeof__\n",
    "\n",
    "    def sizeof(o):\n",
    "        if id(o) in seen:       # do not double count the same object\n",
    "            return 0\n",
    "        seen.add(id(o))\n",
    "        s = getsizeof(o, default_size)\n",
    "\n",
    "        if verbose:\n",
    "            print(s, type(o), repr(o), file=stderr)\n",
    "\n",
    "        for typ, handler in all_handlers.items():\n",
    "            if isinstance(o, typ):\n",
    "                s += sum(map(sizeof, handler(o)))\n",
    "                break\n",
    "        return s\n",
    "\n",
    "    return sizeof(o)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dl; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=64\n",
    "with_input=False\n",
    "with_decoded=True\n",
    "with_loss=False\n",
    "dl = learn.dls.valid.new_dl(X_valid, y=y_valid)\n",
    "if bs: setattr(dl, \"bs\", bs)\n",
    "else: assert dl.bs, \"you need to pass a bs != 0\"\n",
    "output = list(learn.get_preds(dl=dl, with_input=with_input, with_decoded=with_decoded, with_loss=with_loss, reorder=False))\n",
    "if with_decoded and len(learn.dls.tls) >= 2 and hasattr(learn.dls.tls[-1], \"tfms\") and hasattr(learn.dls.tls[-1].tfms, \"decodes\"):\n",
    "    output[2 + with_input] = learn.dls.tls[-1].tfms.decode(output[2 + with_input])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.dls.tls[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = learn.get_X_preds(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.valid.one_batch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets.valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas, target, preds  = learn.get_preds(dl=dls.valid, with_input=False, with_loss=False, with_decoded=True, act=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[splits_2[1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.feature_importance(X[splits_2[1]], y[splits_2[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(75):\n",
    "    print(str(i))\n",
    "    probas, target, preds = learn.get_X_preds(X[splits_2[1]], y[splits_2[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#60000 samples 0 dropout\n",
    "learn.fit_one_cycle(25, lr_max=1e-4, cbs=[EarlyStoppingCallback(monitor='accuracy', min_delta=0.01, patience=5)])#, SaveModelCallback(every_epoch=True)])\n",
    "learn.recorder.plot_metrics()\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "#conf_matrix = interp.plot_confusion_matrix(normalize=True)\n",
    "\n",
    "print(accuracy_local(interp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = interp.plot_confusion_matrix(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#60000 samples\n",
    "learn.fit_one_cycle(25, lr_max=1e-3, cbs=[EarlyStoppingCallback(monitor='accuracy', min_delta=0.01, patience=3)])#, SaveModelCallback(every_epoch=True)])\n",
    "learn.recorder.plot_metrics()\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "conf_matrix = interp.plot_confusion_matrix(normalize=True)\n",
    "\n",
    "print(accuracy_local(interp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, lr_max=1e-2)#, cbs=EarlyStoppingCallback(monitor='accuracy', min_delta=0.1, patience=5))\n",
    "learn.recorder.plot_metrics()\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "conf_matrix = interp.plot_confusion_matrix(normalize=True)\n",
    "\n",
    "print(accuracy_local(interp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_label_errors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x for x in splits_2[0] if x not in ordered_label_errors], splits_2[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tfms = [TSStandardize(by_var=True), Nan2Value()]\n",
    "clf = TSClassifier(X, y, splits=[[x for x in splits_2[0] if x not in ordered_label_errors], splits_2[1]], arch=InceptionTimePlus, batch_tfms=batch_tfms, metrics=accuracy, cbs=ShowGraph(), verbose=True)\n",
    "clf.fit_one_cycle(3, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(clf)\n",
    "conf_matrix = interp.plot_confusion_matrix(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = pd.read_csv(ml_path + '/FeatureLabels_' + file_label + '.csv')\n",
    "feature_names = feature_names['0'].sort_values().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = xr.DataArray(X, dims=['sample','feature', 'timestep'], coords=[range(0,12000),feature_names,range(0,180)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_feature_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_features = [x for x in feature_names if 'avg' in x ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_level_features = ['ABSV', 'HGT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = ['WEASD_surface_max', 'SNOD_surface_max', 'UGRD_30M0mbaboveground_max', 'VGRD_30M0mbaboveground_max', 'TMP_surface_min', 'HGT_surface_min', 'VVEL_150mb_avg', 'APCP_surface_sum', 'ACPCP_surface_sum', 'WATR_surface_sum']\n",
    "small_feature_indexes = [list(feature_names).index(x) for x in feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.sel(feature='SNOD_surface_avg', sample=4).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.sel(feature='APCP_surface_sum', sample=range(0,100)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y, splits = get_classification_data('ECG200', split_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.sel(feature=feature, timestep=timestep).values.reshape(6500,len(feature),len(timestep))[splits_2[0][8]][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep=range(0,180)\n",
    "feature = ['WEASD_surface_max', 'SNOD_surface_max', 'UGRD_30M0mbaboveground_max', 'VGRD_30M0mbaboveground_max', 'TMP_surface_min', 'HGT_surface_min', 'VVEL_150mb_avg', 'APCP_surface_sum', 'ACPCP_surface_sum', 'WATR_surface_sum']\n",
    "bad_features = ['PRES_surface_max', 'TSOIL_1M2mbelowground_max', 'TMP_surface_min', 'RH_2maboveground_max']\n",
    "tfms = [None, [Categorize()]]\n",
    "X = da.sel(feature=feature, timestep=timestep).values.reshape(12000,len(feature),len(timestep))\n",
    "dsets2 = TSDatasets(X, y, tfms=tfms, splits=splits_2, inplace=True)\n",
    "#create the dataloader\n",
    "dls2 = TSDataLoaders.from_dsets(dsets2.train, dsets2.valid, bs=[64], batch_tfms=[TSStandardize(by_var=True), Nan2Value()], num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_features = get_ts_features(np.nan_to_num(X), y, n_jobs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls2.train.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls2.valid.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??get_ts_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TST: 30: .29\n",
    "#TST: 80: .2\n",
    "#TST: 180: .4\n",
    "\n",
    "#Try TST\n",
    "model = InceptionTimePlus(dls2.vars, dls2.c, dls2.len)#, res_dropout=.3, fc_dropout=.8)\n",
    "#config = {'res_dropout':.3, 'fc_dropout':.8}\n",
    "learn = Learner(dls2, model, loss_func=LabelSmoothingCrossEntropy(), metrics=[RocAuc(), accuracy],  cbs=ShowGraphCallback2())\n",
    "#learn = TSClassifier( y, splits=splits_3, arch=TST, arch_config=config, batch_tfms=[TSStandardize(), Nan2Value()], metrics=accuracy, cbs=ShowGraph(), verbose=True)\n",
    "#learn.fit_one_cycle(10, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "conf_matrix = interp.plot_confusion_matrix(normalize=True)\n",
    "\n",
    "print(accuracy_local(interp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import LayerConductance\n",
    "from captum.attr import NeuronConductance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = learn.model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_tensor = b[0].type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = b[1].cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_probs = net(test_input_tensor).detach().cpu().numpy()\n",
    "out_classes = np.argmax(out_probs, axis=1)\n",
    "train_accuracy = sum(out_classes == test_labels) / len(test_labels)\n",
    "print(\"Accuracy:\", train_accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_metric('Train Accuracy', train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_input_tensor.requires_grad_()\n",
    "attr, delta = ig.attribute(test_input_tensor,target=1, return_convergence_delta=True)\n",
    "attr = attr.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = feature_names[feature_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to print importances and visualize distribution\n",
    "def visualize_importances(feature_names, importances, title=\"Average Feature Importances\", plot=True, axis_title=\"Features\"):\n",
    "    pd.DataFrame(zip(feature_names, np.mean(importances, axis=(0,2)))).sort_values(1, ascending=False)[:25].plot.barh(x=0, y=1)\n",
    "\n",
    "visualize_importances(feature_names, attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features =  pd.DataFrame(zip(feature_names, np.mean(attr, axis=(0,2))), columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features.to_csv('../Scratch/Notebooks/' + '/FeatureImportances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = pd.read_csv('../Scratch/Notebooks/' + '/FeatureImportances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_X = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_feature_means = feature_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_feature_std = feature_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_filter = top_features[:100].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = orig_X[:, feature_filter, :]\n",
    "feature_means = orig_feature_means[feature_filter]\n",
    "feature_std = orig_feature_std[feature_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_values = torch.zeros_like(TSTensor(X))\n",
    "for i in range(0,X.shape[1]):\n",
    "    fill_values[:,i,:] = torch.full_like(TSTensor(X[:,i,:]), feature_means[i])\n",
    "        \n",
    "X_noNan = torch.where(torch.isnan(TSTensor(X)), fill_values, TSTensor(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noNan[:,:,-30:].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tsai.22nocaputm]",
   "language": "python",
   "name": "conda-env-tsai.22nocaputm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
