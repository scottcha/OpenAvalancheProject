{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: 1.parsegfs.html\n",
    "title: 1.ParseGFS\n",
    "subtitle: Notebook which takes GFS model data, filteres it to relevant avalanche regions\n",
    "  and outputs it to regional .netcdf files\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp parse_gfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParseGFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### uses env oap_datapipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes: At least on my machine I found a few things were affecting the performance and stability of the disk operations\n",
    "1. The Dask based operations either in Xarray or directly in Dask (using local cluster) where less efficient than just using joblib parallel.  So I tend ot use Xarray in single process mode and do the parallel work with joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| test_ignore\n",
    "#%reload_ext autoreload\n",
    "#%autoreload 2\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import salem\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from joblib import Parallel, delayed\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023.7.0'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data structure\n",
    "\n",
    "OAPMLData/\n",
    "\n",
    "    1.RawWeatherData/\n",
    "        gfs/\n",
    "            <season>/\n",
    "                /<state or country>/\n",
    "    2.GFSFiltered(x)Interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These parameters need to be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| test_ignore\n",
    "seasons = ['15-16','16-17', '17-18', '18-19', '19-20', '20-21'] \n",
    "state = 'Colorado'\n",
    "\n",
    "interpolate = 1 #interpolation factor: whether we can to augment the data through lat/lon interpolation; 1 no interpolation, 4 is 4x interpolation\n",
    "\n",
    "input_data_root = '//openmediavault/DataSynced/OAPMLData/'\n",
    "output_data_root = 'D:/OAPMLData/'\n",
    "\n",
    "n_jobs = 24 #number of parallel processes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below here should be executed based on the above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ParseGFS:\n",
    "    \"\"\"Class which provides the basic utilities and processing to transform a set of GFS hourly weather file \n",
    "       to a set of filtered, aggregated and optionally interpolated netCDF files organized.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def season_to_snow_start_date(season):\n",
    "        if season == '15-16':\n",
    "            return '2015-11-01'\n",
    "        elif season == '16-17':\n",
    "            return '2016-11-01'\n",
    "        elif season == '17-18':\n",
    "            return '2017-11-01'\n",
    "        elif season == '18-19':\n",
    "            return '2018-11-01'\n",
    "        elif season == '19-20':\n",
    "            return '2019-11-01'\n",
    "        elif season == '20-21':\n",
    "            return '2020-11-01'\n",
    "    \n",
    "    def __init__(self, season, state, input_data_root, output_data_root=None, interpolate=1, resample_length='1d'):\n",
    "        \"\"\"Initialize the class\n",
    "        \n",
    "        Keyword arguments:\n",
    "        season: the season code (e.g., 15-16, 16-17) for the season you are processing\n",
    "        state: the name of the state or country we are processing\n",
    "        data_root: the root path of the data folders which contains the 1.RawWeatherData folder\n",
    "        interpolate: the degree of interpolation (1x and 4x have been tested, 1x is default)   \n",
    "        resample_length: the timeframe to resample to\n",
    "        \"\"\"\n",
    "        self.season = season\n",
    "        self.snow_start_date = ParseGFS.season_to_snow_start_date(season)\n",
    "        self.state = state\n",
    "        self.interpolate = interpolate\n",
    "        assert(input_data_root is not None)\n",
    "        self.input_data_root = input_data_root\n",
    "        if(output_data_root is None):\n",
    "            self.output_data_root = input_data_root\n",
    "        self.state_path = None\n",
    "        self.resample_length = resample_length\n",
    "        \n",
    "        #make sure these are correct, but these generally don't need to change\n",
    "        #if state == 'Washington' or state == 'Canada':\n",
    "        self.state_path = state\n",
    "        #else:\n",
    "        #    self.state_path = 'Colorado' #the nc file which contains the weather data contains both utah and colorado\n",
    "\n",
    "\n",
    "        #Path to USAvalancheRegions.geojson in this repo\n",
    "        self.region_path = '../Data' \n",
    "        #path to the gfs netcdf files for input\n",
    "        self.dataset_path = input_data_root + '/1.RawWeatherData/gfs/' + season + '/' + self.state_path + '/'\n",
    "        self.precip_dataset_path = input_data_root + '/1.RawWeatherData/gfs/' + season + '/' + self.state_path + 'AccumulationGrib/'\n",
    "\n",
    "        self.filtered_path = output_data_root + '2.GFSFiltered' + str(interpolate) + 'xInterpolation' + resample_length + '/' + season + '/'\n",
    "\n",
    "        #input file pattern as we'll read a full winter season in one pass\n",
    "        self.file_pattern2 = '00.f0[0-2]*.nc'\n",
    "\n",
    "        p = 181\n",
    "        if season in ['15-16', '19-20']:\n",
    "            p = 182 #leap years\n",
    "\n",
    "        self.date_values_pd = pd.date_range(self.snow_start_date, periods=p, freq=\"D\")\n",
    "        if self.state == 'ColoradoSmall':\n",
    "            #truncate dates to just first n days\n",
    "            self.date_values_pd = self.date_values_pd[0:24]\n",
    "        \n",
    "        print(self.dataset_path + ' Is Input Directory')\n",
    "        print(self.filtered_path + ' Is output directory of filtering')\n",
    "        \n",
    "        #check dates end on April 30 which is the last day we support\n",
    "        #assert(self.date_values_pd[-1].month == 4)\n",
    "        #assert(self.date_values_pd[-1].day == 30)\n",
    "        \n",
    "        if not os.path.exists(self.filtered_path):\n",
    "            os.makedirs(self.filtered_path)\n",
    "        \n",
    "        #Read in all avy region shapes and metadata\n",
    "        regions_df = None\n",
    "        if self.state == 'Canada':\n",
    "            regions_df = gpd.read_file(self.region_path + '/CAAvalancheRegions.geojson')\n",
    "        else:\n",
    "            regions_df = gpd.read_file(self.region_path + '/USAvalancheRegions.geojson')\n",
    "        #filter to just the ones where we have lables for training\n",
    "        self.training_regions_df = regions_df[regions_df['is_training']==True].copy()\n",
    "\n",
    "        #TODO: this needs to not rely on a code change to add a region\n",
    "        if self.state == 'Washington':\n",
    "            self.training_regions_df = self.training_regions_df[self.training_regions_df['center']=='Northwest Avalanche Center']\n",
    "        elif self.state == 'Utah':\n",
    "            self.training_regions_df = self.training_regions_df[self.training_regions_df['center']=='Utah Avalanche Center']\n",
    "        elif self.state == 'Colorado':\n",
    "            self.training_regions_df = self.training_regions_df[self.training_regions_df['center']=='Colorado Avalanche Information Center']\n",
    "        elif self.state == 'ColoradoSmall':\n",
    "            #used in debugging/profiling\n",
    "            self.training_regions_df = self.training_regions_df[self.training_regions_df['center']=='Colorado Avalanche Information Center']\n",
    "            #just get the first region\n",
    "            self.training_regions_df = self.training_regions_df.iloc[0:1]\n",
    "        elif self.state == 'Canada':\n",
    "            #include everything\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        self.training_regions_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        \n",
    "    def interpolate_and_write(self, tmp_ds):\n",
    "        import salem #required for multiprocessing--new behavior\n",
    "        \"\"\"\n",
    "        interpolate and filter each day \n",
    "        don't use dask for this, much faster to process the files in parallel \n",
    "        \n",
    "        Keyword arguments:\n",
    "        tmp_ds: the dataframe to process\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        redo_date = []\n",
    "        date = tmp_ds.time.dt.strftime('%Y%m%d').values[0]\n",
    "        for _, row in self.training_regions_df.iterrows():\n",
    "            print(\"Calculating region: \" + row['name'])\n",
    "            f = self.filtered_path + 'Region_' + row['name'] + '_' + date + '.nc'\n",
    "            try:\n",
    "                if self.interpolate == 1:                    \n",
    "                    #subset the xarray dataset to the region defined by a geojson polygon in the row; don't use salem for interpolation\n",
    "                    tmp_subset = tmp_ds.salem.subset(geometry=row['geometry'])     \n",
    "                else:\n",
    "                    #TODO: I've noticed that this might set a few vars, like snowdepth, to nan even when \n",
    "                    #interpolate == 1; need to investigate before using.\n",
    "                    new_lon = np.linspace(tmp_ds.longitude[0], tmp_ds.longitude[-1], tmp_ds.dims['longitude'] * self.interpolate)\n",
    "                    new_lat = np.linspace(tmp_ds.latitude[0], tmp_ds.latitude[-1], tmp_ds.dims['latitude'] * self.interpolate)\n",
    "                    interpolated_ds = tmp_ds.interp(latitude=new_lat, longitude=new_lon)\n",
    "                    tmp_subset = interpolated_ds.salem.subset(geometry=row['geometry']).salem.roi(geometry=row['geometry'])\n",
    "            except ValueError:            \n",
    "                errors.append('Value Error: Ensure the correct training regions have been provided')\n",
    "                del tmp_subset\n",
    "                continue\n",
    "            except Exception as err:\n",
    "                print('Salem subset exception ' + format(err))\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                comp = dict(zlib=True, complevel=7)\n",
    "                encoding = {var: comp for var in tmp_subset.data_vars}                                     \n",
    "                tmp_subset.load().to_netcdf(f, encoding=encoding )\n",
    "            except Exception as err:\n",
    "                print('output exception')\n",
    "                os.remove(f)\n",
    "                errors.append(f + ' -- ' + format(err))\n",
    "                redo_date.append(date)\n",
    "                del tmp_subset\n",
    "                continue\n",
    "            \n",
    "        \n",
    "        return (errors, redo_date)\n",
    "    \n",
    "    def resample(self, t):\n",
    "        \"\"\"\n",
    "        Convert netcdf files which already pivot across levels\n",
    "        from a full forcecast file which covers many days\n",
    "        to one which only covers one day in the future\n",
    "        also changes the data from hourly to daily min, avg, and max values\n",
    "        \n",
    "        Keyword arguments:\n",
    "        t: the pandas datetime to process\n",
    "       \n",
    "        \"\"\"\n",
    "        \n",
    "        print('On time: ' + str(t) + '\\n')\n",
    "        precip_ds = None\n",
    "        pd_t = pd.to_datetime(t)\n",
    "        with xr.open_mfdataset(self.precip_dataset_path + 'gfs.0p25.' + t + self.file_pattern2, combine='nested', concat_dim='time', parallel=False) as precip_ds:\n",
    "            #3-hour Accumulation (initial+0 to initial+3)\n",
    "            #6-hour Accumulation (initial+0 to initial+6)\n",
    "            #3-hour Accumulation (initial+6 to initial+9)\n",
    "            #6-hour Accumulation (initial+6 to initial+12)\n",
    "            #3-hour Accumulation (initial+12 to initial+15)\n",
    "            #6-hour Accumulation (initial+12 to initial+18)\n",
    "            #3-hour Accumulation (initial+18 to initial+21)\n",
    "            #6-hour Accumulation (initial+18 to initial+24)\n",
    "            #correct the values to all be 3 hour accumulations\n",
    "            corrected_dses = []\n",
    "            for i in range(0,len(precip_ds.time.values)):\n",
    "                if i % 2 == 0:\n",
    "                    corrected_dses.append(precip_ds.isel(time=i))\n",
    "                else:\n",
    "                    tmp_ds = precip_ds.isel(time=i) - precip_ds.isel(time=i-1)         \n",
    "                    corrected_dses.append(precip_ds.isel(time=i).assign(tmp_ds))\n",
    "            precip_ds = xr.concat(corrected_dses, dim='time').persist()\n",
    "            \n",
    "            #resample\n",
    "            total_name_dict = {}\n",
    "            for k in precip_ds.data_vars.keys():\n",
    "                total_name_dict[k] = k + '_sum'\n",
    "            resampled_precip_ds = precip_ds.resample(time=self.resample_length)\n",
    "            sum_resample = resampled_precip_ds.sum().rename(total_name_dict)   \n",
    "\n",
    "        ret_value = None\n",
    "        try:\n",
    "            with xr.open_mfdataset(self.dataset_path + 'gfs.0p25.' + t + self.file_pattern2, combine='nested', concat_dim='time', parallel=False) as ds: \n",
    "\n",
    "                #make sure we are just getting the first 24 hours                \n",
    "                min_name_dict = {}\n",
    "                max_name_dict = {}\n",
    "                avg_name_dict = {}\n",
    "                for k in ds.data_vars.keys():\n",
    "                    min_name_dict[k] = k + '_min'\n",
    "                    max_name_dict[k] = k + '_max'\n",
    "                    avg_name_dict[k] = k + '_avg'\n",
    "\n",
    "                resampled_ds = ds.resample(time=self.resample_length)\n",
    "                min_resample = resampled_ds.min().rename(min_name_dict)\n",
    "                max_resample = resampled_ds.max().rename(max_name_dict)\n",
    "                avg_resample = resampled_ds.mean().rename(avg_name_dict)\n",
    "\n",
    "                merged_ds = xr.merge([min_resample, max_resample, avg_resample, sum_resample]).persist()\n",
    "                try:\n",
    "                    ret_value = self.interpolate_and_write(merged_ds)\n",
    "              \n",
    "                except Exception as err:\n",
    "                    return self.input_data_root + self.state_path + '_' + t + '.nc' + ' -- ' + format(err)\n",
    "                    ds.close()\n",
    "                    merged_ds.close()\n",
    "                del merged_ds\n",
    "        except OSError as err:\n",
    "            print('Missing files for time: ' + t)\n",
    "        return ret_value\n",
    "            \n",
    "    \n",
    "    def check_resample(self, dates):\n",
    "        \"\"\"\n",
    "        method to check if there are any file open issues with the newly output files\n",
    "        \n",
    "        Keyword arguments:\n",
    "        date: pandas dates to check\n",
    "        \"\"\"\n",
    "        dates_with_errors = []\n",
    "        for t in dates.strftime('%Y%m%d'):\n",
    "            try:\n",
    "                with xr.open_dataset(self.input_data_root + self.state_path + '_' + t + '.nc') as file:\n",
    "                    file.close()\n",
    "                continue                \n",
    "            except Exception as e: \n",
    "                #print(format(e))\n",
    "                dates_with_errors.append(t)\n",
    "        return dates_with_errors\n",
    "    \n",
    "    def resample_local(self, jobs=4):\n",
    "        \"\"\"\n",
    "        Executes the resample process on the local machine.\n",
    "        All-Nan Slice and Divide warnings can be ignored\n",
    "            \n",
    "        Keyword arguments:\n",
    "        jobs: number of parallel processs to use (default = 4)\n",
    "        \"\"\"\n",
    "        results = Parallel(n_jobs=jobs, backend='loky')(map(delayed(self.resample), self.date_values_pd.strftime('%Y%m%d')))\n",
    "        #the new code seems to largely prevent corruption, truncating for now\n",
    "        return results\n",
    "        \n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On season: 15-16\n",
      "//openmediavault/DataSynced/OAPMLData//1.RawWeatherData/gfs/15-16/Canada/ Is Input Directory\n",
      "D:/OAPMLData/2.GFSFiltered1xInterpolation3H/15-16/ Is output directory of filtering\n",
      "On season: 16-17\n",
      "//openmediavault/DataSynced/OAPMLData//1.RawWeatherData/gfs/16-17/Canada/ Is Input Directory\n",
      "D:/OAPMLData/2.GFSFiltered1xInterpolation3H/16-17/ Is output directory of filtering\n",
      "On season: 17-18\n",
      "//openmediavault/DataSynced/OAPMLData//1.RawWeatherData/gfs/17-18/Canada/ Is Input Directory\n",
      "D:/OAPMLData/2.GFSFiltered1xInterpolation3H/17-18/ Is output directory of filtering\n",
      "On season: 18-19\n",
      "//openmediavault/DataSynced/OAPMLData//1.RawWeatherData/gfs/18-19/Canada/ Is Input Directory\n",
      "D:/OAPMLData/2.GFSFiltered1xInterpolation3H/18-19/ Is output directory of filtering\n",
      "On season: 19-20\n",
      "//openmediavault/DataSynced/OAPMLData//1.RawWeatherData/gfs/19-20/Canada/ Is Input Directory\n",
      "D:/OAPMLData/2.GFSFiltered1xInterpolation3H/19-20/ Is output directory of filtering\n",
      "On season: 20-21\n",
      "//openmediavault/DataSynced/OAPMLData//1.RawWeatherData/gfs/20-21/Canada/ Is Input Directory\n",
      "D:/OAPMLData/2.GFSFiltered1xInterpolation3H/20-21/ Is output directory of filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\scott\\miniconda3\\envs\\oap_datapipeline\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 30.7 s\n",
      "Wall time: 4h 7min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#| test_ignore\n",
    "\n",
    "#24 jobs once removed netcdf file lock\n",
    "#CPU times: total: 25.7 s\n",
    "#Wall time: 12min 29s\n",
    "for s in seasons:\n",
    "    print(\"On season: \" + s)\n",
    "    pgfs = ParseGFS(s, state, input_data_root, output_data_root, resample_length='3H')\n",
    "    results = pgfs.resample_local(jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
