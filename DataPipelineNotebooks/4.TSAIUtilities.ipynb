{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tsai_utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from joblib import Parallel, delayed\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tsai.all import * \n",
    "from openavalancheproject.tsai_simple_transforms import * \n",
    "from openavalancheproject.prep_ml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TSAIUtilities:\n",
    "    def __init__(self, X, label):\n",
    "        self.X = X\n",
    "        self.num_features = X.shape[1]\n",
    "        self.label = label\n",
    "\n",
    "    def numba_calcs(self, np_data):\n",
    "        feature_stds = np.std(np_data, axis=(0,2))\n",
    "        feature_means = np.mean(np_data, axis=(0,2))\n",
    "        return feature_stds, feature_means\n",
    "        \n",
    "    def calc_std_and_mean(self, data, sample_lower_bound, sample_upper_bound):\n",
    "        feature_stds = []\n",
    "        feature_means = []\n",
    "        num = 10\n",
    "        for i in range(0, data.shape[1], num):\n",
    "            print('on ' + str(i))\n",
    "            upper = i + num\n",
    "            if upper > data.shape[1]:\n",
    "                upper = data.shape[1]\n",
    "            np_data = data[sample_lower_bound:sample_upper_bound,i:upper,:]\n",
    "            np_data = np.nan_to_num(np_data)\n",
    "            tmp_stds, tmp_means = self.numba_calcs(np_data)\n",
    "            feature_stds.extend(tmp_stds)\n",
    "            feature_means.extend(tmp_means)\n",
    "        return feature_stds, feature_means\n",
    "\n",
    "    def calc_min_max(self, data, sample_lower_bound, sample_upper_bound):\n",
    "        feature_mins = []\n",
    "        feature_maxs = []\n",
    "        num = 10\n",
    "        for i in range(0, data.shape[1], num):\n",
    "            print('on ' + str(i))\n",
    "            upper = i + num\n",
    "            if upper > data.shape[1]:\n",
    "                upper = data.shape[1]\n",
    "            np_data = data[sample_lower_bound:sample_upper_bound,i:upper,:]\n",
    "            np_data = np.nan_to_num(np_data)\n",
    "            tmp_min = np.min(np_data, axis=(0,2))\n",
    "            tmp_max = np.max(np_data, axis=(0,2))\n",
    "            feature_mins.extend(tmp_min)\n",
    "            feature_maxs.extend(tmp_max)\n",
    "        return feature_mins, feature_maxs \n",
    "\n",
    "    def get_y_as_cat(self, y_df):\n",
    "        #convert the labels to encoded values\n",
    "        labels = y_df[self.label].unique()\n",
    "        if 'Low' in labels:\n",
    "            labels = ['Low', 'Moderate', 'Considerable', 'High']\n",
    "        elif 'Low_Falling' in labels:\n",
    "            lables = ['Low_Falling', 'Low_Flat', \n",
    "                      'Moderate_Falling', 'Moderate_Flat', 'Moderate_Rising', \n",
    "                      'Considerable_Falling', 'Considerable_Flat', 'Considerable_Rising',\n",
    "                      'High_Falling', 'High_Flat', 'High_Rising']                    \n",
    "        else:\n",
    "            labels.sort()\n",
    "        cat_type = CategoricalDtype(categories=labels, ordered=True)\n",
    "        y_df[self.label + '_Cat'] = y_df[self.label].astype(cat_type)\n",
    "        y = y_df[self.label + '_Cat'].cat.codes.values\n",
    "\n",
    "        cat_dict = dict( enumerate(y_df[self.label + '_Cat'].cat.categories ) )\n",
    "        return y, cat_dict\n",
    "    \n",
    "    def filter_features(self, feature_list, only_var=False):\n",
    "        #remove any prefixed with var\n",
    "        feature_list = set([x for x in feature_list if 'var' not in x])\n",
    "        feature_list = set([x for x in feature_list if 'ABSV' not in x])\n",
    "        feature_list = set([x for x in feature_list if 'CLMR' not in x])\n",
    "        feature_list = set([x for x in feature_list if 'HGT' not in x])\n",
    "        feature_list = set([x for x in feature_list if 'PRES' not in x])\n",
    "        feature_list = set([x for x in feature_list if 'CPOFP' not in x])\n",
    "        feature_list = set([x for x in feature_list if 'HLCY' not in x])\n",
    "        feature_list = set([x for x in feature_list if 'PV_EQ' not in x])\n",
    "        feature_list = set([x for x in feature_list if 'ICEC' not in x])\n",
    "        feature_list = set([x for x in feature_list if 'LAND' not in x])\n",
    "        feature_list = set([x for x in feature_list if 'SPFH' not in x])\n",
    "        feature_list = set([x for x in feature_list if 'DPT' not in x])\n",
    "        feature_list = set([x for x in feature_list if 'TCDC' not in x])\n",
    "        feature_list = set([x for x in feature_list if 'HINDEX' not in x])\n",
    "        feature_list = set([x for x in feature_list if 'mabovemeansealevel' not in x])\n",
    "        feature_list = set([x for x in feature_list if 'tropopause' not in x])\n",
    "        if not only_var:\n",
    "            #operate only on avg and sum\n",
    "            feature_list = set([x for x in feature_list if 'min' not in x])\n",
    "            feature_list = set([x for x in feature_list if 'max' not in x])\n",
    "\n",
    "            #get cloud cover\n",
    "            cloud_features = set([x for x in feature_list if 'TCDC' in x])\n",
    "            feature_list -= cloud_features\n",
    "            \n",
    "            #get any surface levels\n",
    "            surface_features = set([x for x in feature_list if 'surface' in x])\n",
    "            feature_list = feature_list - surface_features            \n",
    "\n",
    "            #get all wind\n",
    "            ugrd_wind_features = set([x for x in feature_list if 'UGRD' in x])\n",
    "            vgrd_wind_features = set([x for x in feature_list if 'VGRD' in x])\n",
    "            wind_features = set(list(ugrd_wind_features) + list(vgrd_wind_features))\n",
    "            feature_list -= wind_features\n",
    "            wind_features = set([x for x in wind_features if 'aboveground' not in x])\n",
    "            \n",
    "            #get any relative to ground (except wind)\n",
    "            aboveground_features = set([x for x in feature_list if 'aboveground' in x])\n",
    "            feature_list = feature_list - aboveground_features\n",
    "            \n",
    "            feature_prefix = set([x.split('_')[0] for x in feature_list])\n",
    "            pressure_features = []\n",
    "            for p in feature_prefix:\n",
    "                #get a random subset for the features defined by pressure level\n",
    "                tmp = [x for x in feature_list if p in x]\n",
    "                tmp2 = pd.Series(tmp).sample(frac=.1)\n",
    "                #return tmp2\n",
    "                pressure_features.extend(list(tmp2))\n",
    "        \n",
    "            l = list(surface_features) + list(aboveground_features) + list(pressure_features) + list(cloud_features) + list(wind_features)\n",
    "            \n",
    "            feature_list = l\n",
    "        feature_list = list(feature_list)\n",
    "        feature_list.sort()\n",
    "        return feature_list\n",
    "    \n",
    "    def create_dls(self, X, y, feature_mins, feature_maxs, feature_indexes = None, num_test_files = 11, num_days=28, splits=None, sample_frac = 1):\n",
    "        #index file which indicates which rows in X are train or test\n",
    "        #be carful these don't overlap\n",
    "        num_test = num_test_files * 3000\n",
    "        train_test_split = y.shape[0]-num_test\n",
    "        #can use a smaller train subset to make development faster\n",
    "        if splits == None:\n",
    "            splits_2 = (L(list(pd.Series([i for i in range(0,train_test_split)]).sample(frac=sample_frac).values)).shuffle(), L(list(pd.Series([i for i in range(train_test_split,train_test_split+num_test)]).sample(frac=sample_frac).values)).shuffle())\n",
    "        else:\n",
    "            splits_2 = (L(list(pd.Series(splits[0]).sample(frac=sample_frac).values)), L(list(pd.Series(splits[1]).sample(frac=sample_frac).values)))\n",
    "        #splits_2 = (L([i for i in range(0,train_test_split)]), L([i for i in range(train_test_split,train_test_split+num_test)]))\n",
    "        #splits_3 = (L([i for i in range(0*64,12*64)]), L([i for i in range(train_test_split,train_test_split+8*64)]))\n",
    "        #create the dataset\n",
    "        tfms = [None, [Categorize()]]\n",
    "        dsets = TSDatasets(X, y, tfms=tfms, splits=splits_2, inplace=False)\n",
    "        \n",
    "        #create the dataloader\n",
    "        #batch_tfms = [TSStandardize(mean=torch.tensor(feature_mean), std=torch.tensor(feature_std), by_var=True), \n",
    "        #              Nan2Value()]\n",
    "        #batch_tfms = [TSFilter(filtered_feature_indexes), TSSimpleStandardize(mean=np.array([feature_means[x] for x in filtered_feature_indexes]).astype(np.float32), std=np.array([feature_stds[x] for x in filtered_feature_indexes]).astype(np.float32)), Nan2Value()]\n",
    "        #m = (feature_means[filtered_feature_indexes]).astype(np.float32)\n",
    "        #s = (feature_stds[filtered_feature_indexes]).astype(np.float32)\n",
    "        mins = feature_mins.astype(np.float32)\n",
    "        maxs = feature_maxs.astype(np.float32)\n",
    "        #batch_tfms = [TSFilter(), TSSimpleStandardize(mean=m, std=s), Nan2Value()]\n",
    "        batch_tfms = [TSFilter(), TSSimpleNormalize(mins=mins, maxs=maxs), Nan2Value()]\n",
    "        #batch_tfms = [TSFilter(filtered_feature_indexes, days=num_days), TSSimpleStandardize(mean=np.array(feature_means).astype(np.float32), std=np.array(feature_stds).astype(np.float32)), Nan2Value()]\n",
    "        dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64], batch_tfms=batch_tfms, num_workers=4, inplace=False)\n",
    "        return splits_2, dls\n",
    "    \n",
    "    def create_different_splits(self, y_df, valid_season = '18-19'):\n",
    "        a = y_df[y_df['season']!=valid_season].index\n",
    "        b = y_df[y_df['season']==valid_season].index\n",
    "        splits = (L(list(pd.Series(a))).shuffle(), L(list(pd.Series(b))).shuffle())\n",
    "        return splits\n",
    "    \n",
    "    \n",
    "    def augment_labels_with_trends(self, all_label_file, labels, label_to_add_trend_info='Day1DangerAboveTreeline'):\n",
    "        #note this removes labels from labels, ensure you reindex X and reset the labels index after running this  \n",
    "        #add extra labels which also allow us to have labels which indicate the trend in the avy direction\n",
    "        #the thought here is that predicting a rise or flat danger is usually easier than predicting when \n",
    "        #to lower the danger so seperating these in to seperate clases\n",
    "       \n",
    "        all_labels = pd.read_csv(all_label_file, low_memory=False,\n",
    "                        dtype={'Day1Danger_OctagonAboveTreelineEast': 'object',\n",
    "                                'Day1Danger_OctagonAboveTreelineNorth': 'object',\n",
    "                                'Day1Danger_OctagonAboveTreelineNorthEast': 'object',\n",
    "                                'Day1Danger_OctagonAboveTreelineNorthWest': 'object',\n",
    "                                'Day1Danger_OctagonAboveTreelineSouth': 'object',\n",
    "                                'Day1Danger_OctagonAboveTreelineSouthEast': 'object',\n",
    "                                'Day1Danger_OctagonAboveTreelineSouthWest': 'object',\n",
    "                                'Day1Danger_OctagonAboveTreelineWest': 'object',\n",
    "                                'Day1Danger_OctagonBelowTreelineEast': 'object',\n",
    "                                'Day1Danger_OctagonBelowTreelineNorth': 'object',\n",
    "                                'Day1Danger_OctagonBelowTreelineNorthEast': 'object',\n",
    "                                'Day1Danger_OctagonBelowTreelineNorthWest': 'object',\n",
    "                                'Day1Danger_OctagonBelowTreelineSouth': 'object',\n",
    "                                'Day1Danger_OctagonBelowTreelineSouthEast': 'object',\n",
    "                                'Day1Danger_OctagonBelowTreelineSouthWest': 'object',\n",
    "                                'Day1Danger_OctagonBelowTreelineWest': 'object',\n",
    "                                'Day1Danger_OctagonNearTreelineEast': 'object',\n",
    "                                'Day1Danger_OctagonNearTreelineNorth': 'object',\n",
    "                                'Day1Danger_OctagonNearTreelineNorthEast': 'object',\n",
    "                                'Day1Danger_OctagonNearTreelineNorthWest': 'object',\n",
    "                                'Day1Danger_OctagonNearTreelineSouth': 'object',\n",
    "                                'Day1Danger_OctagonNearTreelineSouthEast': 'object',\n",
    "                                'Day1Danger_OctagonNearTreelineSouthWest': 'object',\n",
    "                                'Day1Danger_OctagonNearTreelineWest': 'object',\n",
    "                                'SpecialStatement': 'object',\n",
    "                                'image_paths': 'object',\n",
    "                                'image_types': 'object',\n",
    "                                'image_urls': 'object'})\n",
    "        \n",
    "        all_labels = all_labels[all_labels[label_to_add_trend_info] != 'no-data']\n",
    "        all_labels['parsed_date'] = pd.to_datetime(all_labels['Day1Date'], format='%Y%m%d')\n",
    "        \n",
    "        #ensure we are only using label data for regions we are looking at\n",
    "        #return region_zones\n",
    "        all_labels['UnifiedRegion'] = all_labels.apply(lambda x : PrepML.lookup_forecast_region(x['UnifiedRegion']), axis=1)        \n",
    "                              \n",
    "        all_labels = all_labels[all_labels['UnifiedRegion']!='Unknown region']\n",
    "        \n",
    "        #add a season column\n",
    "        tmp = pd.DataFrame.from_records(all_labels['parsed_date'].apply(PrepML.date_to_season).reset_index(drop=True))\n",
    "        all_labels.reset_index(drop=True, inplace=True)\n",
    "        all_labels['season'] = tmp[1]\n",
    "        \n",
    "        labels_trends = pd.DataFrame()\n",
    "        for r in labels['UnifiedRegion'].unique():\n",
    "            region_df = all_labels[all_labels['UnifiedRegion']==r]\n",
    "            season_labels = labels[labels['UnifiedRegion']==r]\n",
    "            for s in labels['season'].unique():                \n",
    "                region_season_df = region_df[region_df['season']==s]\n",
    "                region_season_labels = season_labels[season_labels['season']==s]\n",
    "                for i, row in region_season_labels.iterrows():\n",
    "                    d = row['parsed_date']                    \n",
    "\n",
    "                    prev_label_row = region_season_df[region_season_df['parsed_date'] == d - pd.Timedelta(days=1)]\n",
    "                    if(len(prev_label_row) == 0):\n",
    "                        #print('Couldnt find prev for ' + r + ' ' + s + ' ' + str(d) + ' len ' + str(len(prev_label_row)) + ' ' + str(region_season_df['parsed_date'] - pd.Timedelta(days=1)))\n",
    "                        continue\n",
    "                    lookup = {'Low':0, 'Moderate':1, 'Considerable':2, 'High':3, 'Extreme': 4}\n",
    "                    prev = lookup[prev_label_row[label_to_add_trend_info].iloc[0]]\n",
    "                    cur = lookup[row[label_to_add_trend_info]]\n",
    "                    trend = '_Unknown'\n",
    "                    if prev == cur:\n",
    "                        trend = '_Flat'\n",
    "                    elif prev < cur:\n",
    "                        trend = '_Rising'\n",
    "                    elif prev >  cur:\n",
    "                        trend = '_Falling'\n",
    "\n",
    "                    labels.loc[i, label_to_add_trend_info + 'WithTrend'] = row[label_to_add_trend_info] + trend\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:timeseriesai]",
   "language": "python",
   "name": "conda-env-timeseriesai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
