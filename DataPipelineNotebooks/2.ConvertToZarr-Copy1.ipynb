{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp convert_to_zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook which converts per region netCDF files to Zarr files to make them more efficient when indexing\n",
    "\n",
    "### uses pangeo_small environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ignore\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import xarray as xr\n",
    "import zarr\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import os\n",
    "from numcodecs import Blosc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the parameters here\n",
    "### Ensure all State and Regions you want to transform are specified here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ignore\n",
    "data_root = '/media/scottcha/Data2/OAPMLData/'\n",
    "output_root = '/media/scottcha/E1/Data/OAPMLData/Temp2'\n",
    "interpolation = 1\n",
    "\n",
    "#currently only have Washington regions and one season specified for the tutorial\n",
    "#uncomment regions and seasons if doing a larger transform\n",
    "regions = {#'Utah': ['Abajos', 'Logan', 'Moab', 'Ogden', 'Provo', \n",
    "           #'Salt Lake', 'Skyline', 'Uintas'],  \n",
    "           #'Colorado': ['Grand Mesa Zone', 'Sangre de Cristo Range', 'Steamboat Zone', 'Front Range Zone',\n",
    "           #'Vail Summit Zone', 'Sawatch Zone', 'Aspen Zone', \n",
    "           #'North San Juan Mountains', 'South San Juan Mountains', 'Gunnison Zone'],\n",
    "           #'Washington': ['Mt Hood', 'Olympics', 'Snoqualmie Pass', 'Stevens Pass',\n",
    "           #'WA Cascades East, Central', 'WA Cascades East, North', 'WA Cascades East, South',\n",
    "           #'WA Cascades West, Central', 'WA Cascades West, Mt Baker', 'WA Cascades West, South'\n",
    "           #]           \n",
    "           'Colorado': [\"Aspen Zone\",]#\"Northwest Coastal\", \"Northwest Inland\", \"Sea To Sky\", \n",
    "                       #\"South Coast Inland\", \"South Coast\", \"North Rockies\", \n",
    "                       #\"Cariboos\", \"North Columbia\", \"South Columbia\", \"Purcells\", \n",
    "                       #\"Kootenay Boundary\", \"South Rockies\", \"Lizard Range and Flathead\", \n",
    "                       #\"Vancouver Island\", \"Kananaskis Country, Alberta Parks\", \"Chic Chocs, Avalanche Quebec\",\n",
    "                       #\"Little Yoho\", \"Banff, Yoho and Kootenay National Parks\", \"Glacier National Park\",\n",
    "                       #\"Waterton Lakes National Park\", \"Jasper National Park\"]\n",
    "           }\n",
    "seasons = ['19-20']#, '20-21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/media/scottcha/Data2/OAPMLData/3.GFSFiltered1xInterpolation1d/19-20/Region_Aspen Zone_20200103.nc', chunks={'latitude':1, 'longitude':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:37659</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>12</li>\n",
       "  <li><b>Cores: </b>24</li>\n",
       "  <li><b>Memory: </b>135.10 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:37659' processes=12 threads=24, memory=135.10 GB>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(n_workers=12)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds4 = xr.open_mfdataset('/media/scottcha/Data2/OAPMLData/3.GFSFiltered1xInterpolation3h/19-20/Region_Little Yoho_2020010[1-2].nc', combine='by_coords', chunks={'latitude':1, 'longitude':1, 'time':1}, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Frozen(SortedKeysDict({'time': (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), 'latitude': (1,), 'longitude': (1,)}))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds4.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds4 = ds4.to_array(name='vars').chunk({'time':8, 'latitude':1, 'longitude':1, 'variable':-1}).to_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Frozen(SortedKeysDict({'variable': (1641,), 'time': (8, 8), 'latitude': (1,), 'longitude': (1,)}))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds4.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x7f16479af710>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds4.to_zarr('/media/scottcha/E1/Data/OAPMLData/Temp2/Little_yoho2.zarr', consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.to_array(name='vars').chunk({'time':7, 'latitude':4, 'longitude':4, 'variable':-1}).to_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:    (latitude: 2, longitude: 4, time: 1, variable: 1641)\n",
       "Coordinates:\n",
       "  * time       (time) datetime64[ns] 2020-01-03\n",
       "  * latitude   (latitude) float64 39.25 39.5\n",
       "  * longitude  (longitude) float64 -107.2 -107.0 -106.8 -106.5\n",
       "  * variable   (variable) &lt;U47 &#x27;CLMR_1hybridlevel_min&#x27; ... &#x27;WATR_surface_sum&#x27;\n",
       "Data variables:\n",
       "    vars       (variable, time, latitude, longitude) float64 dask.array&lt;chunksize=(1641, 1, 2, 4), meta=np.ndarray&gt;</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (latitude: 2, longitude: 4, time: 1, variable: 1641)\n",
       "Coordinates:\n",
       "  * time       (time) datetime64[ns] 2020-01-03\n",
       "  * latitude   (latitude) float64 39.25 39.5\n",
       "  * longitude  (longitude) float64 -107.2 -107.0 -106.8 -106.5\n",
       "  * variable   (variable) <U47 'CLMR_1hybridlevel_min' ... 'WATR_surface_sum'\n",
       "Data variables:\n",
       "    vars       (variable, time, latitude, longitude) float64 dask.array<chunksize=(1641, 1, 2, 4), meta=np.ndarray>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:    (latitude: 2, longitude: 4, time: 1, variable: 1641)\n",
       "Coordinates:\n",
       "  * time       (time) datetime64[ns] 2020-01-02\n",
       "  * latitude   (latitude) float64 39.25 39.5\n",
       "  * longitude  (longitude) float64 -107.2 -107.0 -106.8 -106.5\n",
       "  * variable   (variable) &lt;U47 &#x27;CLMR_1hybridlevel_min&#x27; ... &#x27;WATR_surface_sum&#x27;\n",
       "Data variables:\n",
       "    vars       (variable, time, latitude, longitude) float64 dask.array&lt;chunksize=(1, 1, 2, 4), meta=np.ndarray&gt;</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (latitude: 2, longitude: 4, time: 1, variable: 1641)\n",
       "Coordinates:\n",
       "  * time       (time) datetime64[ns] 2020-01-02\n",
       "  * latitude   (latitude) float64 39.25 39.5\n",
       "  * longitude  (longitude) float64 -107.2 -107.0 -106.8 -106.5\n",
       "  * variable   (variable) <U47 'CLMR_1hybridlevel_min' ... 'WATR_surface_sum'\n",
       "Data variables:\n",
       "    vars       (variable, time, latitude, longitude) float64 dask.array<chunksize=(1, 1, 2, 4), meta=np.ndarray>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2 = ds.to_array(name='vars').chunk({'time':1, 'latitude':-1, 'longitude':-1}).to_dataset()\n",
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3 = ds.chunk({'time':1, 'latitude':1, 'longitude':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE)\n",
    "enc = {x: {\"compressor\": compressor} for x in ds1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x7fa91810bfb0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ds1.to_zarr('/media/scottcha/E1/Data/OAPMLData/Temp2/Aspen.zarr', consolidated=True, encoding=enc)\n",
    "ds1.to_zarr('/media/scottcha/E1/Data/OAPMLData/Temp2/Aspen.zarr', consolidated=True, append_dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:    (latitude: 2, longitude: 4, time: 1, variable: 1641)\n",
       "Coordinates:\n",
       "  * time       (time) datetime64[ns] 2020-01-02\n",
       "  * latitude   (latitude) float64 39.25 39.5\n",
       "  * longitude  (longitude) float64 -107.2 -107.0 -106.8 -106.5\n",
       "  * variable   (variable) &lt;U47 &#x27;CLMR_1hybridlevel_min&#x27; ... &#x27;WATR_surface_sum&#x27;\n",
       "Data variables:\n",
       "    vars       (variable, time, latitude, longitude) float64 dask.array&lt;chunksize=(1641, 1, 1, 1), meta=np.ndarray&gt;</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (latitude: 2, longitude: 4, time: 1, variable: 1641)\n",
       "Coordinates:\n",
       "  * time       (time) datetime64[ns] 2020-01-02\n",
       "  * latitude   (latitude) float64 39.25 39.5\n",
       "  * longitude  (longitude) float64 -107.2 -107.0 -106.8 -106.5\n",
       "  * variable   (variable) <U47 'CLMR_1hybridlevel_min' ... 'WATR_surface_sum'\n",
       "Data variables:\n",
       "    vars       (variable, time, latitude, longitude) float64 dask.array<chunksize=(1641, 1, 1, 1), meta=np.ndarray>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'variable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/src/xarray/xarray/core/dataset.py\u001b[0m in \u001b[0;36m_construct_dataarray\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m             \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'variable'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-39c0656e130b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.to_zarr(zarr_path, consolidated=True, encoding=enc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/src/xarray/xarray/core/dataset.py\u001b[0m in \u001b[0;36msortby\u001b[0;34m(self, variables, ascending)\u001b[0m\n\u001b[1;32m   4995\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4996\u001b[0m             \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4997\u001b[0;31m         \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4998\u001b[0m         \u001b[0maligned_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4999\u001b[0m         \u001b[0maligned_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maligned_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/xarray/xarray/core/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   4995\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4996\u001b[0m             \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4997\u001b[0;31m         \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4998\u001b[0m         \u001b[0maligned_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4999\u001b[0m         \u001b[0maligned_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maligned_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/xarray/xarray/core/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_dataarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy_listed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/xarray/xarray/core/dataset.py\u001b[0m in \u001b[0;36m_construct_dataarray\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m             _, name, variable = _get_virtual_variable(\n\u001b[0;32m-> 1162\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_level_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1163\u001b[0m             )\n\u001b[1;32m   1164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/xarray/xarray/core/dataset.py\u001b[0m in \u001b[0;36m_get_virtual_variable\u001b[0;34m(variables, key, level_vars, dim_sizes)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mref_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_index_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mref_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mref_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvar_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'variable'"
     ]
    }
   ],
   "source": [
    "ds.sortby('variable')#.to_zarr(zarr_path, consolidated=True, encoding=enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvertToZarr:\n",
    "    \"\"\"\n",
    "    Class which encapsulates the logic to convert a set of filtered netCDF files to Zarr\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seasons, regions, data_root, interpolate=1, resample_length='1d'):\n",
    "        \"\"\"\n",
    "        Initialize the class\n",
    "        \n",
    "        Keyword Arguments\n",
    "        seasons: list of season values to process\n",
    "        regions: dictonary of Key: State and Value: List of Regions to process for that state\n",
    "        data_root: the root path of the data folders which contains the 3.GFSFiltered1xInterpolation\n",
    "        interpolate: the amount of interpolation applied in in the previous ParseGFS notebook (used for finding the correct input/output paths)\n",
    "        \"\"\"\n",
    "        self.processed_path = data_root + '/3.GFSFiltered'+ str(interpolate) + 'xInterpolation' + resample_length + '/'\n",
    "        self.zarr_base_path = output_root + '/4.GFSFiltered'+ str(interpolate) + 'xInterpolationZarr' + resample_length + '/'\n",
    "        \n",
    "        self.seasons = seasons\n",
    "        self.regions = regions\n",
    "        self.data_root = data_root\n",
    "        self.compressor = Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE)\n",
    "        \n",
    "        if not os.path.exists(self.zarr_base_path):\n",
    "            os.makedirs(self.zarr_base_path)\n",
    "    \n",
    "    def compute_region(self, region_name, season, state):\n",
    "        \"\"\"\n",
    "        Calculates the zarr conversion for a specific region, season and state and indexes it for efficient lookup \n",
    "        \n",
    "        Keyword Arguments\n",
    "        region_name: name of the region to process\n",
    "        season: season to process\n",
    "        state: state to process (region must be a part of the state)\n",
    "        \"\"\"\n",
    "        first = True\n",
    "        base_path = self.processed_path + season + '/' + '/Region_' + region_name \n",
    "        zarr_path = self.zarr_base_path + season + '/' + state + '/Region_' + region_name + '.zarr'\n",
    "        \n",
    "        #TODO: refactor these to be shared code as logic also exists in ParseGFS\n",
    "        p = 181\n",
    "        if season in ['15-16', '19-20']:\n",
    "            p = 182 #leap years\n",
    "        \n",
    "        if season == '15-16':\n",
    "            snow_start_date = '2015-11-01'\n",
    "        elif season == '16-17':\n",
    "            snow_start_date = '2016-11-01'\n",
    "        elif season == '17-18':\n",
    "            snow_start_date = '2017-11-01'\n",
    "        elif season == '18-19':\n",
    "            snow_start_date = '2018-11-01'\n",
    "        elif season == '19-20':\n",
    "            snow_start_date = '2019-11-01'\n",
    "        elif season == '20-21':\n",
    "            snow_start_date = '2020-11-01'\n",
    "        else:\n",
    "            raise Exception('No known season ' + season)\n",
    "\n",
    "        date_values_pd = pd.date_range(snow_start_date, periods=p, freq=\"D\")\n",
    "        try:\n",
    "            with xr.open_zarr(zarr_path) as z:\n",
    "                if z.time.values[-1] == date_values_pd[-1]:\n",
    "                    print(' already exists: ' + region_name + ' ' + season + ' ' + state)\n",
    "                    z.close()\n",
    "                    return\n",
    "                else:\n",
    "                    #already exists but incomplete\n",
    "                    date_values_pd = [pd.Timestamp(v) for v in date_values_pd.values.astype('datetime64[ns]') if v not in z.time.values]\n",
    "                    print(' some exist but have to complete ' + str(len(date_values_pd)))\n",
    "                    first = False\n",
    "        except ValueError as err:\n",
    "            #ignore as it doesn't exist yet\n",
    "            pass\n",
    "        \n",
    "        #sometimes vars get added, filter to only the list of vars in the first dataset for that region\n",
    "        #TODO: handle the case where the first dataset has more vars than subsequent ones\n",
    "        final_vars = None\n",
    "        last_ds = xr.open_dataset(base_path + '_' + date_values_pd[-1].strftime('%Y%m%d') + '.nc')\n",
    "        \n",
    "        last_ds = last_ds.to_array(name='vars').chunk({'time':-1, 'latitude':1, 'longitude':1, 'variable':-1}).to_dataset()\n",
    "        last_vars = list(last_ds.variable.values)\n",
    "        \n",
    "        for d in date_values_pd:\n",
    "\n",
    "            path =  base_path + '_' + d.strftime('%Y%m%d') + '.nc'\n",
    "            print('On ' + str(path.split('/')[-1]))\n",
    "\n",
    "            try:\n",
    "                ds = xr.open_dataset(path, chunks={'latitude':1, 'longitude':1})\n",
    "            except OSError as err:\n",
    "                print(' missing file: ' + path)\n",
    "                continue\n",
    "\n",
    "            ds = ds.to_array(name='vars').chunk({'time':-1, 'latitude':1, 'longitude':1, 'variable':-1}).to_dataset()\n",
    "\n",
    "\n",
    "            if first:\n",
    "                #find intersection of the first and last variables to try to ensure\n",
    "                #that we are using only the intersection\n",
    "                final_vars = [v for v in list(ds.variable.values) if v in last_vars]\n",
    "                ds = ds.sel(variable=ds.variable.isin(final_vars))\n",
    "                enc = {x: {\"compressor\": self.compressor} for x in ds}\n",
    "                ds.sortby('variable').to_zarr(zarr_path, consolidated=True, encoding=enc)\n",
    "                first=False\n",
    "            else:\n",
    "                assert(final_vars is not None)\n",
    "                ds = ds.sel(variable=ds.variable.isin(final_vars))                \n",
    "                ds.sortby('variable').to_zarr(zarr_path, consolidated=True, append_dim='time')\n",
    "\n",
    "\n",
    "    def process_tuple(self, t): \n",
    "        \"\"\"\n",
    "        Entry method to call compute_region with a tuple\n",
    "        Basically a helper for executing with joblib parallel\n",
    "        \n",
    "        Keyword Arguments\n",
    "        t: the tuple containing the region, season and state\n",
    "        \"\"\"\n",
    "        self.compute_region(t[0], t[1], t[2])\n",
    "    \n",
    "    def make_list(self):\n",
    "        \"\"\"\n",
    "        Helper method to make the list of values to process\n",
    "        \"\"\"\n",
    "        to_process = []\n",
    "        for s in self.seasons:\n",
    "            for state in self.regions.keys():           \n",
    "                for r in self.regions[state]:\n",
    "                    to_process.append((r,s,state))\n",
    "        return to_process\n",
    "    \n",
    "    def convert_local(self, jobs=15):\n",
    "        l = self.make_list()\n",
    "    \n",
    "        #one state & season takes about 6 hours with 15 cores on my machine\n",
    "        Parallel(n_jobs=jobs, backend=\"multiprocessing\")(map(delayed(self.process_tuple), l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmppath =  '/media/scottcha/E1/Data/OAPMLData/3.GFSFiltered1xInterpolation/19-20/Region_Mt Hood_20191120.nc'\n",
    "#ds = xr.open_dataset(tmppath, chunks={'latitude':1, 'longitude':1})\n",
    "#ds = ds.to_array(name='vars').chunk({'time':1, 'latitude':1, 'longitude':1, 'variable':-1}).to_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_vars = list(ds.variable.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp_ds = ds.sel(variable=ds.variable.isin(final_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ignore\n",
    "ctz = ConvertToZarr(seasons, regions, data_root, resample_length='1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Region_Aspen Zone_20191101.nc\n",
      "On Region_Aspen Zone_20191102.nc\n",
      "On Region_Aspen Zone_20191103.nc\n",
      "On Region_Aspen Zone_20191104.nc\n",
      "On Region_Aspen Zone_20191105.nc\n",
      "On Region_Aspen Zone_20191106.nc\n",
      "On Region_Aspen Zone_20191107.nc\n",
      "On Region_Aspen Zone_20191108.nc\n",
      "On Region_Aspen Zone_20191109.nc\n",
      "On Region_Aspen Zone_20191110.nc\n",
      "On Region_Aspen Zone_20191111.nc\n",
      "On Region_Aspen Zone_20191112.nc\n",
      "On Region_Aspen Zone_20191113.nc\n",
      "On Region_Aspen Zone_20191114.nc\n",
      "On Region_Aspen Zone_20191115.nc\n",
      "On Region_Aspen Zone_20191116.nc\n",
      "On Region_Aspen Zone_20191117.nc\n",
      "On Region_Aspen Zone_20191118.nc\n",
      "On Region_Aspen Zone_20191119.nc\n",
      "On Region_Aspen Zone_20191120.nc\n",
      "On Region_Aspen Zone_20191121.nc\n",
      "On Region_Aspen Zone_20191122.nc\n",
      "On Region_Aspen Zone_20191123.nc\n",
      "On Region_Aspen Zone_20191124.nc\n",
      "On Region_Aspen Zone_20191125.nc\n",
      "On Region_Aspen Zone_20191126.nc\n",
      "On Region_Aspen Zone_20191127.nc\n",
      "On Region_Aspen Zone_20191128.nc\n",
      "On Region_Aspen Zone_20191129.nc\n",
      "On Region_Aspen Zone_20191130.nc\n",
      "On Region_Aspen Zone_20191201.nc\n",
      "On Region_Aspen Zone_20191202.nc\n",
      "On Region_Aspen Zone_20191203.nc\n",
      "On Region_Aspen Zone_20191204.nc\n",
      "On Region_Aspen Zone_20191205.nc\n",
      "On Region_Aspen Zone_20191206.nc\n",
      "On Region_Aspen Zone_20191207.nc\n",
      "On Region_Aspen Zone_20191208.nc\n",
      "On Region_Aspen Zone_20191209.nc\n",
      "On Region_Aspen Zone_20191210.nc\n",
      "On Region_Aspen Zone_20191211.nc\n",
      "On Region_Aspen Zone_20191212.nc\n",
      "On Region_Aspen Zone_20191213.nc\n",
      "On Region_Aspen Zone_20191214.nc\n",
      "On Region_Aspen Zone_20191215.nc\n",
      "On Region_Aspen Zone_20191216.nc\n",
      "On Region_Aspen Zone_20191217.nc\n",
      "On Region_Aspen Zone_20191218.nc\n",
      "On Region_Aspen Zone_20191219.nc\n",
      "On Region_Aspen Zone_20191220.nc\n",
      "On Region_Aspen Zone_20191221.nc\n",
      "On Region_Aspen Zone_20191222.nc\n",
      "On Region_Aspen Zone_20191223.nc\n",
      "On Region_Aspen Zone_20191224.nc\n",
      "On Region_Aspen Zone_20191225.nc\n",
      "On Region_Aspen Zone_20191226.nc\n",
      "On Region_Aspen Zone_20191227.nc\n",
      "On Region_Aspen Zone_20191228.nc\n",
      "On Region_Aspen Zone_20191229.nc\n",
      "On Region_Aspen Zone_20191230.nc\n",
      "On Region_Aspen Zone_20191231.nc\n",
      "On Region_Aspen Zone_20200101.nc\n",
      "On Region_Aspen Zone_20200102.nc\n",
      "On Region_Aspen Zone_20200103.nc\n",
      "On Region_Aspen Zone_20200104.nc\n",
      "On Region_Aspen Zone_20200105.nc\n",
      "On Region_Aspen Zone_20200106.nc\n",
      "On Region_Aspen Zone_20200107.nc\n",
      "On Region_Aspen Zone_20200108.nc\n",
      "On Region_Aspen Zone_20200109.nc\n",
      "On Region_Aspen Zone_20200110.nc\n",
      "On Region_Aspen Zone_20200111.nc\n",
      "On Region_Aspen Zone_20200112.nc\n",
      "On Region_Aspen Zone_20200113.nc\n",
      "On Region_Aspen Zone_20200114.nc\n",
      "On Region_Aspen Zone_20200115.nc\n",
      "On Region_Aspen Zone_20200116.nc\n",
      "On Region_Aspen Zone_20200117.nc\n",
      "On Region_Aspen Zone_20200118.nc\n",
      "On Region_Aspen Zone_20200119.nc\n",
      "On Region_Aspen Zone_20200120.nc\n",
      "On Region_Aspen Zone_20200121.nc\n",
      "On Region_Aspen Zone_20200122.nc\n",
      "On Region_Aspen Zone_20200123.nc\n",
      "On Region_Aspen Zone_20200124.nc\n",
      "On Region_Aspen Zone_20200125.nc\n",
      "On Region_Aspen Zone_20200126.nc\n",
      "On Region_Aspen Zone_20200127.nc\n",
      "On Region_Aspen Zone_20200128.nc\n",
      "On Region_Aspen Zone_20200129.nc\n",
      "On Region_Aspen Zone_20200130.nc\n",
      "On Region_Aspen Zone_20200131.nc\n",
      "On Region_Aspen Zone_20200201.nc\n",
      "On Region_Aspen Zone_20200202.nc\n",
      "On Region_Aspen Zone_20200203.nc\n",
      "On Region_Aspen Zone_20200204.nc\n",
      "On Region_Aspen Zone_20200205.nc\n",
      "On Region_Aspen Zone_20200206.nc\n",
      "On Region_Aspen Zone_20200207.nc\n",
      "On Region_Aspen Zone_20200208.nc\n",
      "On Region_Aspen Zone_20200209.nc\n",
      "On Region_Aspen Zone_20200210.nc\n",
      "On Region_Aspen Zone_20200211.nc\n",
      "On Region_Aspen Zone_20200212.nc\n",
      "On Region_Aspen Zone_20200213.nc\n",
      "On Region_Aspen Zone_20200214.nc\n",
      "On Region_Aspen Zone_20200215.nc\n",
      "On Region_Aspen Zone_20200216.nc\n",
      "On Region_Aspen Zone_20200217.nc\n",
      "On Region_Aspen Zone_20200218.nc\n",
      "On Region_Aspen Zone_20200219.nc\n",
      "On Region_Aspen Zone_20200220.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-66:\n",
      "Process ForkPoolWorker-69:\n",
      "Process ForkPoolWorker-78:\n",
      "Process ForkPoolWorker-76:\n",
      "Process ForkPoolWorker-74:\n",
      "Process ForkPoolWorker-80:\n",
      "Process ForkPoolWorker-68:\n",
      "Process ForkPoolWorker-62:\n",
      "Process ForkPoolWorker-75:\n",
      "Process ForkPoolWorker-71:\n",
      "Process ForkPoolWorker-70:\n",
      "Process ForkPoolWorker-63:\n",
      "Process ForkPoolWorker-65:\n",
      "Process ForkPoolWorker-77:\n",
      "Process ForkPoolWorker-73:\n",
      "Process ForkPoolWorker-79:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-67:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/scottcha/miniconda3/envs/pangeo_small3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/scottcha/miniconda3/envs/pangeo_small3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/scottcha/miniconda3/envs/pangeo_small3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/scottcha/miniconda3/envs/pangeo_small3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/scottcha/miniconda3/envs/pangeo_small3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Wall time: 6min 13s\n",
    "\n",
    "ctz.convert_local(jobs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstmp = xr.Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `dstmp.to_zarr()` not found.\n"
     ]
    }
   ],
   "source": [
    "?dstmp.to_zarr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo_small3]",
   "language": "python",
   "name": "conda-env-pangeo_small3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
