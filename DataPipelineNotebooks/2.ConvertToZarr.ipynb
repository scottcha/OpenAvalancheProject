{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: 2.converttozarr.html\n",
    "title: 2.ConvertToZarr\n",
    "subtitle: Notebook which converts per region netCDF files to Zarr files to make them\n",
    "  more efficient when indexing\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp convert_to_zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| test_ignore\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import xarray as xr\n",
    "import zarr\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import os\n",
    "from numcodecs import Blosc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the parameters here\n",
    "### Ensure all State and Regions you want to transform are specified here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| test_ignore\n",
    "data_root = 'D:/OAPMLData/'\n",
    "output_root = 'D:/OAPMLData/'\n",
    "interpolation = 1\n",
    "\n",
    "#currently only have Colorado regions and one season specified for the tutorial\n",
    "#uncomment regions and seasons if doing a larger transform\n",
    "regions = {#'Utah': ['Abajos', 'Logan', 'Moab', 'Ogden', 'Provo', \n",
    "           #'Salt Lake', 'Skyline', 'Uintas'], \n",
    "           #'Colorado': ['Aspen Zone'] #small perf test\n",
    "           'Colorado': ['Grand Mesa Zone', 'Sangre de Cristo Range', 'Steamboat Zone', 'Front Range Zone',\n",
    "           'Vail Summit Zone', 'Sawatch Zone', 'Aspen Zone', \n",
    "           'North San Juan Mountains', 'South San Juan Mountains', 'Gunnison Zone'],\n",
    "           #'Washington': ['Mt Hood', 'Olympics', 'Snoqualmie Pass', 'Stevens Pass',\n",
    "           #'WA Cascades East, Central', 'WA Cascades East, North', 'WA Cascades East, South',\n",
    "           #'WA Cascades West, Central', 'WA Cascades West, Mt Baker', 'WA Cascades West, South'\n",
    "           #]           \n",
    "           'Canada': [\"Northwest Coastal\", \"Northwest Inland\", \"Sea To Sky\", \n",
    "                       \"South Coast Inland\", \"South Coast\", \"North Rockies\", \n",
    "                       \"Cariboos\", \"North Columbia\", \"South Columbia\", \"Purcells\", \n",
    "                       \"Kootenay Boundary\", \"South Rockies\", \"Lizard Range and Flathead\", \n",
    "                       \"Vancouver Island\", \"Kananaskis Country, Alberta Parks\", \"Chic Chocs, Avalanche Quebec\",\n",
    "                       \"Little Yoho\", \"Banff, Yoho and Kootenay National Parks\", \"Glacier National Park\",\n",
    "                       \"Waterton Lakes National Park\", \"Jasper National Park\"]\n",
    "           }\n",
    "seasons = ['15-16', '16-17', '17-18', '18-19', '19-20', '20-21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConvertToZarr:\n",
    "    \"\"\"\n",
    "    Class which encapsulates the logic to convert a set of filtered netCDF files to Zarr\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seasons, regions, data_root, interpolate=1, resample_length='1d', n_jobs=24, debug=False):\n",
    "        \"\"\"\n",
    "        Initialize the class\n",
    "        \n",
    "        Keyword Arguments\n",
    "        seasons: list of season values to process\n",
    "        regions: dictonary of Key: State and Value: List of Regions to process for that state\n",
    "        data_root: the root path of the data folders which contains the 3.GFSFiltered1xInterpolation\n",
    "        interpolate: the amount of interpolation applied in in the previous ParseGFS notebook (used for finding the correct input/output paths)\n",
    "        resample_length: the resample length used in the previous ParseGFS notebook (used for determining the input sequence lenght for the timeseries)\n",
    "        n_jobs: the number of jobs to run in parallel\n",
    "        debug: only use 28 days of the data for a small perf test\n",
    "        \"\"\"\n",
    "        self.processed_path = data_root + '/2.GFSFiltered'+ str(interpolate) + 'xInterpolation' + resample_length + '/'\n",
    "        self.zarr_base_path = output_root + '/3.GFSFiltered'+ str(interpolate) + 'xInterpolationZarr' + resample_length + '/'\n",
    "        \n",
    "        self.seasons = seasons\n",
    "        self.regions = regions\n",
    "        self.data_root = data_root\n",
    "        self.compressor = Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE)\n",
    "        self.n_jobs = n_jobs\n",
    "        self.debug = debug \n",
    "        if not os.path.exists(self.zarr_base_path):\n",
    "            os.makedirs(self.zarr_base_path)\n",
    "    \n",
    "    def compute_region(self, region_name, season, state):\n",
    "        \"\"\"\n",
    "        Calculates the zarr conversion for a specific region, season and state and indexes it for efficient lookup \n",
    "        \n",
    "        Keyword Arguments\n",
    "        region_name: name of the region to process\n",
    "        season: season to process\n",
    "        state: state to process (region must be a part of the state)\n",
    "        \"\"\"\n",
    "        first = True\n",
    "        base_path = self.processed_path + season + '/' + '/Region_' + region_name \n",
    "        zarr_path = self.zarr_base_path + season + '/' + state + '/Region_' + region_name + '.zarr'\n",
    "        \n",
    "        #TODO: refactor these to be shared code as logic also exists in ParseGFS\n",
    "        p = 181\n",
    "        if season in ['15-16', '19-20']:\n",
    "            p = 182 #leap years\n",
    "        if self.debug == True:\n",
    "            p = 28 #small perf test\n",
    "        \n",
    "        if season == '15-16':\n",
    "            snow_start_date = '2015-11-01'\n",
    "        elif season == '16-17':\n",
    "            snow_start_date = '2016-11-01'\n",
    "        elif season == '17-18':\n",
    "            snow_start_date = '2017-11-01'\n",
    "        elif season == '18-19':\n",
    "            snow_start_date = '2018-11-01'\n",
    "        elif season == '19-20':\n",
    "            snow_start_date = '2019-11-01'\n",
    "        elif season == '20-21':\n",
    "            snow_start_date = '2020-11-01'\n",
    "        else:\n",
    "            raise Exception('No known season ' + season)\n",
    "\n",
    "        date_values_pd = pd.date_range(snow_start_date, periods=p, freq=\"D\")\n",
    "        try:\n",
    "            with xr.open_zarr(zarr_path) as z:\n",
    "                if z.time.values[-1] == date_values_pd[-1]:\n",
    "                    print(' already exists: ' + region_name + ' ' + season + ' ' + state)\n",
    "                    z.close()\n",
    "                    return\n",
    "                else:\n",
    "                    #already exists but incomplete\n",
    "                    date_values_pd = [pd.Timestamp(v) for v in date_values_pd.values.astype('datetime64[ns]') if v not in z.time.values]\n",
    "                    print(' some exist but have to complete ' + str(len(date_values_pd)))\n",
    "                    first = False\n",
    "        except ValueError as err:\n",
    "            #ignore as it doesn't exist yet\n",
    "            pass\n",
    "        except FileNotFoundError as err2:\n",
    "            #ignore as it doesn't exist yet\n",
    "            pass\n",
    "        \n",
    "        #sometimes vars get added, filter to only the list of vars in the first dataset for that region\n",
    "        #TODO: handle the case where the first dataset has more vars than subsequent ones\n",
    "        final_vars = None\n",
    "        last_ds = xr.open_dataset(base_path + '_' + date_values_pd[-1].strftime('%Y%m%d') + '.nc')\n",
    "        #TODO: instead of variable-1, try vars -1\n",
    "        last_ds = last_ds.to_array(name='vars').chunk({'time':-1, 'latitude':-1, 'longitude':-1, 'variable':-1}).to_dataset()\n",
    "        last_vars = list(last_ds.variable.values)\n",
    "        \n",
    "        for d in date_values_pd:\n",
    "\n",
    "            path =  base_path + '_' + d.strftime('%Y%m%d') + '.nc'\n",
    "            print('On ' + str(path.split('/')[-1]))\n",
    "\n",
    "            try:\n",
    "                ds = xr.open_dataset(path, chunks={'latitude':1, 'longitude':1})\n",
    "            except OSError as err:\n",
    "                print(' missing file: ' + path)\n",
    "                continue\n",
    "\n",
    "            ds = ds.to_array(name='vars').chunk({'time':-1, 'latitude':-1, 'longitude':-1, 'variable':-1}).to_dataset()\n",
    "\n",
    "\n",
    "            if first:\n",
    "                #find intersection of the first and last variables to try to ensure\n",
    "                #that we are using only the intersection\n",
    "                final_vars = [v for v in list(ds.variable.values) if v in last_vars]\n",
    "                ds = ds.sel(variable=ds.variable.isin(final_vars))\n",
    "                enc = {x: {\"compressor\": self.compressor} for x in ds}\n",
    "                ds.sortby('variable').load().to_zarr(zarr_path, consolidated=True, encoding=enc)\n",
    "                first=False\n",
    "            else:\n",
    "                assert(final_vars is not None)\n",
    "                ds = ds.sel(variable=ds.variable.isin(final_vars))                \n",
    "                ds.sortby('variable').load().to_zarr(zarr_path, consolidated=True, append_dim='time')\n",
    "\n",
    "\n",
    "    def process_tuple(self, t): \n",
    "        \"\"\"\n",
    "        Entry method to call compute_region with a tuple\n",
    "        Basically a helper for executing with joblib parallel\n",
    "        \n",
    "        Keyword Arguments\n",
    "        t: the tuple containing the region, season and state\n",
    "        \"\"\"\n",
    "        self.compute_region(t[0], t[1], t[2])\n",
    "    \n",
    "    def make_list(self):\n",
    "        \"\"\"\n",
    "        Helper method to make the list of values to process\n",
    "        \"\"\"\n",
    "        to_process = []\n",
    "        for s in self.seasons:\n",
    "            for state in self.regions.keys():           \n",
    "                for r in self.regions[state]:\n",
    "                    to_process.append((r,s,state))\n",
    "        return to_process\n",
    "    \n",
    "    def convert_local(self, jobs):\n",
    "        l = self.make_list()\n",
    "    \n",
    "        Parallel(n_jobs=jobs)(map(delayed(self.process_tuple), l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| test_ignore\n",
    "ctz = ConvertToZarr(seasons, regions, data_root, resample_length='3H', debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\scott\\miniconda3\\envs\\oap_datapipeline\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5min 31s\n",
      "Wall time: 2d 4h 37min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#| test_ignore\n",
    "\n",
    "#all regions(CO) 1 season\n",
    "#1 disk\n",
    "#24 cores\n",
    "#CPU times: total: 8.16 s\n",
    "#Wall time: 2h 7min 41s\n",
    "\n",
    "#one region (CO) 1 season\n",
    "#1 disk\n",
    "#24 cores\n",
    "#CPU times: total: 15.6 ms\n",
    "#Wall time: 2min 53s\n",
    "#1.2MB\n",
    "\n",
    "#one region (CO) 1 season; with load\n",
    "#1 disk & 2 disks (didn't make a difference)\n",
    "#24 cores\n",
    "#CPU times: total: 62.6 ms\n",
    "#Wall time: 2min 37s\n",
    "#1.2MB\n",
    "\n",
    "#CA & CO all seasons\n",
    "#1 disk; 24 cores\n",
    "#CPU times: total: 5min 31s\n",
    "#Wall time: 2d 4h 37min 34s\n",
    "#16gb \n",
    "ctz.convert_local(jobs=ctz.n_jobs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
