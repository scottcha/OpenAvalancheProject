{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp convert_to_zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook which converts per region netCDF files to Zarr files to make them more efficient when indexing\n",
    "\n",
    "### uses pangeo_small environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ignore\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import xarray as xr\n",
    "import zarr\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the parameters here\n",
    "### Ensure all State and Regions you want to transform are specified here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ignore\n",
    "data_root = '/media/scottcha/E1/Data/OAPMLData/'\n",
    "\n",
    "interpolation = 1\n",
    "\n",
    "#currently only have Washington regions and one season specified for the tutorial\n",
    "#uncomment regions and seasons if doing a larger transform\n",
    "regions = {#'Utah': ['Abajos', 'Logan', 'Moab', 'Ogden', 'Provo', \n",
    "           #'Salt Lake', 'Skyline', 'Uintas'],  \n",
    "           #'Colorado': ['Grand Mesa Zone', 'Sangre de Cristo Range', 'Steamboat Zone', 'Front Range Zone',\n",
    "           #'Vail Summit Zone', 'Sawatch Zone', 'Aspen Zone', \n",
    "           #'North San Juan Mountains', 'South San Juan Mountains', 'Gunnison Zone'],\n",
    "           'Washington': ['Mt Hood', 'Olympics', 'Snoqualmie Pass', 'Stevens Pass',\n",
    "           'WA Cascades East, Central', 'WA Cascades East, North', 'WA Cascades East, South',\n",
    "           'WA Cascades West, Central', 'WA Cascades West, Mt Baker', 'WA Cascades West, South'\n",
    "           ]\n",
    "           }\n",
    "seasons = ['15-16']#, '16-17', '17-18', '18-19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvertToZarr:\n",
    "    \"\"\"\n",
    "    Class which encapsulates the logic to convert a set of filtered netCDF files to Zarr\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seasons, regions, data_root, interpolate=1):\n",
    "        \"\"\"\n",
    "        Initialize the class\n",
    "        \n",
    "        Keyword Arguments\n",
    "        seasons: list of season values to process\n",
    "        regions: dictonary of Key: State and Value: List of Regions to process for that state\n",
    "        data_root: the root path of the data folders which contains the 3.GFSFiltered1xInterpolation\n",
    "        interpolate: the amount of interpolation applied in in the previous ParseGFS notebook (used for finding the correct input/output paths)\n",
    "        \"\"\"\n",
    "        self.processed_path = data_root + '/3.GFSFiltered'+ str(interpolate) + 'xInterpolation/'\n",
    "        self.zarr_base_path = data_root + '/4.GFSFiltered'+ str(interpolate) + 'xInterpolationZarr/'\n",
    "        \n",
    "        self.seasons = seasons\n",
    "        self.regions = regions\n",
    "        self.data_root = data_root\n",
    "        \n",
    "        if not os.path.exists(self.zarr_base_path):\n",
    "            os.makedirs(self.zarr_base_path)\n",
    "    \n",
    "    def compute_region(self, region_name, season, state):\n",
    "        \"\"\"\n",
    "        Calculates the zarr conversion for a specific region, season and state and indexes it for efficient lookup \n",
    "        \n",
    "        Keyword Arguments\n",
    "        region_name: name of the region to process\n",
    "        season: season to process\n",
    "        state: state to process (region must be a part of the state)\n",
    "        \"\"\"\n",
    "        first = True\n",
    "        base_path = self.processed_path + season + '/' + '/Region_' + region_name \n",
    "        zarr_path = self.zarr_base_path + season + '/' + state + '/Region_' + region_name + '.zarr'\n",
    "        \n",
    "        #TODO: refactor these to be shared code as logic also exists in ParseGFS\n",
    "        p = 181\n",
    "        if season in ['15-16', '19-20']:\n",
    "            p = 182 #leap years\n",
    "\n",
    "        snow_start_date = '2015-11-01'\n",
    "        if season == '16-17':\n",
    "            snow_start_date = '2016-11-01'\n",
    "        if season == '17-18':\n",
    "            snow_start_date = '2017-11-01'\n",
    "        if season == '18-19':\n",
    "            snow_start_date = '2018-11-01'\n",
    "        if season == '19-20':\n",
    "            snow_start_date = '2019-11-01'\n",
    "\n",
    "        date_values_pd = pd.date_range(snow_start_date, periods=p, freq=\"D\")\n",
    "        try:\n",
    "            with xr.open_zarr(zarr_path) as z:\n",
    "                if z.time.values[-1] == date_values_pd[-1]:\n",
    "                    print(' already exists: ' + region_name + ' ' + season + ' ' + state)\n",
    "                    z.close()\n",
    "                    return\n",
    "                else:\n",
    "                    #already exists but incomplete\n",
    "                    date_values_pd = [pd.Timestamp(v) for v in date_values_pd.values.astype('datetime64[ns]') if v not in z.time.values]\n",
    "                    print(' some exist but have to complete ' + str(len(date_values_pd)))\n",
    "                    first = False\n",
    "        except ValueError as err:\n",
    "            #ignore as it doesn't exist yet\n",
    "            pass\n",
    "\n",
    "        for d in date_values_pd:\n",
    "\n",
    "            path =  base_path + '_' + d.strftime('%Y%m%d') + '.nc'\n",
    "            print('On ' + str(path.split('/')[-1]))\n",
    "\n",
    "            try:\n",
    "                ds = xr.open_dataset(path, chunks={'latitude':1, 'longitude':1})\n",
    "            except OSError as err:\n",
    "                print(' missing file: ' + path)\n",
    "                continue\n",
    "\n",
    "            ds = ds.to_array(name='vars').chunk({'time':1, 'latitude':1, 'longitude':1, 'variable':-1}).to_dataset()\n",
    "\n",
    "            try:\n",
    "\n",
    "                if first:\n",
    "                    ds.to_zarr(zarr_path, consolidated=True)\n",
    "                    first=False\n",
    "                else:\n",
    "                    ds.to_zarr(zarr_path, consolidated=True, append_dim='time')\n",
    "            except ValueError as err:\n",
    "                print('Value Error on ' + zarr_path)\n",
    "                return\n",
    "\n",
    "    def process_tuple(self, t): \n",
    "        \"\"\"\n",
    "        Entry method to call compute_region with a tuple\n",
    "        Basically a helper for executing with joblib parallel\n",
    "        \n",
    "        Keyword Arguments\n",
    "        t: the tuple containing the region, season and state\n",
    "        \"\"\"\n",
    "        self.compute_region(t[0], t[1], t[2])\n",
    "    \n",
    "    def make_list(self):\n",
    "        \"\"\"\n",
    "        Helper method to make the list of values to process\n",
    "        \"\"\"\n",
    "        to_process = []\n",
    "        for s in self.seasons:\n",
    "            for state in self.regions.keys():           \n",
    "                for r in self.regions[state]:\n",
    "                    to_process.append((r,s,state))\n",
    "        return to_process\n",
    "    \n",
    "    def convert_local(self, jobs=15):\n",
    "        l = self.make_list()\n",
    "    \n",
    "        #one state & season takes about 6 hours with 15 cores on my machine\n",
    "        Parallel(n_jobs=jobs, backend=\"multiprocessing\")(map(delayed(self.process_tuple), l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ignore\n",
    "ctz = ConvertToZarr(seasons, regions, data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ignore\n",
    "ctz.convert_local()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo_small] *",
   "language": "python",
   "name": "conda-env-pangeo_small-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
