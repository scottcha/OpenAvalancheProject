{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp convert_to_zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook which converts per region netCDF files to Zarr files to make them more efficient when indexing\n",
    "\n",
    "### uses pangeo_small environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ignore\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import xarray as xr\n",
    "import zarr\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import os\n",
    "from numcodecs import Blosc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the parameters here\n",
    "### Ensure all State and Regions you want to transform are specified here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ignore\n",
    "data_root = '/media/scottcha/Data2/OAPMLData/'\n",
    "output_root = '/media/scottcha/E1/Data/OAPMLData/'\n",
    "interpolation = 1\n",
    "\n",
    "#currently only have Washington regions and one season specified for the tutorial\n",
    "#uncomment regions and seasons if doing a larger transform\n",
    "regions = {#'Utah': ['Abajos', 'Logan', 'Moab', 'Ogden', 'Provo', \n",
    "           #'Salt Lake', 'Skyline', 'Uintas'],  \n",
    "           'Colorado': ['Grand Mesa Zone', 'Sangre de Cristo Range', 'Steamboat Zone', 'Front Range Zone',\n",
    "           'Vail Summit Zone', 'Sawatch Zone', 'Aspen Zone', \n",
    "           'North San Juan Mountains', 'South San Juan Mountains', 'Gunnison Zone'],\n",
    "           'Washington': ['Mt Hood', 'Olympics', 'Snoqualmie Pass', 'Stevens Pass',\n",
    "           #'WA Cascades East, Central', 'WA Cascades East, North', 'WA Cascades East, South',\n",
    "           #'WA Cascades West, Central', 'WA Cascades West, Mt Baker', 'WA Cascades West, South'\n",
    "           #]           \n",
    "           'Canada': [\"Northwest Coastal\", \"Northwest Inland\", \"Sea To Sky\", \n",
    "                       \"South Coast Inland\", \"South Coast\", \"North Rockies\", \n",
    "                       \"Cariboos\", \"North Columbia\", \"South Columbia\", \"Purcells\", \n",
    "                       \"Kootenay Boundary\", \"South Rockies\", \"Lizard Range and Flathead\", \n",
    "                       \"Vancouver Island\", \"Kananaskis Country, Alberta Parks\", \"Chic Chocs, Avalanche Quebec\",\n",
    "                       \"Little Yoho\", \"Banff, Yoho and Kootenay National Parks\", \"Glacier National Park\",\n",
    "                       \"Waterton Lakes National Park\", \"Jasper National Park\"]\n",
    "           }\n",
    "seasons = ['15-16', '16-17', '17-18', '18-19', '19-20', '20-21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvertToZarr:\n",
    "    \"\"\"\n",
    "    Class which encapsulates the logic to convert a set of filtered netCDF files to Zarr\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seasons, regions, data_root, interpolate=1, resample_length='1d'):\n",
    "        \"\"\"\n",
    "        Initialize the class\n",
    "        \n",
    "        Keyword Arguments\n",
    "        seasons: list of season values to process\n",
    "        regions: dictonary of Key: State and Value: List of Regions to process for that state\n",
    "        data_root: the root path of the data folders which contains the 3.GFSFiltered1xInterpolation\n",
    "        interpolate: the amount of interpolation applied in in the previous ParseGFS notebook (used for finding the correct input/output paths)\n",
    "        \"\"\"\n",
    "        self.processed_path = data_root + '/3.GFSFiltered'+ str(interpolate) + 'xInterpolation' + resample_length + '/'\n",
    "        self.zarr_base_path = output_root + '/4.GFSFiltered'+ str(interpolate) + 'xInterpolationZarr' + resample_length + '/'\n",
    "        \n",
    "        self.seasons = seasons\n",
    "        self.regions = regions\n",
    "        self.data_root = data_root\n",
    "        self.compressor = Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE)\n",
    "        \n",
    "        if not os.path.exists(self.zarr_base_path):\n",
    "            os.makedirs(self.zarr_base_path)\n",
    "    \n",
    "    def compute_region(self, region_name, season, state):\n",
    "        \"\"\"\n",
    "        Calculates the zarr conversion for a specific region, season and state and indexes it for efficient lookup \n",
    "        \n",
    "        Keyword Arguments\n",
    "        region_name: name of the region to process\n",
    "        season: season to process\n",
    "        state: state to process (region must be a part of the state)\n",
    "        \"\"\"\n",
    "        first = True\n",
    "        base_path = self.processed_path + season + '/' + '/Region_' + region_name \n",
    "        zarr_path = self.zarr_base_path + season + '/' + state + '/Region_' + region_name + '.zarr'\n",
    "        \n",
    "        #TODO: refactor these to be shared code as logic also exists in ParseGFS\n",
    "        p = 181\n",
    "        if season in ['15-16', '19-20']:\n",
    "            p = 182 #leap years\n",
    "        \n",
    "        if season == '15-16':\n",
    "            snow_start_date = '2015-11-01'\n",
    "        elif season == '16-17':\n",
    "            snow_start_date = '2016-11-01'\n",
    "        elif season == '17-18':\n",
    "            snow_start_date = '2017-11-01'\n",
    "        elif season == '18-19':\n",
    "            snow_start_date = '2018-11-01'\n",
    "        elif season == '19-20':\n",
    "            snow_start_date = '2019-11-01'\n",
    "        elif season == '20-21':\n",
    "            snow_start_date = '2020-11-01'\n",
    "        else:\n",
    "            raise Exception('No known season ' + season)\n",
    "\n",
    "        date_values_pd = pd.date_range(snow_start_date, periods=p, freq=\"D\")\n",
    "        try:\n",
    "            with xr.open_zarr(zarr_path) as z:\n",
    "                if z.time.values[-1] == date_values_pd[-1]:\n",
    "                    print(' already exists: ' + region_name + ' ' + season + ' ' + state)\n",
    "                    z.close()\n",
    "                    return\n",
    "                else:\n",
    "                    #already exists but incomplete\n",
    "                    date_values_pd = [pd.Timestamp(v) for v in date_values_pd.values.astype('datetime64[ns]') if v not in z.time.values]\n",
    "                    print(' some exist but have to complete ' + str(len(date_values_pd)))\n",
    "                    first = False\n",
    "        except ValueError as err:\n",
    "            #ignore as it doesn't exist yet\n",
    "            pass\n",
    "        \n",
    "        #sometimes vars get added, filter to only the list of vars in the first dataset for that region\n",
    "        #TODO: handle the case where the first dataset has more vars than subsequent ones\n",
    "        final_vars = None\n",
    "        last_ds = xr.open_dataset(base_path + '_' + date_values_pd[-1].strftime('%Y%m%d') + '.nc')\n",
    "        #TODO: instead of variable-1, try vars -1\n",
    "        last_ds = last_ds.to_array(name='vars').chunk({'time':-1, 'latitude':-1, 'longitude':-1, 'variable':-1}).to_dataset()\n",
    "        last_vars = list(last_ds.variable.values)\n",
    "        \n",
    "        for d in date_values_pd:\n",
    "\n",
    "            path =  base_path + '_' + d.strftime('%Y%m%d') + '.nc'\n",
    "            print('On ' + str(path.split('/')[-1]))\n",
    "\n",
    "            try:\n",
    "                ds = xr.open_dataset(path, chunks={'latitude':1, 'longitude':1})\n",
    "            except OSError as err:\n",
    "                print(' missing file: ' + path)\n",
    "                continue\n",
    "\n",
    "            ds = ds.to_array(name='vars').chunk({'time':-1, 'latitude':-1, 'longitude':-1, 'variable':-1}).to_dataset()\n",
    "\n",
    "\n",
    "            if first:\n",
    "                #find intersection of the first and last variables to try to ensure\n",
    "                #that we are using only the intersection\n",
    "                final_vars = [v for v in list(ds.variable.values) if v in last_vars]\n",
    "                ds = ds.sel(variable=ds.variable.isin(final_vars))\n",
    "                enc = {x: {\"compressor\": self.compressor} for x in ds}\n",
    "                ds.sortby('variable').to_zarr(zarr_path, consolidated=True, encoding=enc)\n",
    "                first=False\n",
    "            else:\n",
    "                assert(final_vars is not None)\n",
    "                ds = ds.sel(variable=ds.variable.isin(final_vars))                \n",
    "                ds.sortby('variable').to_zarr(zarr_path, consolidated=True, append_dim='time')\n",
    "\n",
    "\n",
    "    def process_tuple(self, t): \n",
    "        \"\"\"\n",
    "        Entry method to call compute_region with a tuple\n",
    "        Basically a helper for executing with joblib parallel\n",
    "        \n",
    "        Keyword Arguments\n",
    "        t: the tuple containing the region, season and state\n",
    "        \"\"\"\n",
    "        self.compute_region(t[0], t[1], t[2])\n",
    "    \n",
    "    def make_list(self):\n",
    "        \"\"\"\n",
    "        Helper method to make the list of values to process\n",
    "        \"\"\"\n",
    "        to_process = []\n",
    "        for s in self.seasons:\n",
    "            for state in self.regions.keys():           \n",
    "                for r in self.regions[state]:\n",
    "                    to_process.append((r,s,state))\n",
    "        return to_process\n",
    "    \n",
    "    def convert_local(self, jobs=15):\n",
    "        l = self.make_list()\n",
    "    \n",
    "        #one state & season takes about 6 hours with 15 cores on my machine\n",
    "        Parallel(n_jobs=jobs, backend=\"multiprocessing\")(map(delayed(self.process_tuple), l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ignore\n",
    "ctz = ConvertToZarr(seasons, regions, data_root, resample_length='3H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15402/2406378655.py:63: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  with xr.open_zarr(zarr_path) as z:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Region_Vail Summit Zone_20151101.nc\n",
      "On Region_Vail Summit Zone_20151102.nc\n",
      "On Region_Vail Summit Zone_20151103.nc\n",
      "On Region_Vail Summit Zone_20151104.nc\n",
      "On Region_Vail Summit Zone_20151105.nc\n",
      "On Region_Vail Summit Zone_20151106.nc\n",
      "On Region_Vail Summit Zone_20151107.nc\n",
      "On Region_Vail Summit Zone_20151108.nc\n",
      "On Region_Vail Summit Zone_20151109.nc\n",
      "On Region_Vail Summit Zone_20151110.nc\n",
      "On Region_Vail Summit Zone_20151111.nc\n",
      "On Region_Vail Summit Zone_20151112.nc\n",
      "On Region_Vail Summit Zone_20151113.nc\n",
      "On Region_Vail Summit Zone_20151114.nc\n",
      "On Region_Vail Summit Zone_20151115.nc\n",
      "On Region_Vail Summit Zone_20151116.nc\n",
      "On Region_Vail Summit Zone_20151117.nc\n",
      "On Region_Vail Summit Zone_20151118.nc\n",
      "On Region_Vail Summit Zone_20151119.nc\n",
      "On Region_Vail Summit Zone_20151120.nc\n",
      "On Region_Vail Summit Zone_20151121.nc\n",
      "On Region_Vail Summit Zone_20151122.nc\n",
      "On Region_Vail Summit Zone_20151123.nc\n",
      "On Region_Vail Summit Zone_20151124.nc\n",
      "On Region_Vail Summit Zone_20151125.nc\n",
      "On Region_Vail Summit Zone_20151126.nc\n",
      "On Region_Vail Summit Zone_20151127.nc\n",
      "On Region_Vail Summit Zone_20151128.nc\n",
      "On Region_Vail Summit Zone_20151129.nc\n",
      "On Region_Vail Summit Zone_20151130.nc\n",
      "On Region_Vail Summit Zone_20151201.nc\n",
      "On Region_Vail Summit Zone_20151202.nc\n",
      "On Region_Vail Summit Zone_20151203.nc\n",
      "On Region_Vail Summit Zone_20151204.nc\n",
      "On Region_Vail Summit Zone_20151205.nc\n",
      "On Region_Vail Summit Zone_20151206.nc\n",
      "On Region_Vail Summit Zone_20151207.nc\n",
      "On Region_Vail Summit Zone_20151208.nc\n",
      "On Region_Vail Summit Zone_20151209.nc\n",
      "On Region_Vail Summit Zone_20151210.nc\n",
      "On Region_Vail Summit Zone_20151211.nc\n",
      "On Region_Vail Summit Zone_20151212.nc\n",
      "On Region_Vail Summit Zone_20151213.nc\n",
      "On Region_Vail Summit Zone_20151214.nc\n",
      "On Region_Vail Summit Zone_20151215.nc\n",
      "On Region_Vail Summit Zone_20151216.nc\n",
      "On Region_Vail Summit Zone_20151217.nc\n",
      "On Region_Vail Summit Zone_20151218.nc\n",
      "On Region_Vail Summit Zone_20151219.nc\n",
      "On Region_Vail Summit Zone_20151220.nc\n",
      "On Region_Vail Summit Zone_20151221.nc\n",
      "On Region_Vail Summit Zone_20151222.nc\n",
      "On Region_Vail Summit Zone_20151223.nc\n",
      "On Region_Vail Summit Zone_20151224.nc\n",
      "On Region_Vail Summit Zone_20151225.nc\n",
      "On Region_Vail Summit Zone_20151226.nc\n",
      "On Region_Vail Summit Zone_20151227.nc\n",
      "On Region_Vail Summit Zone_20151228.nc\n",
      "On Region_Vail Summit Zone_20151229.nc\n",
      "On Region_Vail Summit Zone_20151230.nc\n",
      "On Region_Vail Summit Zone_20151231.nc\n",
      "On Region_Vail Summit Zone_20160101.nc\n",
      "On Region_Vail Summit Zone_20160102.nc\n",
      "On Region_Vail Summit Zone_20160103.nc\n",
      "On Region_Vail Summit Zone_20160104.nc\n",
      "On Region_Vail Summit Zone_20160105.nc\n",
      "On Region_Vail Summit Zone_20160106.nc\n",
      "On Region_Vail Summit Zone_20160107.nc\n",
      "On Region_Vail Summit Zone_20160108.nc\n",
      "On Region_Vail Summit Zone_20160109.nc\n",
      "On Region_Vail Summit Zone_20160110.nc\n",
      "On Region_Vail Summit Zone_20160111.nc\n",
      "On Region_Vail Summit Zone_20160112.nc\n",
      "On Region_Vail Summit Zone_20160113.nc\n",
      "On Region_Vail Summit Zone_20160114.nc\n",
      "On Region_Vail Summit Zone_20160115.nc\n",
      "On Region_Vail Summit Zone_20160116.nc\n",
      "On Region_Vail Summit Zone_20160117.nc\n",
      "On Region_Vail Summit Zone_20160118.nc\n",
      "On Region_Vail Summit Zone_20160119.nc\n",
      "On Region_Vail Summit Zone_20160120.nc\n",
      "On Region_Vail Summit Zone_20160121.nc\n",
      "On Region_Vail Summit Zone_20160122.nc\n",
      "On Region_Vail Summit Zone_20160123.nc\n",
      "On Region_Vail Summit Zone_20160124.nc\n",
      "On Region_Vail Summit Zone_20160125.nc\n",
      "On Region_Vail Summit Zone_20160126.nc\n",
      "On Region_Vail Summit Zone_20160127.nc\n",
      "On Region_Vail Summit Zone_20160128.nc\n",
      "On Region_Vail Summit Zone_20160129.nc\n",
      "On Region_Vail Summit Zone_20160130.nc\n",
      "On Region_Vail Summit Zone_20160131.nc\n",
      "On Region_Vail Summit Zone_20160201.nc\n",
      "On Region_Vail Summit Zone_20160202.nc\n",
      "On Region_Vail Summit Zone_20160203.nc\n",
      "On Region_Vail Summit Zone_20160204.nc\n",
      "On Region_Vail Summit Zone_20160205.nc\n",
      "On Region_Vail Summit Zone_20160206.nc\n",
      "On Region_Vail Summit Zone_20160207.nc\n",
      "On Region_Vail Summit Zone_20160208.nc\n",
      "On Region_Vail Summit Zone_20160209.nc\n",
      "On Region_Vail Summit Zone_20160210.nc\n",
      "On Region_Vail Summit Zone_20160211.nc\n",
      "On Region_Vail Summit Zone_20160212.nc\n",
      "On Region_Vail Summit Zone_20160213.nc\n",
      "On Region_Vail Summit Zone_20160214.nc\n",
      "On Region_Vail Summit Zone_20160215.nc\n",
      "On Region_Vail Summit Zone_20160216.nc\n",
      "On Region_Vail Summit Zone_20160217.nc\n",
      "On Region_Vail Summit Zone_20160218.nc\n",
      "On Region_Vail Summit Zone_20160219.nc\n",
      "On Region_Vail Summit Zone_20160220.nc\n",
      "On Region_Vail Summit Zone_20160221.nc\n",
      "On Region_Vail Summit Zone_20160222.nc\n",
      "On Region_Vail Summit Zone_20160223.nc\n",
      "On Region_Vail Summit Zone_20160224.nc\n",
      "On Region_Vail Summit Zone_20160225.nc\n",
      "On Region_Vail Summit Zone_20160226.nc\n",
      "On Region_Vail Summit Zone_20160227.nc\n",
      "On Region_Vail Summit Zone_20160228.nc\n",
      "On Region_Vail Summit Zone_20160229.nc\n",
      "On Region_Vail Summit Zone_20160301.nc\n",
      "On Region_Vail Summit Zone_20160302.nc\n",
      "On Region_Vail Summit Zone_20160303.nc\n",
      "On Region_Vail Summit Zone_20160304.nc\n",
      "On Region_Vail Summit Zone_20160305.nc\n",
      "On Region_Vail Summit Zone_20160306.nc\n",
      "On Region_Vail Summit Zone_20160307.nc\n",
      "On Region_Vail Summit Zone_20160308.nc\n",
      "On Region_Vail Summit Zone_20160309.nc\n",
      "On Region_Vail Summit Zone_20160310.nc\n",
      "On Region_Vail Summit Zone_20160311.nc\n",
      "On Region_Vail Summit Zone_20160312.nc\n",
      "On Region_Vail Summit Zone_20160313.nc\n",
      "On Region_Vail Summit Zone_20160314.nc\n",
      "On Region_Vail Summit Zone_20160315.nc\n",
      "On Region_Vail Summit Zone_20160316.nc\n",
      "On Region_Vail Summit Zone_20160317.nc\n",
      "On Region_Vail Summit Zone_20160318.nc\n",
      "On Region_Vail Summit Zone_20160319.nc\n",
      "On Region_Vail Summit Zone_20160320.nc\n",
      "On Region_Vail Summit Zone_20160321.nc\n",
      "On Region_Vail Summit Zone_20160322.nc\n",
      "On Region_Vail Summit Zone_20160323.nc\n",
      "On Region_Vail Summit Zone_20160324.nc\n",
      "On Region_Vail Summit Zone_20160325.nc\n",
      "On Region_Vail Summit Zone_20160326.nc\n",
      "On Region_Vail Summit Zone_20160327.nc\n",
      "On Region_Vail Summit Zone_20160328.nc\n",
      "On Region_Vail Summit Zone_20160329.nc\n",
      "On Region_Vail Summit Zone_20160330.nc\n",
      "On Region_Vail Summit Zone_20160331.nc\n",
      "On Region_Vail Summit Zone_20160401.nc\n",
      "On Region_Vail Summit Zone_20160402.nc\n",
      "On Region_Vail Summit Zone_20160403.nc\n",
      "On Region_Vail Summit Zone_20160404.nc\n",
      "On Region_Vail Summit Zone_20160405.nc\n",
      "On Region_Vail Summit Zone_20160406.nc\n",
      "On Region_Vail Summit Zone_20160407.nc\n",
      "On Region_Vail Summit Zone_20160408.nc\n",
      "On Region_Vail Summit Zone_20160409.nc\n",
      "On Region_Vail Summit Zone_20160410.nc\n",
      "On Region_Vail Summit Zone_20160411.nc\n",
      "On Region_Vail Summit Zone_20160412.nc\n",
      "On Region_Vail Summit Zone_20160413.nc\n",
      "On Region_Vail Summit Zone_20160414.nc\n",
      "On Region_Vail Summit Zone_20160415.nc\n",
      "On Region_Vail Summit Zone_20160416.nc\n",
      "On Region_Vail Summit Zone_20160417.nc\n",
      "On Region_Vail Summit Zone_20160418.nc\n",
      "On Region_Vail Summit Zone_20160419.nc\n",
      "On Region_Vail Summit Zone_20160420.nc\n",
      "On Region_Vail Summit Zone_20160421.nc\n",
      "On Region_Vail Summit Zone_20160422.nc\n",
      "On Region_Vail Summit Zone_20160423.nc\n",
      "On Region_Vail Summit Zone_20160424.nc\n",
      "On Region_Vail Summit Zone_20160425.nc\n",
      "On Region_Vail Summit Zone_20160426.nc\n",
      "On Region_Vail Summit Zone_20160427.nc\n",
      "On Region_Vail Summit Zone_20160428.nc\n",
      "On Region_Vail Summit Zone_20160429.nc\n",
      "On Region_Vail Summit Zone_20160430.nc\n",
      "CPU times: user 192 ms, sys: 122 ms, total: 314 ms\n",
      "Wall time: 20min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#test_ignore\n",
    "#1 region two seasons\n",
    "#1 disk\n",
    "#20 cores, no compression \n",
    "#CPU times: user 902 ms, sys: 472 ms, total: 1.37 s\n",
    "#Wall time: 23min 36s\n",
    "#103MB\n",
    "#compression same time but 52MB\n",
    "\n",
    "# all regions two seasons\n",
    "# 2 disks\n",
    "# 12 cores\n",
    "# Wall Time 132 mins\n",
    "# 1.2GB\n",
    "\n",
    "#all regions(CO and CA) all  seasons\n",
    "#2 disks\n",
    "#20 cores\n",
    "#CPU times: user 1min 48s, sys: 46.9 s, total: 2min 35s\n",
    "#Wall time: 1d 12h 1min 33s\n",
    "# 15gb for all data compressed\n",
    "\n",
    "ctz.convert_local(jobs = 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo_small4]",
   "language": "python",
   "name": "conda-env-pangeo_small4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
