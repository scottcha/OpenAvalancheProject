{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp prep_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# This notebook is the primary notebook to take data in a per region format and transform it in to a format appropriate for ML problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ignore\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data structure\n",
    "\n",
    "OAPMLData\\\n",
    "    CleanedForecastsNWAC_CAIC_UAC.V1.2013-2020.csv #labels downloaded from https://oapstorageprod.blob.core.windows.net/oap-training-data/Data/V1.1FeaturesWithLabels2013-2020.zip\n",
    "    \n",
    "\n",
    "    1.RawWeatherData/\n",
    "        gfs/\n",
    "            <season>/\n",
    "                /<state or country>/\n",
    "    2.GFSDaily(x)Interpolation/\n",
    "    3.GFSFiltered(x)Interpolation/\n",
    "    4.GFSFiltered(x)InterpolationZarr/\n",
    "    5.MLData\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ignore\n",
    "data_root = '/media/scottcha/E1/Data/OAPMLData/'\n",
    "#data_root = '/media/scottcha/E1/Data/Temp/Tutorial/'\n",
    "interpolation = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PrepML:\n",
    "    \n",
    "    \n",
    "    def __init__(self, data_root, interpolate=1, date_start='2015-11-01', date_end='2020-04-30', date_train_test_cutoff='2019-11-01'):\n",
    "        \"\"\"\n",
    "        Initialize the class\n",
    "        \n",
    "        Keyword Arguments\n",
    "        data_root: the root path of the data folders which contains the 4.GFSFiltered1xInterpolationZarr\n",
    "        interpolate: the amount of interpolation applied in in the previous ParseGFS notebook (used for finding the correct input/output paths)        \n",
    "        date_start: Earlist date to include in label set (default: '2015-11-01')\n",
    "        date_end: Latest date to include in label set (default: '2020-04-30')\n",
    "        date_train_test_cutoff: Date to use as a cutoff between the train and test labels (default: '2019-11-01')\n",
    "        \"\"\"\n",
    "        self.data_root = data_root\n",
    "        self.interpolation = interpolate\n",
    "        self.date_start = date_start\n",
    "        self.date_end = date_end\n",
    "        self.date_train_test_cutoff = date_train_test_cutoff\n",
    "        self.nc_path = data_root + '/3.GFSFiltered'+ str(self.interpolation) + 'xInterpolation/'\n",
    "        self.processed_path = data_root + '/4.GFSFiltered'+ str(self.interpolation) + 'xInterpolationZarr/'\n",
    "        self.path_to_labels = data_root + 'CleanedForecastsNWAC_CAIC_UAC.V1.2013-2020.csv'\n",
    "        self.ml_path = data_root + '/5.MLData'\n",
    "        self.date_col = 'Day1Date'\n",
    "        self.region_col = 'UnifiedRegion'\n",
    "        self.parsed_date_col = 'parsed_date'\n",
    "        if not os.path.exists(self.ml_path):\n",
    "            os.makedirs(self.ml_path)\n",
    "            \n",
    "        #map states to regions for purposes of data lookup\n",
    "        self.regions = {\n",
    "            'Utah': ['Abajos', 'Logan', 'Moab', 'Ogden', 'Provo', \n",
    "            'Salt Lake', 'Skyline', 'Uintas'],  \n",
    "            'Colorado': ['Grand Mesa Zone', 'Sangre de Cristo Range', 'Steamboat Zone', 'Front Range Zone',\n",
    "            'Vail Summit Zone', 'Sawatch Zone', 'Aspen Zone', \n",
    "            'North San Juan Mountains', 'South San Juan Mountains', 'Gunnison Zone'],\n",
    "            'Washington': ['Mt Hood', 'Olympics', 'Snoqualmie Pass', 'Stevens Pass',\n",
    "            'WA Cascades East, Central', 'WA Cascades East, North', 'WA Cascades East, South',\n",
    "            'WA Cascades West, Central', 'WA Cascades West, Mt Baker', 'WA Cascades West, South'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "        \n",
    "    @staticmethod\n",
    "    def lookup_forecast_region(label_region):\n",
    "        \"\"\"\n",
    "        mapping between region names as the labels and the forecasts have slightly different standards\n",
    "        TODO: could add a unified mapping upstream in parseGFS files or in the label generation\n",
    "\n",
    "        Keyword Arguments:\n",
    "        label_region: region as defined in the labels file\n",
    "\n",
    "        returns the region as defined in the features\n",
    "        \"\"\"\n",
    "        if label_region == 'Mt Hood':\n",
    "            return 'Mt Hood'\n",
    "        elif label_region == 'Olympics':\n",
    "            return 'Olympics'\n",
    "        elif label_region == 'Cascade Pass - Snoq. Pass':\n",
    "            return 'Snoqualmie Pass'\n",
    "        elif label_region == 'Cascade Pass - Stevens Pass':\n",
    "            return 'Stevens Pass'\n",
    "        elif label_region == 'Cascade East - Central':\n",
    "            return 'WA Cascades East, Central'\n",
    "        elif label_region == 'Cascade East - North':\n",
    "            return 'WA Cascades East, North'\n",
    "        elif label_region == 'Cascade East - South':\n",
    "            return 'WA Cascades East, South'\n",
    "        elif label_region == 'Cascade West - Central':\n",
    "            return 'WA Cascades West, Central'\n",
    "        elif label_region == 'Cascade West - North':\n",
    "            return 'WA Cascades West, Mt Baker'\n",
    "        elif label_region == 'Cascade West - South':\n",
    "            return 'WA Cascades West, South'\n",
    "        elif label_region == 'Abajo':\n",
    "            return 'Abajos'\n",
    "        elif label_region == 'Logan':\n",
    "            return 'Logan'\n",
    "        elif label_region == 'Moab':\n",
    "            return 'Moab'\n",
    "        elif label_region == 'Ogden':\n",
    "            return 'Ogden'\n",
    "        elif label_region == 'Provo':\n",
    "            return 'Provo'\n",
    "        elif label_region == 'Salt Lake':\n",
    "            return 'Salt Lake'\n",
    "        elif label_region == 'Skyline':\n",
    "            return 'Skyline'\n",
    "        elif label_region == 'Uintas':\n",
    "            return 'Uintas'\n",
    "        elif label_region == 'Grand Mesa':\n",
    "            return 'Grand Mesa Zone'\n",
    "        elif label_region == 'Sangre de Cristo':\n",
    "            return 'Sangre de Cristo Range'\n",
    "        elif label_region == 'Steamboat & Flat Tops':\n",
    "            return 'Steamboat Zone'\n",
    "        elif label_region == 'Front Range':\n",
    "            return 'Front Range Zone'\n",
    "        elif label_region == 'Vail & Summit County':\n",
    "            return 'Vail Summit Zone'\n",
    "        elif label_region == 'Sawatch Range':\n",
    "            return 'Sawatch Zone'\n",
    "        elif label_region == 'Aspen':\n",
    "            return 'Aspen Zone'\n",
    "        elif label_region == 'Northern San Juan':\n",
    "            return 'North San Juan Mountains'\n",
    "        elif label_region == 'Southern San Juan':\n",
    "            return 'South San Juan Mountains'\n",
    "        elif label_region == 'Gunnison':\n",
    "            return 'Gunnison Zone'\n",
    "        else:\n",
    "            return 'Got region ' + label_region + ' but its an unknown region'\n",
    "    \n",
    "    @staticmethod\n",
    "    def date_to_season(d):\n",
    "        \"\"\"\n",
    "        mapping of date to season\n",
    "        \n",
    "        Keyword Arguments\n",
    "        d: datetime64\n",
    "        \n",
    "        returns season indicator\n",
    "        \"\"\"\n",
    "        if d >= np.datetime64('2014-11-01') and d <= np.datetime64('2015-04-30'):\n",
    "            return (np.datetime64('2014-11-01'), '14-15')\n",
    "        elif d >= np.datetime64('2015-11-01') and d <= np.datetime64('2016-04-30'):\n",
    "            return (np.datetime64('2015-11-01'), '15-16')\n",
    "        elif d >= np.datetime64('2016-11-01') and d <= np.datetime64('2017-04-30'):\n",
    "            return (np.datetime64('2016-11-01'), '16-17')\n",
    "        elif d >= np.datetime64('2017-11-01') and d <= np.datetime64('2018-04-30'):\n",
    "            return (np.datetime64('2017-11-01'), '17-18')\n",
    "        elif d >= np.datetime64('2018-11-01') and d <= np.datetime64('2019-04-30'):\n",
    "            return (np.datetime64('2018-11-01'), '18-19')\n",
    "        elif d >= np.datetime64('2019-11-01') and d <= np.datetime64('2020-04-30'):\n",
    "            return (np.datetime64('2019-11-01'), '19-20')\n",
    "        else:\n",
    "            #print('Unknown season ' + str(d))\n",
    "            return (-1,'Unknown')\n",
    "    \n",
    "   \n",
    "    def get_state_for_region(self, region):\n",
    "        \"\"\"\n",
    "        Returns the state for a given region\n",
    "        \n",
    "        Keywork Arguments\n",
    "        region: region we want to lookup the state for\n",
    "        \"\"\"\n",
    "        for k in self.regions.keys():\n",
    "            if region in self.regions[k]:\n",
    "                return k\n",
    "\n",
    "        raise Exception('No region with name ' + region)\n",
    "    \n",
    "    def prep_labels(self, overwrite_cache=False):\n",
    "        \"\"\"\n",
    "        Preps the data and lable sets in to two sets, train & test\n",
    "        \n",
    "        Keyword Arguments\n",
    "        overwrite_cache: True indicates we want to recalculate the lat/lon combos, False indicates use the values if they exist in the cache file (otherwise calcualte and cache it)\n",
    "        \n",
    "        returns the train & test sets\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        #maintaining this as a dict since the arrays are ragged and its more efficient this way\n",
    "        #storing one sample for each region to get the lat/lon layout\n",
    "        region_zones = []\n",
    "        region_data = {}\n",
    "        for region in self.regions.keys():\n",
    "            for r in self.regions[region]:\n",
    "                region_zones.append(r)\n",
    "                region_data[r] = xr.open_dataset(self.nc_path + '15-16/' + '/Region_' + r + '_20160101.nc')\n",
    "        \n",
    "        #Read in all the label data\n",
    "        self.labels = pd.read_csv(self.path_to_labels, low_memory=False,\n",
    "                dtype={'Day1Danger_OctagonAboveTreelineEast': 'object',\n",
    "                       'Day1Danger_OctagonAboveTreelineNorth': 'object',\n",
    "                       'Day1Danger_OctagonAboveTreelineNorthEast': 'object',\n",
    "                       'Day1Danger_OctagonAboveTreelineNorthWest': 'object',\n",
    "                       'Day1Danger_OctagonAboveTreelineSouth': 'object',\n",
    "                       'Day1Danger_OctagonAboveTreelineSouthEast': 'object',\n",
    "                       'Day1Danger_OctagonAboveTreelineSouthWest': 'object',\n",
    "                       'Day1Danger_OctagonAboveTreelineWest': 'object',\n",
    "                       'Day1Danger_OctagonBelowTreelineEast': 'object',\n",
    "                       'Day1Danger_OctagonBelowTreelineNorth': 'object',\n",
    "                       'Day1Danger_OctagonBelowTreelineNorthEast': 'object',\n",
    "                       'Day1Danger_OctagonBelowTreelineNorthWest': 'object',\n",
    "                       'Day1Danger_OctagonBelowTreelineSouth': 'object',\n",
    "                       'Day1Danger_OctagonBelowTreelineSouthEast': 'object',\n",
    "                       'Day1Danger_OctagonBelowTreelineSouthWest': 'object',\n",
    "                       'Day1Danger_OctagonBelowTreelineWest': 'object',\n",
    "                       'Day1Danger_OctagonNearTreelineEast': 'object',\n",
    "                       'Day1Danger_OctagonNearTreelineNorth': 'object',\n",
    "                       'Day1Danger_OctagonNearTreelineNorthEast': 'object',\n",
    "                       'Day1Danger_OctagonNearTreelineNorthWest': 'object',\n",
    "                       'Day1Danger_OctagonNearTreelineSouth': 'object',\n",
    "                       'Day1Danger_OctagonNearTreelineSouthEast': 'object',\n",
    "                       'Day1Danger_OctagonNearTreelineSouthWest': 'object',\n",
    "                       'Day1Danger_OctagonNearTreelineWest': 'object',\n",
    "                       'SpecialStatement': 'object',\n",
    "                       'image_paths': 'object',\n",
    "                       'image_types': 'object',\n",
    "                       'image_urls': 'object'})\n",
    "     \n",
    "        self.labels['parsed_date'] = pd.to_datetime(self.labels[self.date_col], format='%Y%m%d')\n",
    "        \n",
    "        metadata_cols = [self.date_col, self.region_col]\n",
    "        label_cols = ['Day1DangerBelowTreeline', 'Day1DangerNearTreeline', 'Day1DangerAboveTreeline']\n",
    "        self.labels[self.region_col] = self.labels.apply(lambda x : PrepML.lookup_forecast_region(x[self.region_col]), axis=1)\n",
    "        self.labels = self.labels[self.labels[self.region_col]!='Unknown region']\n",
    "        \n",
    "        #ensure we are only using label data for regions we are looking at\n",
    "        self.labels = self.labels[self.labels[self.region_col].isin(region_zones)]\n",
    "\n",
    "        #add a season column\n",
    "        tmp = pd.DataFrame.from_records(self.labels[self.parsed_date_col].apply(PrepML.date_to_season))\n",
    "        self.labels['season'] = tmp[1]\n",
    "        self.labels.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        #some region/seasons have excessive errors in the data, remove those\n",
    "        self.labels = self.labels[self.labels['season'].isin(['15-16', '16-17', '17-18', '18-19'])]\n",
    "        self.labels = self.labels[~self.labels.index.isin(self.labels[(self.labels['season']=='15-16') & (self.labels[self.region_col]=='Steamboat Zone')].index)]\n",
    "        self.labels = self.labels[~self.labels.index.isin(self.labels[(self.labels['season']=='16-17') & (self.labels[self.region_col]=='Front Range Zone')].index)]\n",
    "        \n",
    "        #add extra labels which also allow us to have labels which indicate the trend in the avy direction\n",
    "        #the thought here is that predicting a rise or flat danger is usually easier than predicting when \n",
    "        #to lower the danger so seperating these in to seperate clases\n",
    "        #TODO: this should be dynamic based on label passed in, not hard coded to above treeline\n",
    "        labels_trends = pd.DataFrame()\n",
    "        for r in self.labels[self.region_col].unique():\n",
    "            for s in self.labels['season'].unique():\n",
    "                region_season_df = self.labels[self.labels['season']==s]\n",
    "                region_season_df = region_season_df[region_season_df[self.region_col]==r]\n",
    "                if(len(region_season_df) == 0):\n",
    "                    continue\n",
    "                region_season_df.sort_values(by='parsed_date', inplace=True)\n",
    "                region_season_df.reset_index(inplace=True, drop=True)\n",
    "                region_season_df['Day1DangerAboveTreelineValue'] = region_season_df['Day1DangerAboveTreeline'].map({'Low':0, 'Moderate':1, 'Considerable':2, 'High':3})\n",
    "                region_season_df.loc[0,'Day1DangerAboveTreelineWithTrend'] = region_season_df.iloc[0]['Day1DangerAboveTreeline'] + '_Initial'\n",
    "\n",
    "                for i in range(1,len(region_season_df)):\n",
    "                    prev = region_season_df.iloc[i-1]['Day1DangerAboveTreelineValue']\n",
    "                    cur = region_season_df.loc[i,'Day1DangerAboveTreelineValue']\n",
    "                    trend = '_Unknown'\n",
    "                    if prev == cur:\n",
    "                        trend = '_Flat'\n",
    "                    elif prev < cur:\n",
    "                        trend = '_Rising'\n",
    "                    elif prev >  cur:\n",
    "                        trend = '_Falling'\n",
    "\n",
    "                    region_season_df.loc[i,'Day1DangerAboveTreelineWithTrend'] = region_season_df.iloc[i]['Day1DangerAboveTreeline'] + trend\n",
    "                labels_trends = pd.concat([labels_trends,region_season_df])\n",
    "        assert(len(labels_trends)==len(self.labels))\n",
    "        self.labels = labels_trends\n",
    "        \n",
    "        lat_lon_union = pd.DataFrame()\n",
    "        lat_lon_path = self.processed_path + 'lat_lon_union.csv'\n",
    "        if overwrite_cache or not os.path.exists(lat_lon_path):   \n",
    "            #find union of all lat/lon/region to just grids with values\n",
    "            #the process to filter the lat/lon is expensive but we need to do it here (1-5 seconds per region)\n",
    "            #as the helps the batch process select relevant data\n",
    "            for r in region_data.keys():\n",
    "                print(r)\n",
    "                region_df = region_data[r].stack(lat_lon = ('latitude', 'longitude')).lat_lon.to_dataframe()\n",
    "                tmp_df = pd.DataFrame.from_records(region_df['lat_lon'], columns=['latitude', 'longitude'])\n",
    "                indexes_to_drop = []\n",
    "                for index, row in tmp_df.iterrows():\n",
    "                    #TODO: there might be a more efficient way than doing this one by one?\n",
    "                    if 0 == np.count_nonzero(region_data[r].to_array().sel(latitude=row['latitude'], longitude=row['longitude']).stack(time_var = ('time', 'variable')).dropna(dim='time_var', how='all').values):\n",
    "                        indexes_to_drop.append(index)\n",
    "                tmp_df.drop(indexes_to_drop, axis=0, inplace=True)\n",
    "                tmp_df[self.region_col] = r\n",
    "                lat_lon_union = pd.concat([lat_lon_union, tmp_df])        \n",
    "        \n",
    "                #cache the data\n",
    "                lat_lon_union.to_csv()\n",
    "        else:\n",
    "            #load the cached data\n",
    "            lat_lon_union = pd.read_csv(self.processed_path + 'lat_lon_union.csv',float_precision='round_trip')\n",
    "        #join in with the labels so we have a label per lat/lon pair\n",
    "        lat_lon_union = lat_lon_union.set_index(self.region_col, drop=False).join(self.labels.set_index(self.region_col, drop=False), how='left', lsuffix='left', rsuffix='right')\n",
    "        \n",
    "        #define the split between train and test\n",
    "        date_min = np.datetime64(self.date_start)\n",
    "        date_max = np.datetime64(self.date_end)\n",
    "        train_date_cutoff = np.datetime64(self.date_train_test_cutoff)\n",
    "\n",
    "        #split the train/test data\n",
    "        labels_data_union = lat_lon_union[lat_lon_union[self.parsed_date_col] >= date_min]\n",
    "        labels_data_union = labels_data_union[labels_data_union[self.parsed_date_col] <= date_max]\n",
    "        #copy so we can delete the overall data and only keep the filtered\n",
    "        labels_data_train = labels_data_union[labels_data_union[self.parsed_date_col] <= train_date_cutoff].copy()\n",
    "        labels_data_test = labels_data_union[labels_data_union[self.parsed_date_col] > train_date_cutoff].copy()\n",
    "        labels_data_train.reset_index(inplace=True)\n",
    "        labels_data_test.reset_index(inplace=True)\n",
    "        \n",
    "        return labels_data_train, labels_data_test\n",
    "    \n",
    "   \n",
    "    def get_data_zarr(self, region, lat, lon, lookback_days, date):\n",
    "        \"\"\"\n",
    "        utility to get data for a specific point\n",
    "        \n",
    "        Keyword Arguments\n",
    "        region: the region the point exists in\n",
    "        lat: the latitude of the point to lookup\n",
    "        lon: the longitude of the point to lookup\n",
    "        lookback_days: the number of days prior to the date to also return\n",
    "        date: the date which marks the end of the dataset (same date as the desired label)\n",
    "        \"\"\"\n",
    "        #print(region + ' ' + str(lat) + ', ' + str(lon) + ' ' + str(date))\n",
    "        state = self.get_state_for_region(region)\n",
    "        earliest_data, season = PrepML.date_to_season(date)\n",
    "\n",
    "        path = self.processed_path + '/' + season + '/' + state + '/Region_' + region + '.zarr'\n",
    "        #print('*Opening file ' + path)\n",
    "\n",
    "        tmp_ds = xr.open_zarr(path, consolidated=True)\n",
    "        start_day = date - np.timedelta64(lookback_days-1, 'D')\n",
    "        #print('start day ' + str(start_day))\n",
    "        tmp_ds = tmp_ds.sel(latitude=lat, longitude=lon, method='nearest').sel(time=slice(start_day, date))\n",
    "\n",
    "        date_values_pd = pd.date_range(start_day, periods=lookback_days, freq='D')\n",
    "        #reindex should fill missing values with NA\n",
    "        tmp_ds = tmp_ds.reindex({'time': date_values_pd})\n",
    "\n",
    "        tmp_ds = tmp_ds.reset_index(dims_or_levels='time', drop=True).load()\n",
    "        return tmp_ds\n",
    "    \n",
    "    \n",
    "    def process_sample(self, iter_tuple, lookback_days):\n",
    "        \"\"\"\n",
    "        Convienience method to take a tuple and pull the data for it from the zarr files\n",
    "\n",
    "        Keyword Arguments\n",
    "        iter_tuple: \n",
    "        lookback_days: the number of days prior to the date to also return\n",
    "        \"\"\"\n",
    "        row = iter_tuple[1]\n",
    "        d = row[self.parsed_date_col] \n",
    "\n",
    "        lat = row['latitude']\n",
    "        lon = row['longitude']\n",
    "        reg = row[self.region_col]            \n",
    "        #print('region: ' + reg + ' date ' + str(d))\n",
    "        ds = self.get_data_zarr(reg, lat, lon, lookback_days, d)\n",
    "\n",
    "        #print(\"actual data\")\n",
    "        if ds.time.shape[0] != lookback_days:    \n",
    "            print(ds)\n",
    "            print('Need to drop! Error, incorrect shape ' + str(ds.time.shape[0]) + ' on time ' + str(d))\n",
    "        return (ds)\n",
    "\n",
    "\n",
    "    def get_xr_batch(self, labels, lookback_days=14, batch_size=64, y_column='Day1DangerAboveTreeline', label_values=['Low', 'Moderate', 'Considerable', 'High'], oversample={'Low':True, 'Moderate':False, 'Considerable':False, 'High':True}, random_state=1, n_jobs=-1):\n",
    "        \"\"\"\n",
    "        Primary method to take a set of labels and pull the data for it\n",
    "        the data is large so generally this needs to be done it batches\n",
    "        and then stored on disk\n",
    "        For a set of labels and a target column from the labels set create the ML data\n",
    "\n",
    "        Keyword Arguments\n",
    "        labels: the set of labels we will randomly choose from\n",
    "        lookback_days: the number of days prior to the date in the label to also return which defines the timeseries (default: 14)\n",
    "        batch_size: the size of the data batch to return (default: 64)\n",
    "        y_column: the column in the label set to use as the label (default: Day1DangerAboveTreeline)\n",
    "        label_values: possible values for the y label (default: ['Low', 'Moderate', 'Considerable', 'High'])\n",
    "        oversample: dictionary defining which labels from the label_values set to apply naive oversampling to (default: {'Low':True, 'Moderate':False, 'Considerable':False, 'High':True})\n",
    "        random_state: define a state to force datasets to be returned in a reproducable fashion (deafault: 1)\n",
    "        n_jobs: number of processes to use (default: -1)\n",
    "        \"\"\"\n",
    "\n",
    "        labels_data = labels\n",
    "\n",
    "        X = None     \n",
    "        y = None \n",
    "\n",
    "        first = True   \n",
    "        first_y = True\n",
    "        num_in_place = 0\n",
    "        error_files = []\n",
    "        while num_in_place < batch_size:\n",
    "            if not first:\n",
    "                #if we didn't meet the full batch size \n",
    "                #continue appending until its full\n",
    "                #if num_in_place % 5 == 0:\n",
    "                print('Filling remaining have ' + str(num_in_place))\n",
    "                sample_size = batch_size-num_in_place\n",
    "                if sample_size < len(label_values):\n",
    "                    sample_size = len(label_values)\n",
    "            else: \n",
    "                sample_size = batch_size\n",
    "\n",
    "            batch_lookups = []\n",
    "            for l in label_values:\n",
    "                print('    on label: ' + l + ' with samplesize: ' + str(int(sample_size/len(label_values))))\n",
    "                print('    len: ' + str(len(labels_data[labels_data[y_column]==l])))\n",
    "                label_slice = labels_data[labels_data[y_column]==l]\n",
    "                size = int(sample_size/len(label_values))\n",
    "                #ensure the propose sample is larger than the available values\n",
    "                if len(label_slice) < size:\n",
    "                    size = len(label_slice)\n",
    "                if size > 0:\n",
    "                    batch_lookups.append(label_slice.sample(size, random_state=random_state))\n",
    "\n",
    "                    if not oversample[l]:\n",
    "                        labels_data = labels_data.drop(batch_lookups[-1].index, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            #sample frac=1 causes the data to be shuffled\n",
    "            batch_lookup = pd.concat(batch_lookups).sample(frac=1)\n",
    "            #print('lookup shape: ' + str(batch_lookup.shape))\n",
    "            batch_lookup.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            func = partial(self.process_sample, lookback_days=lookback_days)        \n",
    "            data = Parallel(n_jobs=n_jobs)(map(delayed(func), batch_lookup.iterrows()))        \n",
    "\n",
    "            #print('data has len: ' + str(len(data)))\n",
    "            to_delete = []\n",
    "            #delete backwards so we can delete by index\n",
    "            for i in reversed(range(len(data))):\n",
    "                #print('on i: ' + str(i))\n",
    "                if data[i] is None:\n",
    "                    print('deleting ' + str(i))\n",
    "                    del data[i]\n",
    "                    batch_lookup = batch_lookup.drop(i, axis=0)\n",
    "\n",
    "\n",
    "            for d in sorted(to_delete, reverse=True):\n",
    "                print('deleting ' + str(d))\n",
    "                del data[d]\n",
    "\n",
    "            for f in data:\n",
    "                if f is None:\n",
    "                    print('Still have none in data')\n",
    "\n",
    "            if first and len(data) > 0:                            \n",
    "                X = xr.concat(data, dim='sample') \n",
    "                y = batch_lookup\n",
    "                first = False            \n",
    "            elif not first and len(data) > 0:    \n",
    "                X_t = xr.concat(data, dim='sample')\n",
    "                X = xr.concat([X, X_t], dim='sample')#, coords='all', compat='override') \n",
    "                y = pd.concat([y, batch_lookup], axis=0)\n",
    "\n",
    "            num_in_place = y.shape[0]\n",
    "            #print('Num: ' + str(num_in_place))\n",
    "\n",
    "        y = y.reset_index(drop=True)\n",
    "        X = X.reindex({'sample': y.apply(lambda r: str(r[self.parsed_date_col]) + ': ' + r[self.region_col], axis=1)})\n",
    "        return X, y, labels_data\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_batch_simple(X, y):\n",
    "        \"\"\"\n",
    "        ensure, X and y indexes are aligned\n",
    "\n",
    "        Keyword Arguments\n",
    "        X: The X dataframe\n",
    "        y: the y dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        X = X.sortby(['sample', 'latitude', 'longitude'])\n",
    "\n",
    "        sample = y.apply(lambda row: '{}: {}'.format(row['parsed_date'], row['UnifiedRegion']), axis=1)\n",
    "        y['sample'] = sample\n",
    "        y = y.set_index(['sample', 'latitude', 'longitude'])\n",
    "        y.sort_index(inplace=True)    \n",
    "        y.reset_index(drop=False, inplace=True)\n",
    "        return X, y\n",
    "\n",
    " \n",
    "    def cache_batches(self, labels, batch_size=64, total_rows=6400, train_or_test='train', lookback_days=14, n_jobs=14):\n",
    "        \"\"\"\n",
    "        method to enable batches to be generated based on total amount of data as well as batch size\n",
    "        batches stores as zarr & parquet\n",
    "\n",
    "        Keyword Arguments\n",
    "        labels: the set of labels to choose from\n",
    "        batch_size: the number of samples to cache in a single batch (default: 64)\n",
    "        total_rows: the total number of rows to cache made up of multiple batches (default: 6400)\n",
    "        train_or_test: is this a train or test set--used in the file label (default: train)\n",
    "        lookback_days: number of days prior to the label date to include in the timeseries (default: 14)\n",
    "        n_jobs: number of processes to use (default: 14)  \n",
    "\n",
    "        Returns: remaining labels (labels which weren't used in the dataset creation)\n",
    "        \"\"\"\n",
    "        remaining_labels = labels\n",
    "        for i in range(0, total_rows, batch_size):\n",
    "            print(str(datetime.datetime.now()) + ' On ' + str(i) + ' of ' + str(total_rows))\n",
    "            X, y, remaining_labels = self.get_xr_batch(remaining_labels, lookback_days=lookback_days, batch_size=batch_size, n_jobs=n_jobs)\n",
    "            X.to_zarr(self.ml_path + 'X_' + train_or_test + '_' + str(i/batch_size) + '.zarr')\n",
    "            y.to_parquet(self.ml_path + 'y_' + train_or_test + '_' + str(i/batch_size) + '.parquet')\n",
    "        return remaining_labels\n",
    "    \n",
    " \n",
    "    def cache_batches_np(self,\n",
    "                         labels, \n",
    "                         batch_size=50, \n",
    "                         total_rows=10000, \n",
    "                         train_or_test='train', \n",
    "                         lookback_days=180, \n",
    "                         y_column='Day1DangerAboveTreeline', \n",
    "                         label_values=['Low', 'Moderate', 'Considerable', 'High'], \n",
    "                         oversample={'Low':True, 'Moderate':False, 'Considerable':False, 'High':True},\n",
    "                         n_jobs=14):\n",
    "        \"\"\"\n",
    "        method to enable batches to be generated based on total amount of data as well as batch size\n",
    "        batches returned for further processing\n",
    "\n",
    "        Keyword Arguments\n",
    "        labels: the set of labels to choose from\n",
    "        batch_size: the number of samples to cache in a single batch (default: 64)\n",
    "        total_rows: the total number of rows to cache made up of multiple batches (default: 6400)\n",
    "        train_or_test: is this a train or test set--used in the file label (default: train)\n",
    "        lookback_days: number of days prior to the label date to include in the timeseries (default: 14)\n",
    "        y_column: the column in the label set to use as the label (default: Day1DangerAboveTreeline)\n",
    "        label_values: possible values for the y label (default: ['Low', 'Moderate', 'Considerable', 'High'])\n",
    "        oversample: dictionary defining which labels from the label_values set to apply naive oversampling to (default: {'Low':True, 'Moderate':False, 'Considerable':False, 'High':True})    \n",
    "        n_jobs: number of processes to use (default: 14)  \n",
    "\n",
    "        Returns: tuple containing the batch *X,y) and remaining labels (labels which weren't used in the dataset creation)\n",
    "        \"\"\"\n",
    "        remaining_labels = labels\n",
    "        for i in range(0, total_rows, batch_size):\n",
    "            print(str(datetime.datetime.now()) + ' *On ' + str(i) + ' of ' + str(total_rows))\n",
    "            X, y, remaining_labels = self.get_xr_batch(remaining_labels, lookback_days=lookback_days, batch_size=batch_size, y_column=y_column, n_jobs=n_jobs)         \n",
    "\n",
    "        return PrepML.prepare_batch_simple(X, y), remaining_labels\n",
    "\n",
    "    #TODO: derive num_variables and lookback_days from the input set\n",
    "    #TODO: only write out one y file per X file\n",
    "    def create_memmapped(self, remaining_labels, train_or_test = 'train', num_variables=1131, num_rows = 10000, lookback_days=180, batch=0, batch_size=500):\n",
    "        \"\"\"\n",
    "        Generate a set of batches and store them in a memmapped numpy array\n",
    "        this is the technique used to prep data for timeseriesai notebook\n",
    "        Will store a single numpy X file in the ML directory as well as several y parquet files (one per batch size)\n",
    "        Keyword Arguments\n",
    "        remaining_labels: the set of labels to draw from\n",
    "        train_or_test: is this a train or test set--used in the file label (default: train)\n",
    "        num_variables: number of variables in the X set (default: 1131) \n",
    "        num_rows: total number of rows to store in the file (deafult: 10000)\n",
    "        lookback_days: number of days before the label date to include in the timeseries (default: 180)\n",
    "        batch: batch number to start in (default: 0) used in case you are generating multiple files\n",
    "        batch_size: number of rows to process at once to accomodate memory limitations (default: 500)\n",
    "        \"\"\"\n",
    "\n",
    "        # Save a small empty array\n",
    "        X_temp_fn = self.ml_path + '/temp_X.npy'\n",
    "        np.save(X_temp_fn, np.empty(1))\n",
    "\n",
    "        # Create a np.memmap with desired dtypes and shape of the large array you want to save.\n",
    "        # It's just a placeholder that doesn't contain any data\n",
    "        X_fn = self.ml_path + '/X' + train_or_test + '_batch_' + str(batch) + '_on_disk.npy'\n",
    "\n",
    "        X = np.memmap(X_temp_fn, dtype='float32', shape=(num_rows, num_variables, lookback_days))\n",
    "\n",
    "        # We are going to create a loop to fill in the np.memmap\n",
    "        start = 0\n",
    "        for i in range(0, num_rows, batch_size):\n",
    "            print('On ' + str(i) + ' of ' + str(num_rows))\n",
    "            # You now grab a chunk of your data that fits in memory\n",
    "            # This could come from a pandas dataframe for example        \n",
    "            dfs, remaining_labels = self.cache_batches_np(remaining_labels, batch_size=batch_size, total_rows=500)\n",
    "            #need to make sure all the variables are in the same order (there was an issue that they weren't between train and test sets)\n",
    "            X_df = dfs[0].sortby('variable')\n",
    "            y_df = dfs[1]\n",
    "            end = start + batch_size\n",
    "\n",
    "            # I now fill a slice of the np.memmap         \n",
    "            X[start:end] = X_df.vars.values[:batch_size] #sometimes the process will add a few extras, filter them\n",
    "\n",
    "            #just save y as parquet\n",
    "            y_df[:batch_size].to_parquet(self.ml_path + '/y_' + train_or_test + '_batch_' + str(batch) + '_' + str(i/batch_size) + '.parquet')\n",
    "            start = end\n",
    "            del X_df, y_df\n",
    "\n",
    "        #I can now remove the temp file I created\n",
    "        os.remove(X_temp_fn)\n",
    "\n",
    "        # Once the data is loaded on the np.memmap, I save it as a normal np.array\n",
    "        np.save(X_fn, X)\n",
    "        return remaining_labels\n",
    "\n",
    "\n",
    "    def concat_memapped(self, to_concat_filenames, dim_1_size=1131, dim_2_size=180):\n",
    "        \"\"\"\n",
    "        concat multiple numpy files on disk in to a single file\n",
    "        required for timeseriesai notebook as input to that is a single memmapped file containing X train and test data\n",
    "\n",
    "        Keyword Arguments:\n",
    "        to_concat_filenames: the files to concat\n",
    "        dim_1_size: number of variables in the files (default: 1131)\n",
    "        dim_2_size: number of lookback dates in the files (length of timeseries) (default: 180)\n",
    "        \"\"\"\n",
    "        to_concat = []\n",
    "        for i in range(len(to_concat_filenames)):\n",
    "            to_concat.append(np.load(to_concat_filenames[i], mmap_mode='r'))\n",
    "\n",
    "        dim_0_size = 0\n",
    "\n",
    "        for i in range(len(to_concat)):\n",
    "            dim_0_size += to_concat[i].shape[0]\n",
    "            assert(to_concat[i].shape[1] == dim_1_size)\n",
    "            assert(to_concat[i].shape[2] == dim_2_size)\n",
    "\n",
    "        X_temp_fn = self.ml_path + '/temp_X.npy'\n",
    "        np.save(X_temp_fn, np.empty(1))\n",
    "        X_fn = self.ml_path + '/X_all.npy'\n",
    "        X = np.memmap(X_temp_fn, dtype='float32', shape=(dim_0_size, dim_1_size, dim_2_size))\n",
    "        dim_0_start = 0\n",
    "        for i in range(len(to_concat)):\n",
    "            print('On file ' + str(i) + ' of ' + str(len(to_concat)))\n",
    "            dim_0 = to_concat[i].shape[0]\n",
    "            X[dim_0_start:dim_0_start+dim_0] = to_concat[i]\n",
    "            dim_0_start += dim_0\n",
    "\n",
    "\n",
    "        #I can now remove the temp file I created\n",
    "        os.remove(X_temp_fn)\n",
    "\n",
    "        # Once the data is loaded on the np.memmap, I save it as a normal np.array\n",
    "        np.save(X_fn, X)\n",
    "        del to_concat\n",
    "    \n",
    "    #TODO: add the ability to restart from a cached label file\n",
    "    def generate_train_test_local(self,\n",
    "                                  train_labels, \n",
    "                                  test_labels, \n",
    "                                  num_train_files=1, \n",
    "                                  num_test_files=1, \n",
    "                                  num_train_rows_per_file=10000, \n",
    "                                  num_test_rows_per_file=500, \n",
    "                                  num_variables=1131):\n",
    "        \"\"\"        \n",
    "        create several memapped files\n",
    "        we do this as the technique to create one has some memory limitations\n",
    "        also due to the memory limitations sometimes this process runs out of memory and crashes\n",
    "        which is why we cache the label state after every iteration so we can restart at that state\n",
    "        15 mins for 10000 rows using all 16 cores on my machine\n",
    "        I can generate a max of ~50000 rows per batch with 48 gb of ram before running out of memory\n",
    "        \"\"\"\n",
    "        \n",
    "        #get a sample so we can dump the feature labels\n",
    "        X, _, _ = self.get_xr_batch(train_labels, lookback_days=7, batch_size=4)   \n",
    "        pd.Series(X.variable.data).to_csv(self.ml_path + '/FeatureLabels.csv')\n",
    "        \n",
    "        filenames = []\n",
    "        for i in range(0, num_train_files):\n",
    "            remaining_labels_train = self.create_memmapped(train_labels, train_or_test = 'train', num_variables=num_variables, num_rows=num_train_rows_per_file, batch=i)\n",
    "            filenames.append(self.ml_path + '/Xtrain_batch_' + str(i) + '_on_disk.npy')\n",
    "            #with open(ml_path + 'remaining_labels_train.p', 'wb' ) as file:\n",
    "            #    pickle.dump(remaining_labels_train, file)\n",
    "\n",
    "\n",
    "        #same process for test\n",
    "        for i in range(0, num_test_files):\n",
    "            remaining_labels_test = self.create_memmapped(test_labels, train_or_test = 'test', num_variables=num_variables, num_rows=num_test_rows_per_file, batch=i)\n",
    "            filenames.append(self.ml_path + '/Xtest_batch_' + str(i) + '_on_disk.npy')\n",
    "            #with open(ml_path + 'remaining_labels_test.p', 'wb' ) as file:\n",
    "            #    pickle.dump(remaining_labels_test, file)\n",
    "        \n",
    "        self.concat_memapped(filenames, num_variables)\n",
    "        \n",
    "        return remaining_labels_train, remaining_labels_test\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ignore\n",
    "season = '15-16'\n",
    "state = 'Washington'\n",
    "\n",
    "interpolate = 1 #interpolation factor: whether we can to augment the data through lat/lon interpolation; 1 no interpolation, 4 is 4x interpolation\n",
    "\n",
    "data_root = '/media/scottcha/E1/Data/OAPMLData/'\n",
    "\n",
    "n_jobs = 4 #number of parallel processes, this processing is IO bound so don't set this too high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ignore\n",
    "pml = PrepML(data_root, interpolate, date_start='2015-11-01', date_end='2016-04-30', date_train_test_cutoff='2016-04-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ignore\n",
    "train, test = pml.prep_labels()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ignore\n",
    "pml.generate_train_test_local(train, test, num_variables=978)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo_small] *",
   "language": "python",
   "name": "conda-env-pangeo_small-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
