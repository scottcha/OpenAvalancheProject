{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp prep_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# This notebook is the primary notebook to take data in a per region format and transform it in to a format appropriate for ML problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ignore\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from functools import partial\n",
    "import zarr\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numcodecs\n",
    "from numcodecs import Blosc\n",
    "import dask\n",
    "dask.config.set(**{'array.slicing.split_large_chunks': False})\n",
    "debug = False\n",
    "numcodecs.blosc.use_threads = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data structure\n",
    "\n",
    "OAPMLData\\\n",
    "    CleanedForecastsNWAC_CAIC_UAC.V1.2013-2020.csv #labels downloaded from https://oapstorageprod.blob.core.windows.net/oap-training-data/Data/V1.1FeaturesWithLabels2013-2020.zip\n",
    "    \n",
    "\n",
    "    1.RawWeatherData/\n",
    "        gfs/\n",
    "            <season>/\n",
    "                /<state or country>/\n",
    "    2.GFSFiltered(x)Interpolation/\n",
    "    3.GFSFiltered(x)InterpolationZarr/\n",
    "    4.MLData\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PrepML:\n",
    "    def __init__(self, data_root, output_root = None, nc_root = None, interpolate=1, resample_length='3H', date_start='2015-11-01', date_end='2021-04-30', date_train_test_cutoff='2020-11-01'):\n",
    "        \"\"\"\n",
    "        Initialize the class\n",
    "        \n",
    "        Keyword Arguments\n",
    "        data_root: the root path of the data folders which contains the 4.GFSFiltered1xInterpolationZarr\n",
    "        output_root: optional path to a seperate output base path, if None then data_root is used as output_root\n",
    "        interpolate: the amount of interpolation applied in in the previous ParseGFS notebook (used for finding the correct input/output paths)        \n",
    "        resample_length: the grain the data was resampled to in 1.ParseGFS\n",
    "        date_start: Earlist date to include in label set (default: '2015-11-01')\n",
    "        date_end: Latest date to include in label set (default: '2020-04-30')\n",
    "        date_train_test_cutoff: Date to use as a cutoff between the train and test labels (default: '2019-11-01')\n",
    "        \"\"\"\n",
    "        self.data_root = data_root\n",
    "        if output_root is None:\n",
    "            self.output_root = data_root\n",
    "        else:\n",
    "            self.output_root = output_root\n",
    "            \n",
    "        if nc_root is None:\n",
    "            self.nc_root = data_root\n",
    "        else:\n",
    "            self.nc_root = nc_root\n",
    "        self.interpolation = interpolate\n",
    "        self.resample_length = resample_length\n",
    "        self.date_start = date_start\n",
    "        self.date_end = date_end\n",
    "        self.date_train_test_cutoff = date_train_test_cutoff\n",
    "        self.nc_path = self.nc_root + '/2.GFSFiltered'+ str(self.interpolation) + 'xInterpolation' + resample_length + '/'\n",
    "        self.processed_path = data_root + '/3.GFSFiltered'+ str(self.interpolation) + 'xInterpolationZarr' + resample_length + '/'\n",
    "        self.path_to_labels = data_root + 'CleanedForecastsNWAC_CAIC_UAC_CAC.V1.2013-2021.csv'\n",
    "        self.ml_path = self.output_root + '/4.MLData'\n",
    "        self.date_col = 'Day1Date'\n",
    "        self.region_col = 'UnifiedRegion'\n",
    "        self.parsed_date_col = 'parsed_date'\n",
    "        if not os.path.exists(self.ml_path):\n",
    "            os.makedirs(self.ml_path)\n",
    "            \n",
    "        #map states to regions for purposes of data lookup\n",
    "        self.regions = {\n",
    "            'Utah': ['Abajos', 'Logan', 'Moab', 'Ogden', 'Provo', \n",
    "                     'Salt Lake', 'Skyline', 'Uintas'],  \n",
    "            'Colorado': ['Grand Mesa Zone', 'Sangre de Cristo Range', 'Steamboat Zone', 'Front Range Zone',\n",
    "                         'Vail Summit Zone', 'Sawatch Zone', 'Aspen Zone', \n",
    "                         'North San Juan Mountains', 'South San Juan Mountains', 'Gunnison Zone'],\n",
    "            'Washington': ['Mt Hood', 'Olympics', 'Snoqualmie Pass', 'Stevens Pass',\n",
    "                           'WA Cascades East, Central', 'WA Cascades East, North', 'WA Cascades East, South',\n",
    "                           'WA Cascades West, Central', 'WA Cascades West, Mt Baker', 'WA Cascades West, South'],\n",
    "            'Canada': [\"Northwest Coastal\", \"Northwest Inland\", \"Sea To Sky\", \n",
    "                       \"South Coast Inland\", \"South Coast\", \"North Rockies\", \n",
    "                       \"Cariboos\", \"North Columbia\", \"South Columbia\", \"Purcells\", \n",
    "                       \"Kootenay Boundary\", \"South Rockies\", \"Lizard Range and Flathead\", \n",
    "                       \"Vancouver Island\", \"Kananaskis Country, Alberta Parks\", \"Chic Chocs, Avalanche Quebec\",\n",
    "                       \"Little Yoho\", \"Banff, Yoho and Kootenay National Parks\", \"Glacier National Park\",\n",
    "                       \"Waterton Lakes National Park\", \"Jasper National Park\"]\n",
    "        }\n",
    "    \n",
    "        \n",
    "    @staticmethod\n",
    "    def lookup_forecast_region(label_region):\n",
    "        \"\"\"\n",
    "        mapping between region names as the labels and the forecasts have slightly different standards\n",
    "        TODO: could add a unified mapping upstream in parseGFS files or in the label generation\n",
    "\n",
    "        Keyword Arguments:\n",
    "        label_region: region as defined in the labels file\n",
    "\n",
    "        returns the region as defined in the features\n",
    "        \"\"\"\n",
    "        if label_region == 'Mt Hood':\n",
    "            return 'Mt Hood'\n",
    "        elif label_region == 'Olympics':\n",
    "            return 'Olympics'\n",
    "        elif label_region == 'Cascade Pass - Snoq. Pass':\n",
    "            return 'Snoqualmie Pass'\n",
    "        elif label_region == 'Cascade Pass - Stevens Pass':\n",
    "            return 'Stevens Pass'\n",
    "        elif label_region == 'Cascade East - Central':\n",
    "            return 'WA Cascades East, Central'\n",
    "        elif label_region == 'Cascade East - North':\n",
    "            return 'WA Cascades East, North'\n",
    "        elif label_region == 'Cascade East - South':\n",
    "            return 'WA Cascades East, South'\n",
    "        elif label_region == 'Cascade West - Central':\n",
    "            return 'WA Cascades West, Central'\n",
    "        elif label_region == 'Cascade West - North':\n",
    "            return 'WA Cascades West, Mt Baker'\n",
    "        elif label_region == 'Cascade West - South':\n",
    "            return 'WA Cascades West, South'\n",
    "        elif label_region == 'Abajo':\n",
    "            return 'Abajos'\n",
    "        elif label_region == 'Logan':\n",
    "            return 'Logan'\n",
    "        elif label_region == 'Moab':\n",
    "            return 'Moab'\n",
    "        elif label_region == 'Ogden':\n",
    "            return 'Ogden'\n",
    "        elif label_region == 'Provo':\n",
    "            return 'Provo'\n",
    "        elif label_region == 'Salt Lake':\n",
    "            return 'Salt Lake'\n",
    "        elif label_region == 'Skyline':\n",
    "            return 'Skyline'\n",
    "        elif label_region == 'Uintas':\n",
    "            return 'Uintas'\n",
    "        elif label_region == 'Grand Mesa':\n",
    "            return 'Grand Mesa Zone'\n",
    "        elif label_region == 'Sangre de Cristo' or label_region == 'Sangre De Cristo':\n",
    "            return 'Sangre de Cristo Range'\n",
    "        elif label_region == 'Steamboat & Flat Tops':\n",
    "            return 'Steamboat Zone'\n",
    "        elif label_region == 'Front Range':\n",
    "            return 'Front Range Zone'\n",
    "        elif label_region == 'Vail & Summit County':\n",
    "            return 'Vail Summit Zone'\n",
    "        elif label_region == 'Sawatch Range':\n",
    "            return 'Sawatch Zone'\n",
    "        elif label_region == 'Aspen':\n",
    "            return 'Aspen Zone'\n",
    "        elif label_region == 'Northern San Juan':\n",
    "            return 'North San Juan Mountains'\n",
    "        elif label_region == 'Southern San Juan':\n",
    "            return 'South San Juan Mountains'\n",
    "        elif label_region == 'Gunnison':\n",
    "            return 'Gunnison Zone'\n",
    "        elif label_region == 'kananaskis':\n",
    "            return 'Kananaskis Country, Alberta Parks'\n",
    "        elif label_region == 'Banff  Yoho and Kootenay':\n",
    "            return 'Banff, Yoho and Kootenay National Parks'\n",
    "        elif label_region == 'Jasper':\n",
    "            return 'Jasper National Park'\n",
    "        elif label_region == 'Glacier':\n",
    "            return 'Glacier National Park'\n",
    "        elif label_region == 'Waterton Lakes':\n",
    "            return 'Waterton Lakes National Park'\n",
    "        elif label_region == 'Little Yoho':\n",
    "            return 'Little Yoho'\n",
    "        elif label_region == 'northwest-coastal':\n",
    "            return 'Northwest Coastal'\n",
    "        elif label_region == 'northwest-inland':\n",
    "            return 'Northwest Inland'\n",
    "        elif label_region == 'sea-to-sky':\n",
    "            return 'Sea To Sky'\n",
    "        elif label_region == 'south-coast-inland':\n",
    "            return 'South Coast Inland'\n",
    "        elif label_region == 'south-coast':\n",
    "            return 'South Coast'\n",
    "        elif label_region == 'north-rockies':\n",
    "            return 'North Rockies'\n",
    "        elif label_region == 'north-columbia':\n",
    "            return 'North Columbia'\n",
    "        elif label_region == 'south-columbia':\n",
    "            return 'South Columbia'\n",
    "        elif label_region == 'purcells':\n",
    "            return 'Purcells'\n",
    "        elif label_region == 'kootenay-boundary':\n",
    "            return 'Kootenay Boundary'\n",
    "        elif label_region == 'south-rockies':\n",
    "            return 'South Rockies'\n",
    "        elif label_region == 'lizard-range':\n",
    "            return 'Lizard Range and Flathead'\n",
    "        elif label_region == 'yukon':\n",
    "            return 'Yukon'\n",
    "        elif label_region == 'cariboos':\n",
    "            return 'Cariboos'        \n",
    "        else:\n",
    "            #print('Got region ' + label_region + ' but its not being looked up.')\n",
    "            return label_region\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def date_to_season(d):\n",
    "        \"\"\"\n",
    "        mapping of date to season\n",
    "        \n",
    "        Keyword Arguments\n",
    "        d: datetime64\n",
    "        \n",
    "        returns season indicator\n",
    "        \"\"\"\n",
    "        if d >= np.datetime64('2014-11-01') and d <= np.datetime64('2015-04-30'):\n",
    "            return (np.datetime64('2014-11-01'), '14-15')\n",
    "        elif d >= np.datetime64('2015-11-01') and d <= np.datetime64('2016-04-30'):\n",
    "            return (np.datetime64('2015-11-01'), '15-16')\n",
    "        elif d >= np.datetime64('2016-11-01') and d <= np.datetime64('2017-04-30'):\n",
    "            return (np.datetime64('2016-11-01'), '16-17')\n",
    "        elif d >= np.datetime64('2017-11-01') and d <= np.datetime64('2018-04-30'):\n",
    "            return (np.datetime64('2017-11-01'), '17-18')        \n",
    "        elif d >= np.datetime64('2018-11-01') and d <= np.datetime64('2019-04-30'):\n",
    "            return (np.datetime64('2018-11-01'), '18-19')        \n",
    "        elif d >= np.datetime64('2019-11-01') and d <= np.datetime64('2020-04-30'):            \n",
    "            return (np.datetime64('2019-11-01'), '19-20')\n",
    "        elif d >= np.datetime64('2020-11-01') and d <= np.datetime64('2021-04-30'):            \n",
    "            return (np.datetime64('2020-11-01'), '20-21')\n",
    "        else:\n",
    "            #print('Unknown season ' + str(d))\n",
    "            return (None,'Unknown')\n",
    "    \n",
    "   \n",
    "    def get_state_for_region(self, region):\n",
    "        \"\"\"\n",
    "        Returns the state for a given region\n",
    "        \n",
    "        Keywork Arguments\n",
    "        region: region we want to lookup the state for\n",
    "        \"\"\"\n",
    "        for k in self.regions.keys():\n",
    "            if region in self.regions[k]:\n",
    "                return k\n",
    "\n",
    "        raise Exception('No region with name ' + region)\n",
    "    \n",
    "    def prep_labels(self, overwrite_cache=True):\n",
    "        \"\"\"\n",
    "        Preps the data and lable sets in to two sets, train & test\n",
    "        \n",
    "        Keyword Arguments\n",
    "        overwrite_cache: True indicates we want to recalculate the lat/lon combos, False indicates use the values if they exist in the cache file (otherwise calcualte and cache it)\n",
    "        \n",
    "        returns the train & test sets\n",
    "        \"\"\"\n",
    "        \n",
    "        #find the season\n",
    "        nc_date = np.datetime64(self.date_start)\n",
    "        nc_season = PrepML.date_to_season(nc_date)[1]\n",
    "        \n",
    "        #maintaining this as a dict since the arrays are ragged and its more efficient this way\n",
    "        #storing one sample for each region to get the lat/lon layout\n",
    "        region_zones = []\n",
    "        region_data = {}\n",
    "        for region in self.regions.keys():\n",
    "            for r in self.regions[region]:               \n",
    "                region_zones.append(r)\n",
    "                region_data[r] = xr.open_dataset(self.nc_path + nc_season + '/Region_' + r + '_' + pd.to_datetime(nc_date).strftime('%Y%m%d') + '.nc')\n",
    "        \n",
    "        #Read in all the label data\n",
    "        self.labels = pd.read_csv(self.path_to_labels, low_memory=False,\n",
    "                dtype={'Day1Danger_OctagonAboveTreelineEast': 'object',\n",
    "                       'Day1Danger_OctagonAboveTreelineNorth': 'object',\n",
    "                       'Day1Danger_OctagonAboveTreelineNorthEast': 'object',\n",
    "                       'Day1Danger_OctagonAboveTreelineNorthWest': 'object',\n",
    "                       'Day1Danger_OctagonAboveTreelineSouth': 'object',\n",
    "                       'Day1Danger_OctagonAboveTreelineSouthEast': 'object',\n",
    "                       'Day1Danger_OctagonAboveTreelineSouthWest': 'object',\n",
    "                       'Day1Danger_OctagonAboveTreelineWest': 'object',\n",
    "                       'Day1Danger_OctagonBelowTreelineEast': 'object',\n",
    "                       'Day1Danger_OctagonBelowTreelineNorth': 'object',\n",
    "                       'Day1Danger_OctagonBelowTreelineNorthEast': 'object',\n",
    "                       'Day1Danger_OctagonBelowTreelineNorthWest': 'object',\n",
    "                       'Day1Danger_OctagonBelowTreelineSouth': 'object',\n",
    "                       'Day1Danger_OctagonBelowTreelineSouthEast': 'object',\n",
    "                       'Day1Danger_OctagonBelowTreelineSouthWest': 'object',\n",
    "                       'Day1Danger_OctagonBelowTreelineWest': 'object',\n",
    "                       'Day1Danger_OctagonNearTreelineEast': 'object',\n",
    "                       'Day1Danger_OctagonNearTreelineNorth': 'object',\n",
    "                       'Day1Danger_OctagonNearTreelineNorthEast': 'object',\n",
    "                       'Day1Danger_OctagonNearTreelineNorthWest': 'object',\n",
    "                       'Day1Danger_OctagonNearTreelineSouth': 'object',\n",
    "                       'Day1Danger_OctagonNearTreelineSouthEast': 'object',\n",
    "                       'Day1Danger_OctagonNearTreelineSouthWest': 'object',\n",
    "                       'Day1Danger_OctagonNearTreelineWest': 'object',\n",
    "                       'SpecialStatement': 'object',\n",
    "                       'image_paths': 'object',\n",
    "                       'image_types': 'object',\n",
    "                       'image_urls': 'object'})\n",
    "     \n",
    "        self.labels['parsed_date'] = pd.to_datetime(self.labels[self.date_col], format='%Y%m%d')\n",
    "        \n",
    "        metadata_cols = [self.date_col, self.region_col]\n",
    "        #ensure we are only using label data for regions we are looking at\n",
    "        #return region_zones\n",
    "        self.labels[self.region_col] = self.labels.apply(lambda x : PrepML.lookup_forecast_region(x[self.region_col]), axis=1)        \n",
    "        \n",
    "        self.labels = self.labels[self.labels[self.region_col].isin(region_zones)]                        \n",
    "        self.labels = self.labels[self.labels[self.region_col]!='Unknown region']\n",
    "        \n",
    "        #add a season column\n",
    "        tmp = pd.DataFrame.from_records(self.labels[self.parsed_date_col].apply(PrepML.date_to_season).reset_index(drop=True))\n",
    "        self.labels.reset_index(drop=True, inplace=True)\n",
    "        self.labels['season'] = tmp[1]\n",
    "        #some region/seasons have excessive errors in the data, remove those\n",
    "        self.labels = self.labels[self.labels['season'].isin(['15-16', '16-17', '17-18', '18-19', '19-20', '20-21'])]\n",
    "        self.labels = self.labels[~self.labels.index.isin(self.labels[(self.labels['season']=='15-16') & (self.labels[self.region_col]=='Steamboat Zone')].index)]\n",
    "        self.labels = self.labels[~self.labels.index.isin(self.labels[(self.labels['season']=='16-17') & (self.labels[self.region_col]=='Front Range Zone')].index)]               \n",
    "        \n",
    "        lat_lon_union = pd.DataFrame()\n",
    "        lat_lon_path = self.processed_path + 'lat_lon_union.csv'\n",
    "        if overwrite_cache or not os.path.exists(lat_lon_path):   \n",
    "            #find union of all lat/lon/region to just grids with values\n",
    "            #the process to filter the lat/lon is expensive but we need to do it here (1-5 seconds per region)\n",
    "            #as the helps the batch process select relevant data\n",
    "            for r in region_data.keys():\n",
    "                print(r)\n",
    "                region_df = region_data[r].stack(lat_lon = ('latitude', 'longitude')).lat_lon.to_dataframe()            \n",
    "                tmp_df = pd.DataFrame.from_records(region_df['lat_lon'].values, columns=['latitude', 'longitude'])\n",
    "                indexes_to_drop = []\n",
    "                for index, row in tmp_df.iterrows():\n",
    "                    #TODO: there might be a more efficient way than doing this one by one?\n",
    "                    if 0 == np.count_nonzero(region_data[r].to_array().sel(latitude=row['latitude'], longitude=row['longitude']).stack(time_var = ('time', 'variable')).dropna(dim='time_var', how='all').values):\n",
    "                        indexes_to_drop.append(index)\n",
    "                tmp_df.drop(indexes_to_drop, axis=0, inplace=True)\n",
    "                tmp_df[self.region_col] = r\n",
    "                lat_lon_union = pd.concat([lat_lon_union, tmp_df])        \n",
    "        \n",
    "                #cache the data\n",
    "                lat_lon_union.to_csv(lat_lon_path)\n",
    "        else:\n",
    "            #load the cached data\n",
    "            lat_lon_union = pd.read_csv(lat_lon_path,float_precision='round_trip')\n",
    "        #join in with the labels so we have a label per lat/lon pair\n",
    "        lat_lon_union = lat_lon_union.set_index(self.region_col, drop=False).join(self.labels.set_index(self.region_col, drop=False), how='left', lsuffix='left', rsuffix='right')\n",
    "     \n",
    "        #define the split between train and test\n",
    "        date_min = np.datetime64(self.date_start)\n",
    "        date_max = np.datetime64(self.date_end)\n",
    "        train_date_cutoff = np.datetime64(self.date_train_test_cutoff)\n",
    "\n",
    "        #split the train/test data\n",
    "        labels_data_union = lat_lon_union[lat_lon_union[self.parsed_date_col] >= date_min]\n",
    "        labels_data_union = labels_data_union[labels_data_union[self.parsed_date_col] <= date_max]\n",
    "        #copy so we can delete the overall data and only keep the filtered\n",
    "        labels_data_train = labels_data_union[labels_data_union[self.parsed_date_col] <= train_date_cutoff].copy()\n",
    "        labels_data_test = labels_data_union[labels_data_union[self.parsed_date_col] > train_date_cutoff].copy()\n",
    "        labels_data_train.reset_index(inplace=True)\n",
    "        labels_data_test.reset_index(inplace=True)\n",
    "        \n",
    "        return labels_data_train, labels_data_test\n",
    "    \n",
    "    \n",
    "    def augment_labels_with_trends(self, label_to_add_trend_info='Day1DangerAboveTreelineValue'):\n",
    "        raise NotImplementedError('Method is not fully implemented or tested')\n",
    "        #add extra labels which also allow us to have labels which indicate the trend in the avy direction\n",
    "        #the thought here is that predicting a rise or flat danger is usually easier than predicting when \n",
    "        #to lower the danger so seperating these in to seperate clases\n",
    "        #TODO: this should be dynamic based on label passed in, not hard coded to above treeline\n",
    "        labels_trends = pd.DataFrame()\n",
    "        for r in self.labels[self.region_col].unique():\n",
    "            for s in self.labels['season'].unique():\n",
    "                region_season_df = self.labels[self.labels['season']==s]\n",
    "                region_season_df = region_season_df[region_season_df[self.region_col]==r]\n",
    "                if(len(region_season_df) == 0):\n",
    "                    continue\n",
    "                region_season_df.sort_values(by='parsed_date', inplace=True)\n",
    "                region_season_df.reset_index(inplace=True, drop=True)\n",
    "                region_season_df[label_to_add_trend_info] = region_season_df['Day1DangerAboveTreeline'].map({'Low':0, 'Moderate':1, 'Considerable':2, 'High':3})\n",
    "                region_season_df.loc[0,'Day1DangerAboveTreelineWithTrend'] = region_season_df.iloc[0]['Day1DangerAboveTreeline'] + '_Initial'\n",
    "\n",
    "                for i in range(1,len(region_season_df)):\n",
    "                    prev = region_season_df.iloc[i-1]['Day1DangerAboveTreelineValue']\n",
    "                    cur = region_season_df.loc[i,'Day1DangerAboveTreelineValue']\n",
    "                    trend = '_Unknown'\n",
    "                    if prev == cur:\n",
    "                        trend = '_Flat'\n",
    "                    elif prev < cur:\n",
    "                        trend = '_Rising'\n",
    "                    elif prev >  cur:\n",
    "                        trend = '_Falling'\n",
    "\n",
    "                    region_season_df.loc[i,'Day1DangerAboveTreelineWithTrend'] = region_season_df.iloc[i]['Day1DangerAboveTreeline'] + trend\n",
    "                labels_trends = pd.concat([labels_trends,region_season_df])\n",
    "        assert(len(labels_trends)==len(self.labels))\n",
    "        self.labels = labels_trends\n",
    "\n",
    "    def get_data_zarr(self, region, lat, lon, lookback_days, date, variables=None):\n",
    "        \"\"\"\n",
    "        utility to get data for a specific point\n",
    "        \n",
    "        Keyword Arguments\n",
    "        region: the region the point exists in\n",
    "        lat: the latitude of the point to lookup\n",
    "        lon: the longitude of the point to lookup\n",
    "        lookback_days: the number of days prior to the date to also return\n",
    "        date: the date which marks the end of the dataset (same date as the desired label)\n",
    "        variables: filter to just these variables (default: None indicates return all variables)\n",
    "        \"\"\"\n",
    "        #print(region + ' ' + str(lat) + ', ' + str(lon) + ' ' + str(date))\n",
    "        state = self.get_state_for_region(region)\n",
    "        earliest_data, season = PrepML.date_to_season(date)\n",
    "\n",
    "        path = self.processed_path + '/' + season + '/' + state + '/Region_' + region + '.zarr'\n",
    "        #print('Opening file ' + path)\n",
    "\n",
    "        tmp_ds = xr.open_zarr(path, consolidated=True)\n",
    "        \n",
    "        #filter to just the variables we want\n",
    "        #TODO: this may be more efficient if we use the open_zarr drop to not even read the variables\n",
    "        if variables is not None:\n",
    "            tmp_ds = tmp_ds.sel(variable=tmp_ds.variable.isin(variables))\n",
    "        \n",
    "        start_day = date - np.timedelta64(lookback_days-1, 'D')\n",
    "        #print('start day ' + str(start_day))\n",
    "        tmp_ds = tmp_ds.sel(latitude=lat, longitude=lon, method='nearest').sel(time=slice(start_day, date + np.timedelta64(24, 'h')))        \n",
    "        \n",
    "\n",
    "        date_values_pd = pd.date_range(start=start_day, end=date + np.timedelta64(24, 'h'), freq=self.resample_length)\n",
    "        #date_values_pd = pd.date_range(start_day, periods=lookback_days, freq='D')\n",
    "        #reindex should fill missing values with NA\n",
    "        tmp_ds = tmp_ds.reindex({'time': date_values_pd})\n",
    "\n",
    "        tmp_ds = tmp_ds.reset_index(dims_or_levels='time', drop=True).load()\n",
    "        return tmp_ds\n",
    "    \n",
    "    def get_data_zarr_batch(self, region, season, df, lookback_days, variables=None):\n",
    "        \"\"\"\n",
    "        utility to get data for a set of points\n",
    "        \n",
    "        Keyword Arguments\n",
    "        region: the region the point exists in\n",
    "        season: the season the data is in\n",
    "        df: DataFrame of label rows to pull the data for (should all be from the same region and season)\n",
    "        lookback_days: the number of days prior to the date to also return        \n",
    "        variables: filter to just these variables (default: None indicates return all variables)\n",
    "        \"\"\"\n",
    "        \n",
    "        state = self.get_state_for_region(region)\n",
    "        \n",
    "\n",
    "        path = self.processed_path + '/' + season + '/' + state + '/Region_' + region + '.zarr'\n",
    "        #print('*Opening file ' + path)\n",
    "        \n",
    "        #finds the minimal set of values for the single zarr collection and then appends\n",
    "        #the individaul data to results\n",
    "        results = []\n",
    "       \n",
    "        lats = df['latitude'].unique()\n",
    "        lons = df['longitude'].unique()\n",
    "       \n",
    "        tmp_ds = xr.open_zarr(path, consolidated=True)        \n",
    "        min_ds = tmp_ds.sel(latitude=lats, longitude=lons)\n",
    "      \n",
    "        #filter to just the variables we want\n",
    "        #TODO: this may be more efficient if we use the open_zarr drop to not even read the variables\n",
    "        if variables is not None:\n",
    "            #print('***in var filter with var length ' + str(len(variables)))\n",
    "            min_ds = min_ds.sel(variable=min_ds.variable.isin(variables))\n",
    "        else:\n",
    "            print('***variables is none')\n",
    "\n",
    "        loaded_df = min_ds.sel(latitude=df['latitude'].unique(), longitude=df['longitude'].unique()).load()    \n",
    "        for d in df.iterrows():\n",
    "            d = d[1]\n",
    "            date = d['parsed_date']\n",
    "            start_day = date - np.timedelta64(lookback_days-1, 'D')\n",
    "            \n",
    "            result_df = loaded_df.sel(latitude=d['latitude'], longitude=d['longitude']).sel(time=slice(start_day, date + np.timedelta64(24, 'h')))\n",
    "            #print(str(d['latitude']) + ' ' + str(d['longitude']) + ' ' + str(start_day) + ' ' + str(date))\n",
    "            #return result_df\n",
    "            date_values_pd = pd.date_range(start=start_day, end=date + np.timedelta64(24, 'h'), freq=self.resample_length)\n",
    "            #date_values_pd = pd.date_range(start_day, periods=lookback_days, freq='D')\n",
    "            #reindex should fill missing values with NA\n",
    "            result_df = result_df.reindex({'time': date_values_pd})\n",
    "            result_df = result_df.assign_coords({'sample': date.strftime('%Y%m%d') + ' ' + region}).expand_dims('sample')\n",
    "            results.append(result_df.reset_index(dims_or_levels='time', drop=True))        \n",
    "     \n",
    "        return results\n",
    "    \n",
    "\n",
    "\n",
    "    def process_sample2(self, iter_tuple, df, lookback_days, variables):\n",
    "        region = iter_tuple[1]['UnifiedRegion']\n",
    "        season = iter_tuple[1]['season']\n",
    "        df_r = df[df['UnifiedRegion']==region]               \n",
    "        df_r_s = df_r[df_r['season']==season]\n",
    "        #print('in process sample2')\n",
    "        return self.get_data_zarr_batch(region=region, \n",
    "                                        season=season, \n",
    "                                        df=df_r_s, \n",
    "                                        lookback_days=lookback_days,\n",
    "                                        variables=variables)\n",
    "    \n",
    "    def get_xr_batch(self, \n",
    "                     labels, \n",
    "                     lookback_days=14, \n",
    "                     batch_size=64, \n",
    "                     y_column='Day1DangerAboveTreeline', \n",
    "                     label_values=['Low', 'Moderate', 'Considerable', 'High'], \n",
    "                     oversample={'Low':True, 'Moderate':False, 'Considerable':False, 'High':True}, \n",
    "                     random_state=None,\n",
    "                     variables = None,\n",
    "                     n_jobs=1):\n",
    "        \"\"\"\n",
    "        Primary method to take a set of labels and pull the data for it\n",
    "        the data is large so generally this needs to be done it batches\n",
    "        and then stored on disk\n",
    "        For a set of labels and a target column from the labels set create the ML data\n",
    "\n",
    "        Keyword Arguments\n",
    "        labels: the set of labels we will randomly choose from\n",
    "        lookback_days: the number of days prior to the date in the label to also return which defines the timeseries (default: 14)\n",
    "        batch_size: the size of the data batch to return (default: 64)\n",
    "        y_column: the column in the label set to use as the label (default: Day1DangerAboveTreeline)\n",
    "        label_values: possible values for the y label (default: ['Low', 'Moderate', 'Considerable', 'High'])\n",
    "        oversample: dictionary defining which labels from the label_values set to apply naive oversampling to (default: {'Low':True, 'Moderate':False, 'Considerable':False, 'High':True})\n",
    "        random_state: define a state to force datasets to be returned in a reproducable fashion (default: None)\n",
    "                      if using oversample this needs to be None or you will get the same set of values every time\n",
    "        varaibles: variables to include (default: None which indicates include all variables)\n",
    "        n_jobs: number of processes to use (default: -1)\n",
    "        \"\"\"\n",
    "        if debug:\n",
    "            print('Getting a batch for label ' + y_column)\n",
    "        labels_data = labels\n",
    "\n",
    "        X = None     \n",
    "        y = None \n",
    "\n",
    "        first = True   \n",
    "        first_y = True\n",
    "        num_in_place = 0\n",
    "        error_files = []\n",
    "        while num_in_place < batch_size:\n",
    "            if not first:\n",
    "                #if we didn't meet the full batch size \n",
    "                #continue appending until its full\n",
    "                #if num_in_place % 5 == 0:\n",
    "                if(debug):\n",
    "                    print('Filling remaining have ' + str(num_in_place))\n",
    "                sample_size = batch_size-num_in_place\n",
    "                if sample_size < len(label_values):\n",
    "                    sample_size = len(label_values)\n",
    "            else: \n",
    "                sample_size = batch_size\n",
    "\n",
    "            batch_lookups = []\n",
    "            size = int(sample_size/len(label_values))\n",
    "            #copy this so we can modify label_values during the loop without affecting the iteration\n",
    "            label_iter = label_values.copy()\n",
    "            if debug:\n",
    "                print('have label_iter ' + str(label_iter))\n",
    "            for l in label_iter:\n",
    "                if debug:\n",
    "                    print('    on label: ' + l + ' with samplesize: ' + str(size))\n",
    "                label_len = len(labels_data[labels_data[y_column]==l])\n",
    "                if debug: print('    len: ' + str(label_len))\n",
    "                if label_len == 0:\n",
    "                    #we don't have any more of this label, remove it from the list so we can continue efficiently\n",
    "                    if debug: print('    No more values for label: ' + l)\n",
    "                    label_values.remove(l)\n",
    "                    continue\n",
    "                    \n",
    "                label_slice = labels_data[labels_data[y_column]==l]\n",
    "                \n",
    "                #ensure the propose sample is larger than the available values\n",
    "                pick_size = size\n",
    "                if len(label_slice) < pick_size:\n",
    "                    pick_size = len(label_slice)\n",
    "                    \n",
    "                if pick_size > 0:\n",
    "                    batch_lookups.append(label_slice.sample(pick_size, random_state=random_state))\n",
    "\n",
    "                    if not oversample[l]:\n",
    "                        labels_data = labels_data.drop(batch_lookups[-1].index, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            #sample frac=1 causes the data to be shuffled\n",
    "            if len(batch_lookups) == 0:\n",
    "                #no more data left\n",
    "                break\n",
    "                \n",
    "            batch_lookup = pd.concat(batch_lookups).sample(frac=1, random_state=random_state)\n",
    "            #print('lookup shape: ' + str(batch_lookup.shape))\n",
    "            batch_lookup.reset_index(inplace=True, drop=True)\n",
    "            if debug: print('have n_jobs ' + str(n_jobs))\n",
    "       \n",
    "            tuples = batch_lookup[['UnifiedRegion', 'season']].drop_duplicates()\n",
    "            func = partial(self.process_sample2, df=batch_lookup, lookback_days=lookback_days, variables=variables)        \n",
    "            data2 = Parallel(n_jobs=n_jobs, backend=\"loky\")(map(delayed(func), tuples.iterrows())) \n",
    "            #data2 = []\n",
    "            #for r in tuples.iterrows():      \n",
    "            #    print('foo')\n",
    "            #    data2.append(self.process_sample2(r, df=batch_lookup, lookback_days=lookback_days, variables=variables))\n",
    "            \n",
    "            data = [item for sublist in data2 for item in sublist]\n",
    "            \n",
    "            if first and len(data) > 0:                            \n",
    "                X = xr.concat(data, dim='sample') \n",
    "                y = batch_lookup\n",
    "                first = False            \n",
    "            elif not first and len(data) > 0:    \n",
    "                X_t = xr.concat(data, dim='sample')\n",
    "                X = xr.concat([X, X_t], dim='sample')#, coords='all', compat='override') \n",
    "                y = pd.concat([y, batch_lookup], axis=0)\n",
    "\n",
    "            num_in_place = y.shape[0]\n",
    "        \n",
    "\n",
    "        X = X.sortby(['sample', 'latitude', 'longitude'])\n",
    "        y['sample'] = y['parsed_date'].dt.strftime('%Y%m%d') + ': ' + y['UnifiedRegion']\n",
    "        y = y.sort_values(['sample', 'latitude', 'longitude']).reset_index(drop=True)\n",
    "\n",
    "        \n",
    "        return X, y, labels_data\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_batch_simple(X, y):\n",
    "        \"\"\"\n",
    "        ensure, X and y indexes are aligned\n",
    "\n",
    "        Keyword Arguments\n",
    "        X: The X dataframe\n",
    "        y: the y dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        X = X.sortby(['sample', 'latitude', 'longitude'])\n",
    "\n",
    "        sample = y.apply(lambda row: '{}: {}'.format(row['parsed_date'], row['UnifiedRegion']), axis=1)\n",
    "        y['sample'] = sample\n",
    "        y = y.set_index(['sample', 'latitude', 'longitude'])\n",
    "        y.sort_index(inplace=True)    \n",
    "        y.reset_index(drop=False, inplace=True)\n",
    "        return X, y\n",
    "\n",
    " \n",
    "    def create_zarr(self,                     \n",
    "                    remaining_labels, \n",
    "                    zarr_file,\n",
    "                    variables,\n",
    "                    train_or_test = 'train',                          \n",
    "                    num_rows = 10000, \n",
    "                    lookback_days=180, \n",
    "                    batch=0, \n",
    "                    batch_size=500,\n",
    "                    y_column='Day1DangerAboveTreeline', \n",
    "                    label_values=['Low', 'Moderate', 'Considerable', 'High'], \n",
    "                    oversample={'Low':True, 'Moderate':False, 'Considerable':False, 'High':True},  \n",
    "                    file_label = '',\n",
    "                    n_jobs=1):\n",
    "        \n",
    "        num_variables = len(variables)            \n",
    "        \n",
    "        # We are going to create a loop to fill in the file\n",
    "        start = 0\n",
    "        y = pd.DataFrame()\n",
    "        for i in range(0, num_rows, batch_size):\n",
    "            if debug: print('On ' + str(i) + ' of ' + str(num_rows))\n",
    "            # You now grab a chunk of your data that fits in memory\n",
    "            # This could come from a pandas dataframe for example        \n",
    "            X_df, y_df, remaining_labels = self.get_xr_batch(remaining_labels, \n",
    "                                                       lookback_days=lookback_days, \n",
    "                                                       batch_size=batch_size, \n",
    "                                                       y_column=y_column, \n",
    "                                                       label_values=label_values,\n",
    "                                                       oversample=oversample,\n",
    "                                                       variables=variables,\n",
    "                                                       n_jobs=n_jobs)         \n",
    "        \n",
    "            \n",
    "            X_df, y_df = PrepML.prepare_batch_simple(X_df, y_df)\n",
    "            \n",
    "            #need to make sure all the variables are in the same order (there was an issue that they weren't between train and test sets)\n",
    "            X_df = X_df.sortby('variable')\n",
    "            \n",
    "            if batch_size > X_df.vars.values.shape[0]:\n",
    "                end = start + X_df.vars.values.shape[0]\n",
    "            else:\n",
    "                end = start + batch_size\n",
    "            \n",
    "            offset = batch * num_rows\n",
    "            if debug: print('start: ' + str(start) + ' end: ' + str(end), ' offset: ' + str(offset))\n",
    "            #print(str(X.shape))\n",
    "            if debug: print(str(X_df.vars.values.shape))\n",
    "            if debug: print(str(batch_size))\n",
    "            if debug: print(str(offset) + ' ' + str(offset + end))\n",
    "            # I now fill a slice of the zarr file            \n",
    "            zarr_file[offset + start:offset + end] = X_df.vars.values[:batch_size] #sometimes the process will add a few extras, filter them\n",
    "\n",
    "            \n",
    "            #y_df[:batch_size].to_parquet(self.ml_path + '/y_' + train_or_test + '_batch_' + str(batch) + '_' + file_label + '_' + str(i/batch_size) + '.parquet')\n",
    "            y = pd.concat([y, y_df[:batch_size]])\n",
    "            start = end\n",
    "            del X_df, y_df\n",
    "        \n",
    "        y.to_parquet(self.ml_path + '/y_' + train_or_test + '_batch_' + str(batch) + '_' + file_label + '.parquet')\n",
    "        return remaining_labels\n",
    "    \n",
    "    #TODO: derive lookback_days from the input set\n",
    "    #TODO: only write out one y file per X file\n",
    "    def create_memmapped(self, \n",
    "                         remaining_labels, \n",
    "                         variables,\n",
    "                         train_or_test = 'train',                          \n",
    "                         num_rows = 10000, \n",
    "                         lookback_days=180, \n",
    "                         batch=0, \n",
    "                         batch_size=500,\n",
    "                         y_column='Day1DangerAboveTreeline', \n",
    "                         label_values=['Low', 'Moderate', 'Considerable', 'High'], \n",
    "                         oversample={'Low':True, 'Moderate':False, 'Considerable':False, 'High':True},\n",
    "                         file_label='',\n",
    "                         n_jobs=1):\n",
    "        \"\"\"\n",
    "        Generate a set of batches and store them in a memmapped numpy array\n",
    "        this is the technique used to prep data for timeseriesai notebook\n",
    "        Will store a single numpy X file in the ML directory as well as several y parquet files (one per batch size)\n",
    "        \n",
    "        Keyword Arguments\n",
    "        remaining_labels: the set of labels to draw from\n",
    "        variables: the variables to include, required\n",
    "        train_or_test: is this a train or test set--used in the file label (default: train)\n",
    "        num_variables: number of variables in the X set (default: 1131) \n",
    "        num_rows: total number of rows to store in the file (deafult: 10000)\n",
    "        lookback_days: number of days before the label date to include in the timeseries (default: 180)\n",
    "        batch: batch number to start in (default: 0) used in case you are generating multiple files\n",
    "        batch_size: number of rows to process at once to accomodate memory limitations (default: 500)\n",
    "        y_column: the column in the label set to use as the label (default: Day1DangerAboveTreeline)\n",
    "        label_values: possible values for the y label (default: ['Low', 'Moderate', 'Considerable', 'High'])\n",
    "        oversample: dictionary defining which labels from the label_values set to apply naive oversampling to (default: {'Low':True, 'Moderate':False, 'Considerable':False, 'High':True})    \n",
    "        file_label: optional label for files to distinguish different datasets\n",
    "        n_jobs: number of parallel jobs to run        \n",
    "        \"\"\"\n",
    "\n",
    "        num_variables = len(variables)\n",
    "        \n",
    "        # Save a small empty array\n",
    "        X_temp_fn = self.ml_path + '/temp_X.npy'\n",
    "        np.save(X_temp_fn, np.empty(1))\n",
    "\n",
    "        # Create a np.memmap with desired dtypes and shape of the large array you want to save.\n",
    "        # It's just a placeholder that doesn't contain any data        \n",
    "        X_fn = self.ml_path + '/X' + train_or_test + '_batch_' + str(batch) + '_' + file_label + '_on_disk.npy'\n",
    "\n",
    "        X = None\n",
    "        \n",
    "\n",
    "        # We are going to create a loop to fill in the np.memmap\n",
    "        start = 0\n",
    "        y = pd.DataFrame()\n",
    "        for i in range(0, num_rows, batch_size):\n",
    "            print('On ' + str(i) + ' of ' + str(num_rows))\n",
    "            # You now grab a chunk of your data that fits in memory\n",
    "            # This could come from a pandas dataframe for example        \n",
    "            X_df, y_df, remaining_labels = self.get_xr_batch(remaining_labels, \n",
    "                                                       lookback_days=lookback_days, \n",
    "                                                       batch_size=batch_size, \n",
    "                                                       y_column=y_column, \n",
    "                                                       label_values=label_values,\n",
    "                                                       oversample=oversample,\n",
    "                                                       variables=variables,\n",
    "                                                       n_jobs=n_jobs)         \n",
    "        \n",
    "            \n",
    "            X_df, y_df = PrepML.prepare_batch_simple(X_df, y_df)\n",
    "            \n",
    "            #need to make sure all the variables are in the same order (there was an issue that they weren't between train and test sets)\n",
    "            X_df = X_df.sortby('variable')\n",
    "            \n",
    "            if batch_size > X_df.vars.values.shape[0]:\n",
    "                end = start + X_df.vars.values.shape[0]\n",
    "            else:\n",
    "                end = start + batch_size\n",
    "\n",
    "            if debug: print('start: ' + str(start) + ' end: ' + str(end))\n",
    "            #print(str(X.shape))\n",
    "            if debug: print(str(X_df.vars.values.shape))\n",
    "            if debug: print(str(batch_size))\n",
    "            # I now fill a slice of the np.memmap  \n",
    "            if X is None:\n",
    "                X = np.memmap(X_temp_fn, dtype='float32', shape=(num_rows, num_variables, X_df.vars.values.shape[2]))\n",
    "            \n",
    "            X[start:end] = X_df.vars.values[:batch_size] #sometimes the process will add a few extras, filter them\n",
    "\n",
    "            \n",
    "            #y_df[:batch_size].to_parquet(self.ml_path + '/y_' + train_or_test + '_batch_' + str(batch) + '_' + file_label + '_' + str(i/batch_size) + '.parquet')\n",
    "            y = pd.concat([y, y_df[:batch_size]])\n",
    "            start = end\n",
    "            del X_df, y_df\n",
    "\n",
    "        #I can now remove the temp file I created\n",
    "        os.remove(X_temp_fn)\n",
    "\n",
    "        # Once the data is loaded on the np.memmap, I save it as a normal np.array\n",
    "        np.save(X_fn, X)\n",
    "        y.to_parquet(self.ml_path + '/y_' + train_or_test + '_batch_' + str(batch) + '_' + file_label + '.parquet')\n",
    "        return remaining_labels, X_fn\n",
    "\n",
    "\n",
    "    def concat_memapped(self, to_concat_filenames, file_label='', dim_1_size=1131, dim_2_size=180, temp_destination_path=None):\n",
    "        \"\"\"\n",
    "        concat multiple numpy files on disk in to a single file\n",
    "        required for timeseriesai notebook as input to that is a single memmapped file containing X train and test data\n",
    "\n",
    "        Keyword Arguments:\n",
    "        to_concat_filenames: the files to concat\n",
    "        dim_1_size: number of variables in the files (default: 1131)\n",
    "        dim_2_size: number of lookback dates in the files (length of timeseries) (default: 180)\n",
    "        destination_path: alternate path to put the concat file \n",
    "        \"\"\"\n",
    "        \n",
    "        if temp_destination_path is None:\n",
    "            temp_destination_path = self.ml_path\n",
    "            \n",
    "        to_concat = []\n",
    "        for i in range(len(to_concat_filenames)):\n",
    "            to_concat.append(np.load(to_concat_filenames[i], mmap_mode='r'))\n",
    "\n",
    "        dim_0_size = 0\n",
    "\n",
    "        for i in range(len(to_concat)):\n",
    "            dim_0_size += to_concat[i].shape[0]\n",
    "            assert to_concat[i].shape[1] == dim_1_size, 'Expected dim1 shape as ' + str(dim_1_size) + ' but got ' + str(to_concat[i].shape[1])\n",
    "            assert to_concat[i].shape[2] == dim_2_size, 'Expected dim2 shape as ' + str(dim_2_size) + ' but got ' + str(to_concat[i].shape[2])\n",
    "\n",
    "        X_temp_fn = temp_destination_path + '/temp_X.npy'\n",
    "        np.save(X_temp_fn, np.empty(1))\n",
    "        X_fn = self.ml_path + '/X_all' + '_' + file_label + '.npy'\n",
    "        X = np.memmap(X_temp_fn, dtype='float32', shape=(dim_0_size, dim_1_size, dim_2_size))\n",
    "        dim_0_start = 0\n",
    "        for i in range(len(to_concat)):\n",
    "            print('On file ' + str(i) + ' of ' + str(len(to_concat)))\n",
    "            dim_0 = to_concat[i].shape[0]\n",
    "            X[dim_0_start:dim_0_start+dim_0] = to_concat[i]\n",
    "            dim_0_start += dim_0\n",
    "\n",
    "\n",
    "        #I can now remove the temp file I created\n",
    "        os.remove(X_temp_fn)\n",
    "\n",
    "        # Once the data is loaded on the np.memmap, I save it as a normal np.array\n",
    "        np.save(X_fn, X)\n",
    "        del to_concat\n",
    "    \n",
    "\n",
    "    \n",
    "    def filter_features(self, feature_list):\n",
    "        \"\"\"\n",
    "        Filter list of variable names to a smaller set based on name heuristics\n",
    "        \"\"\"\n",
    "        #remove any prefixed with var\n",
    "        feature_list = set([x for x in feature_list if 'var' not in x])\n",
    "\n",
    "        #operate only on avg and sum\n",
    "        feature_list = set([x for x in feature_list if 'min' not in x])\n",
    "        feature_list = set([x for x in feature_list if 'max' not in x])\n",
    "\n",
    "        #get any surface levels\n",
    "        surface_features = set([x for x in feature_list if 'surface' in x])\n",
    "        feature_list = feature_list - surface_features\n",
    "\n",
    "        #get any relative to ground\n",
    "        aboveground_features = set([x for x in feature_list if 'aboveground' in x])\n",
    "        feature_list = feature_list - aboveground_features\n",
    "\n",
    "        feature_prefix = set([x.split('_')[0] for x in feature_list])\n",
    "        pressure_features = []\n",
    "        for p in feature_prefix:\n",
    "            #get a random subset for the features defined by pressure level\n",
    "            tmp = [x for x in feature_list if p in x]\n",
    "            tmp2 = pd.Series(tmp).sample(frac=.1)\n",
    "            #return tmp2\n",
    "            pressure_features.extend(list(tmp2))\n",
    "\n",
    "        l = list(surface_features) + list(aboveground_features) + list(pressure_features)\n",
    "        l.sort()\n",
    "        return l\n",
    "\n",
    "    #TODO: add the ability to restart from a cached label file\n",
    "    def generate_train_test_local(self,\n",
    "                                  train_labels, \n",
    "                                  test_labels, \n",
    "                                  num_train_files=1, \n",
    "                                  num_test_files=1, \n",
    "                                  num_train_rows_per_file=1000, \n",
    "                                  num_test_rows_per_file=500,\n",
    "                                  batch_size=500,\n",
    "                                  lookback_days = 180,\n",
    "                                  y_column='Day1DangerAboveTreeline', \n",
    "                                  label_values=['Low', 'Moderate', 'Considerable', 'High'], \n",
    "                                  oversample={'Low':True, 'Moderate':False, 'Considerable':False, 'High':True},\n",
    "                                  file_label = '',\n",
    "                                  temp_destination_path=None,\n",
    "                                  file_type = 'zarr',\n",
    "                                  filter_vars = True,\n",
    "                                  n_jobs=1,\n",
    "                                  compressor='lz4',\n",
    "                                  clevel='6'):\n",
    "        \"\"\"        \n",
    "        create several memapped files\n",
    "        we do this as the technique to create one has some memory limitations\n",
    "        also due to the memory limitations sometimes this process runs out of memory and crashes\n",
    "        which is why we cache the label state after every iteration so we can restart at that state\n",
    "        15 mins for 10000 rows using all 16 cores on my machine\n",
    "        I can generate a max of ~50000 rows per batch with 48 gb of ram before running out of memory\n",
    "        \n",
    "        Keyword Arguments:\n",
    "        y_column: the column in the label set to use as the label (default: Day1DangerAboveTreeline)\n",
    "        label_values: possible values for the y label (default: ['Low', 'Moderate', 'Considerable', 'High'])\n",
    "        oversample: dictionary defining which labels from the label_values set to apply naive oversampling to (default: {'Low':True, 'Moderate':False, 'Considerable':False, 'High':True})    \n",
    "        file_label: optional file label to add to the on disk files to distinguish between data sets\n",
    "        temp_destination_path: alternate path to put the concat file temporarily (improves disk throughput)\n",
    "        \"\"\"\n",
    "        assert num_train_rows_per_file % batch_size == 0, 'num_train_rows_per_file needs to be a multiple of batch_size'\n",
    "        assert batch_size <= num_train_rows_per_file, 'num_train_rows_per_file needs to be greater than batch_size'\n",
    "        assert num_test_rows_per_file % batch_size == 0, 'num_test_rows_per_file needs to be a multiple of batch_size'        \n",
    "        assert batch_size <= num_test_rows_per_file, 'num_test_rows_per_file needs to be greater than batch_size'\n",
    "        #not all seasons have the same # of variables so find the common subset first\n",
    "        train_seasons = train_labels['season'].unique()\n",
    "        test_seasons = test_labels['season'].unique()\n",
    "        #find the common vars for each season\n",
    "        #pull one sample of data for each season\n",
    "        data = {}\n",
    "        for s in train_seasons:\n",
    "            label = train_labels[train_labels['season'] == s].sample(n = 1)\n",
    "            assert(len(label==1))\n",
    "            data[s] = self.get_data_zarr(label.iloc[0]['UnifiedRegion'], label.iloc[0]['latitude'], label.iloc[0]['longitude'], lookback_days, label.iloc[0]['parsed_date'])\n",
    "\n",
    "        for s in test_seasons:\n",
    "            label = test_labels[test_labels['season'] == s].sample(n = 1)\n",
    "            assert(len(label==1))\n",
    "            data[s] = self.get_data_zarr(label.iloc[0]['UnifiedRegion'], label.iloc[0]['latitude'], label.iloc[0]['longitude'],  lookback_days, label.iloc[0]['parsed_date'])\n",
    "        \n",
    "        v = []\n",
    "        for d in data.keys():\n",
    "            v.append(set(data[d].variable.values))\n",
    "        final_vars = list(set.intersection(*v))\n",
    "        if debug: print(\"Len of vars: \" + str(len(final_vars))) \n",
    "        if filter_vars:\n",
    "            final_vars = self.filter_features(final_vars)\n",
    "        \n",
    "        #get a sample so we can dump the feature labels\n",
    "        X, _, _ = self.get_xr_batch(train_labels, variables=final_vars, lookback_days=lookback_days, batch_size=4, n_jobs=n_jobs)   \n",
    "        if debug: print(\"Shape of X.time is: \" + str(len(X.time.data)))\n",
    "        pd.Series(X.variable.data).to_csv(self.ml_path + '/FeatureLabels_' + file_label + '.csv')\n",
    "        label_values_copy = label_values.copy()\n",
    "        if file_type == 'zarr':\n",
    "            file_path = self.ml_path + '/X' + '_' + file_label + '.zarr'\n",
    "            c = None\n",
    "            if compressor is not None:\n",
    "                c = Blosc(cname=compressor, clevel=clevel, shuffle=Blosc.BITSHUFFLE)            \n",
    "            zarr_arr = zarr.open(file_path, mode='a', chunks=(1,-1, -1), \n",
    "                                 compressor=c,\n",
    "                                 shape=(num_train_files * num_train_rows_per_file + \n",
    "                                        num_test_files * num_test_rows_per_file, \n",
    "                                        len(final_vars), len(X.time.data)), \n",
    "                                 dtype=np.float32)\n",
    "            for i in range(0, num_train_files):\n",
    "                print('Train: on file ' + str(i+1) + ' of ' + str(num_train_files))\n",
    "                train_labels = self.create_zarr(train_labels, \n",
    "                                                zarr_file=zarr_arr,\n",
    "                                                variables = final_vars,\n",
    "                                                train_or_test = 'train',                                                            \n",
    "                                                num_rows=num_train_rows_per_file, \n",
    "                                                batch=i,\n",
    "                                                batch_size=batch_size,\n",
    "                                                y_column=y_column,\n",
    "                                                lookback_days=lookback_days,\n",
    "                                                label_values=label_values,\n",
    "                                                oversample=oversample,    \n",
    "                                                file_label=file_label,\n",
    "                                                n_jobs=n_jobs)\n",
    "            oversample_test = {}\n",
    "            for k in oversample.keys():\n",
    "                oversample_test[k] = False\n",
    "\n",
    "            #same process for test\n",
    "            for i in range(0, num_test_files):\n",
    "                print('Test: on file ' + str(i+1) + ' of ' + str(num_test_files))\n",
    "                test_labels = self.create_zarr(test_labels, \n",
    "                                               zarr_file=zarr_arr,\n",
    "                                               variables = final_vars,\n",
    "                                               train_or_test = 'test',                                                           \n",
    "                                               num_rows=num_test_rows_per_file, \n",
    "                                               batch=i + num_train_files,\n",
    "                                               batch_size=batch_size,\n",
    "                                               y_column=y_column,\n",
    "                                               lookback_days=lookback_days,\n",
    "                                               label_values=label_values_copy,\n",
    "                                               oversample=oversample_test,\n",
    "                                               file_label=file_label,\n",
    "                                               n_jobs=n_jobs)\n",
    "            return file_path, train_labels, test_labels\n",
    "                \n",
    "        else:        \n",
    "            filenames = []\n",
    "            #keep a copy so that we can ensure test gets the full set\n",
    "            \n",
    "            for i in range(0, num_train_files):\n",
    "                train_labels, files = self.create_memmapped(train_labels, \n",
    "                                                            variables = final_vars,\n",
    "                                                            train_or_test = 'train',                                                            \n",
    "                                                            num_rows=num_train_rows_per_file, \n",
    "                                                            batch=i,\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            y_column=y_column,\n",
    "                                                            label_values=label_values,\n",
    "                                                            oversample=oversample,\n",
    "                                                            file_label=file_label,\n",
    "                                                            n_jobs=n_jobs)\n",
    "                filenames.append(files)\n",
    "\n",
    "            #test files shouldn't use oversampling\n",
    "            oversample_test = {}\n",
    "            for k in oversample.keys():\n",
    "                oversample_test[k] = False\n",
    "\n",
    "            #same process for test\n",
    "            for i in range(0, num_test_files):\n",
    "                test_labels, files = self.create_memmapped(test_labels, \n",
    "                                                           variables = final_vars,\n",
    "                                                           train_or_test = 'test',                                                           \n",
    "                                                           num_rows=num_test_rows_per_file, \n",
    "                                                           batch=i,\n",
    "                                                           batch_size=batch_size,\n",
    "                                                           y_column=y_column,\n",
    "                                                           label_values=label_values_copy,\n",
    "                                                           oversample=oversample_test,\n",
    "                                                           file_label=file_label,\n",
    "                                                           n_jobs=n_jobs)\n",
    "                filenames.append(files)\n",
    "        \n",
    "            return filenames, train_labels, test_labels\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_start_date = '2015-11-01'\n",
    "test_data_end_date = '2016-04-30'\n",
    "test_data_train_cutoff = '2016-03-31'\n",
    "test_data_regions = {'Colorado': ['Grand Mesa Zone', 'Sangre de Cristo Range']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prep_labels():\n",
    "    interpolate = 1 #interpolation factor: whether we can to augment the data through lat/lon interpolation; 1 no interpolation, 4 is 4x interpolation\n",
    "    data_root = '../TestData/'\n",
    "    pml = PrepML(data_root, interpolate=interpolate, date_start=test_data_start_date, date_end=test_data_end_date, date_train_test_cutoff=test_data_train_cutoff)\n",
    "    pml.regions = {'Colorado': ['Grand Mesa Zone', 'Sangre de Cristo Range']}\n",
    "    train, test = pml.prep_labels() \n",
    "    assert len(train) > 0, 'Expected len of test set to be > 0, got ' + str(len(train))\n",
    "    assert len(test) > 0, 'Expected len of train set to be > 0, got ' + str(len(test))\n",
    "    assert (train['season'].unique() == ['15-16'])[0], 'Expected only one season in train, got ' + train['season'].unique()\n",
    "    assert (test['season'].unique() == ['15-16'])[0], 'Expected only one season in test, got ' + test['season'].unique()\n",
    "    assert train['parsed_date'].max() == np.datetime64(test_data_train_cutoff), 'Expected max train date of ' + test_data_train_cutoff + ', got ' + str(train['parsed_date'].max())\n",
    "    assert train['parsed_date'].min() == np.datetime64('2015-11-13'), 'Expected min train date of 2015-11-13, got ' + str(train['parsed_date'].min())\n",
    "    assert test['parsed_date'].max() == np.datetime64('2016-04-20'), 'Expected max test date of 2016-04-20, got ' + str(test['parsed_date'].max())    \n",
    "    assert test['parsed_date'].min() == np.datetime64('2016-04-01'), 'Expected min test date of 2016-04-01, got ' + str(test['parsed_date'].min())\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grand Mesa Zone\n",
      "Sangre de Cristo Range\n"
     ]
    }
   ],
   "source": [
    "train, test = test_prep_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def test_prep_labels_overwrite_cache():\n",
    "    interpolate = 1 #interpolation factor: whether we can to augment the data through lat/lon interpolation; 1 no interpolation, 4 is 4x interpolation\n",
    "    data_root = '../TestData/'\n",
    "    pml = PrepML(data_root, interpolate=interpolate, date_start=test_data_start_date, date_end=test_data_end_date, date_train_test_cutoff=test_data_train_cutoff)\n",
    "    pml.regions = {'Colorado': ['Grand Mesa Zone']}\n",
    "    #get cache\n",
    "    cache_path = pml.processed_path + 'lat_lon_union.csv'\n",
    "    if os.path.isfile(cache_path):  \n",
    "        os.remove(cache_path)\n",
    "        assert os.path.isfile(cache_path) == False, 'Expected cache file to be removed, its was not'\n",
    "    #at this point there should be no cache\n",
    "\n",
    "    train, test = pml.prep_labels(overwrite_cache=True) \n",
    "    cache_grandmesa_only = pd.read_csv(cache_path)\n",
    "    result = len(cache_grandmesa_only['UnifiedRegion'].unique())\n",
    "    assert result == 1, 'Expeted only one region in cache, got ' + str(result)\n",
    "    pml.regions = test_data_regions\n",
    "\n",
    "    #regenerate but don't overwrite cache    \n",
    "    train, test = pml.prep_labels(overwrite_cache=False) \n",
    "    cache_grandmesa_only = pd.read_csv(cache_path)\n",
    "    result = len(cache_grandmesa_only['UnifiedRegion'].unique())\n",
    "    assert result == 1, 'Expeted only one region in cache after overwrite_cache=False, got ' + str(result)\n",
    "    \n",
    "    train, test = pml.prep_labels(overwrite_cache=True) \n",
    "    cache_both = pd.read_csv(cache_path)\n",
    "    result = len(cache_both['UnifiedRegion'].unique())\n",
    "    assert result == 2, 'Expeted two regions in cache, got ' + str(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grand Mesa Zone\n",
      "Grand Mesa Zone\n",
      "Sangre de Cristo Range\n"
     ]
    }
   ],
   "source": [
    "test_prep_labels_overwrite_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_zarr(d, test_lat, test_lon, test_lookback):\n",
    "    assert d.latitude.values == test_lat, 'Expected ' + str(test_lat) + ' got ' + str(d.latitude.values)\n",
    "    assert d.longitude.values == test_lon, 'Expected ' + str(test_lon) + ' got ' + str(d.longitude.values)\n",
    "    assert len(d.time.values) == test_lookback * 8 + 1, 'Expected ' + str(test_lookback * 8 + 1) + ' got ' + str(len(d.time.values)) #3 hour intervals + 1 for boundary\n",
    "    assert len(d.variable.values) == 981, 'Expected 981 variables, got ' + str(len(d.variable.values))\n",
    "    assert d.isel(variable=10).variable == 'ABSV_150mb_max'\n",
    "    assert d.isel(variable=0).variable == 'ABSV_1000mb_avg'\n",
    "    assert d.isel(variable=980).variable == 'WILT_surface_min'\n",
    "    assert np.isnan(d.vars.values).all() == False\n",
    "    \n",
    "def test_get_data_zarr():\n",
    "    #self, region, lat, lon, lookback_days, date, variables=None):    \n",
    "    interpolate = 1 \n",
    "    data_root = '../TestData/'\n",
    "    pml = PrepML(data_root, interpolate=interpolate, date_start=test_data_start_date, date_end=test_data_end_date, date_train_test_cutoff=test_data_train_cutoff)\n",
    "    pml.regions = test_data_regions\n",
    "    test_region = 'Grand Mesa Zone'\n",
    "    test_lat = 39.25\n",
    "    test_lon = -108.25\n",
    "    test_lookback = 7\n",
    "    test_date = np.datetime64('2016-01-01')\n",
    "    \n",
    "    data = pml.get_data_zarr(test_region, lat=test_lat, lon=test_lon, lookback_days=test_lookback, date=test_date)\n",
    "    validate_zarr(data, test_lat, test_lon, test_lookback)\n",
    "    \n",
    "    test_date = np.datetime64('2016-02-01')\n",
    "    data = pml.get_data_zarr(test_region, lat=test_lat, lon=test_lon, lookback_days=test_lookback, date=test_date)\n",
    "    validate_zarr(data, test_lat, test_lon, test_lookback)    \n",
    "      \n",
    "    test_date = np.datetime64('2015-12-19')\n",
    "    data = pml.get_data_zarr(test_region, lat=test_lat, lon=test_lon, lookback_days=test_lookback, date=test_date)\n",
    "    validate_zarr(data, test_lat, test_lon, test_lookback)\n",
    "    \n",
    "    test_region = 'Sangre de Cristo Range'\n",
    "    test_lat = 38.5\n",
    "    test_lon = -106.0\n",
    "    test_lookback = 180\n",
    "    data = pml.get_data_zarr(test_region, lat=test_lat, lon=test_lon, lookback_days=test_lookback, date=test_date)\n",
    "\n",
    "    validate_zarr(data, test_lat, test_lon, test_lookback)\n",
    "    \n",
    "    \n",
    "    return data\n",
    "\n",
    "def validate_zarr_filter(d, test_lat, test_lon, test_lookback):\n",
    "    assert d.latitude.values == test_lat, 'Expected ' + str(test_lat) + ' got ' + str(d.latitude.values)\n",
    "    assert d.longitude.values == test_lon\n",
    "    assert len(d.time.values) == test_lookback * 8 + 1\n",
    "    assert len(d.variable.values) == 3\n",
    "    assert d.isel(variable=0).variable == 'ABSV_1000mb_avg'\n",
    "    assert d.isel(variable=1).variable == 'ABSV_150mb_max'\n",
    "    assert d.isel(variable=2).variable == 'WILT_surface_min'\n",
    "    assert np.isnan(d.vars.values).all() == False\n",
    "    \n",
    "def test_get_data_zarr_filter():\n",
    "    #self, region, lat, lon, lookback_days, date, variables=None):    \n",
    "    interpolate = 1 \n",
    "    data_root = '../TestData/'\n",
    "    pml = PrepML(data_root, interpolate=interpolate, date_start=test_data_start_date, date_end=test_data_end_date, date_train_test_cutoff=test_data_train_cutoff)\n",
    "    pml.regions = test_data_regions\n",
    "    v =  ['ABSV_150mb_max', 'ABSV_1000mb_avg', 'WILT_surface_min']\n",
    "    test_region = 'Grand Mesa Zone'\n",
    "    test_lat = 39.25\n",
    "    test_lon = -108.25\n",
    "    test_lookback = 7\n",
    "    test_date = np.datetime64('2016-01-01')\n",
    "    data = pml.get_data_zarr(test_region, lat=test_lat, lon=test_lon, lookback_days=test_lookback, date=test_date, variables=v)\n",
    "    validate_zarr_filter(data, test_lat, test_lon, test_lookback)\n",
    "    \n",
    "    test_date = np.datetime64('2016-02-01')\n",
    "    data = pml.get_data_zarr(test_region, lat=test_lat, lon=test_lon, lookback_days=test_lookback, date=test_date, variables=v)\n",
    "    validate_zarr_filter(data, test_lat, test_lon, test_lookback)\n",
    "    \n",
    "    test_date = np.datetime64('2015-12-19')\n",
    "    data = pml.get_data_zarr(test_region, lat=test_lat, lon=test_lon, lookback_days=test_lookback, date=test_date, variables=v)\n",
    "    validate_zarr_filter(data, test_lat, test_lon, test_lookback)\n",
    "    \n",
    "    test_region = 'Sangre de Cristo Range'\n",
    "    test_lat = 38.5\n",
    "    test_lon = -106.0\n",
    "    test_lookback = 180\n",
    "    data = pml.get_data_zarr(test_region, lat=test_lat, lon=test_lon, lookback_days=test_lookback, date=test_date, variables=v)\n",
    "\n",
    "    validate_zarr_filter(data, test_lat, test_lon, test_lookback)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = test_get_data_zarr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = test_get_data_zarr_filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_data_zarr_batch():\n",
    "    interpolate = 1 \n",
    "    data_root = '../TestData/'\n",
    "    pml = PrepML(data_root, interpolate=interpolate, date_start=test_data_start_date, date_end=test_data_end_date, date_train_test_cutoff=test_data_train_cutoff)\n",
    "    pml.regions = test_data_regions\n",
    "    train, test = pml.prep_labels()\n",
    "    test_region = 'Grand Mesa Zone'\n",
    "    df = train[train['UnifiedRegion']==test_region].head(3)\n",
    "    print('Grand Mesa Zone head')\n",
    "    test_lat = df.iloc[0]['latitude']\n",
    "    test_lon = df.iloc[0]['longitude']\n",
    "    test_lookback = 7\n",
    "\n",
    "    data = pml.get_data_zarr_batch(test_region, season='15-16', df=df, lookback_days=test_lookback)\n",
    "    for x in data:\n",
    "        validate_zarr(x, test_lat, test_lon, test_lookback)\n",
    "    \n",
    "    print('Grand Mesa Zone tail')\n",
    "    df = train[train['UnifiedRegion']==test_region].tail(3)\n",
    "    test_lat = df.iloc[0]['latitude']\n",
    "    test_lon = df.iloc[0]['longitude']\n",
    "    data = pml.get_data_zarr_batch(test_region, season='15-16', df=df, lookback_days=test_lookback)\n",
    "\n",
    "    for x in data:\n",
    "        validate_zarr(x, test_lat, test_lon, test_lookback)\n",
    "    \n",
    "    print('Sangre de Cristo Range head')\n",
    "    test_region = 'Sangre de Cristo Range'\n",
    "    df = train[train['UnifiedRegion']==test_region].head(3)\n",
    "    test_lat = df.iloc[0]['latitude']\n",
    "    test_lon = df.iloc[0]['longitude']\n",
    "    test_lookback = 180\n",
    "    data = pml.get_data_zarr_batch(test_region, season='15-16', df=df, lookback_days=test_lookback)\n",
    "\n",
    "    for x in data:\n",
    "        validate_zarr(x, test_lat, test_lon, test_lookback)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def test_get_data_zarr_filter_batch():\n",
    "    #self, region, lat, lon, lookback_days, date, variables=None):    \n",
    "    interpolate = 1 \n",
    "    data_root = '../TestData/'\n",
    "    pml = PrepML(data_root, interpolate=interpolate, date_start=test_data_start_date, date_end=test_data_end_date, date_train_test_cutoff=test_data_train_cutoff)\n",
    "    pml.regions = test_data_regions\n",
    "    v =  ['ABSV_150mb_max', 'ABSV_1000mb_avg', 'WILT_surface_min']\n",
    "    train, test = pml.prep_labels()\n",
    "    test_region = 'Grand Mesa Zone'\n",
    "    df = train[train['UnifiedRegion']==test_region].head(3)\n",
    "    print('Grand Mesa Zone head')\n",
    "    test_lat = df.iloc[0]['latitude']\n",
    "    test_lon = df.iloc[0]['longitude']\n",
    "    test_lookback = 7\n",
    "\n",
    "    data = pml.get_data_zarr_batch(test_region, season='15-16', df=df, lookback_days=test_lookback, variables=v)\n",
    "    \n",
    "    for x in data:\n",
    "        validate_zarr_filter(x, test_lat, test_lon, test_lookback)\n",
    "    \n",
    "    print('Grand Mesa Zone tail')\n",
    "    df = train[train['UnifiedRegion']==test_region].tail(3)\n",
    "    test_lat = df.iloc[0]['latitude']\n",
    "    test_lon = df.iloc[0]['longitude']\n",
    "    data = pml.get_data_zarr_batch(test_region, season='15-16', df=df, lookback_days=test_lookback, variables=v)\n",
    "\n",
    "    for x in data:\n",
    "        validate_zarr_filter(x, test_lat, test_lon, test_lookback)\n",
    "    \n",
    "    print('Sangre de Cristo Range head')\n",
    "    test_region = 'Sangre de Cristo Range'\n",
    "    df = train[train['UnifiedRegion']==test_region].head(3)\n",
    "    \n",
    "    test_lookback = 180\n",
    "    data = pml.get_data_zarr_batch(test_region, season='15-16', df=df, lookback_days=test_lookback, variables=v)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    for i in range(len(data)):\n",
    "        test_lat = df.iloc[i]['latitude']\n",
    "        test_lon = df.iloc[i]['longitude']\n",
    "        validate_zarr_filter(data[i], test_lat, test_lon, test_lookback)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grand Mesa Zone\n",
      "Sangre de Cristo Range\n",
      "Grand Mesa Zone head\n",
      "***variables is none\n",
      "Grand Mesa Zone tail\n",
      "***variables is none\n",
      "Sangre de Cristo Range head\n",
      "***variables is none\n"
     ]
    }
   ],
   "source": [
    "d = test_get_data_zarr_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grand Mesa Zone\n",
      "Sangre de Cristo Range\n",
      "Grand Mesa Zone head\n",
      "Grand Mesa Zone tail\n",
      "Sangre de Cristo Range head\n"
     ]
    }
   ],
   "source": [
    "d = test_get_data_zarr_filter_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_date_to_season():\n",
    "    result = PrepML.date_to_season(np.datetime64('2015-11-01'))\n",
    "    assert result[0] == np.datetime64('2015-11-01'), 'Expected 2015-11-01 got ' + str(result[0])\n",
    "    assert result[1] == '15-16', 'Expected 15-16 got ' + result[1]\n",
    "    \n",
    "    result = PrepML.date_to_season(np.datetime64('2015-10-01'))\n",
    "    assert result[0] == None, 'Expected None got ' + str(result[0])\n",
    "    assert result[1] == 'Unknown', 'Expected Unknown got ' + result[1]\n",
    "    \n",
    "    result = PrepML.date_to_season(np.datetime64('2016-04-30'))\n",
    "    assert result[0] == np.datetime64('2015-11-01'), 'Expected 2015-11-01 got ' + str(result[0])\n",
    "    assert result[1] == '15-16', 'Expected 15-16 got ' + result[1]\n",
    "    \n",
    "    result = PrepML.date_to_season(np.datetime64('2016-05-01'))\n",
    "    assert result[0] == None, 'Expected None got ' + str(result[0])\n",
    "    assert result[1] == 'Unknown', 'Expected Unknown got ' + result[1]\n",
    "    \n",
    "    result = PrepML.date_to_season(np.datetime64('2020-01-01'))\n",
    "    assert result[0] == np.datetime64('2019-11-01'), 'Expected 2019-11-01 got ' + str(result[0])\n",
    "    assert result[1] == '19-20', 'Expected 19-20 got ' + result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date_to_season()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_state_for_region():\n",
    "    interpolate = 1 #interpolation factor: whether we can to augment the data through lat/lon interpolation; 1 no interpolation, 4 is 4x interpolation\n",
    "    data_root = '../TestData/'\n",
    "    pml = PrepML(data_root, interpolate=interpolate, date_start=test_data_start_date, date_end=test_data_end_date, date_train_test_cutoff=test_data_train_cutoff)\n",
    "    pml.regions = test_data_regions\n",
    "    \n",
    "    result = pml.get_state_for_region('Grand Mesa Zone')\n",
    "    assert result == 'Colorado', 'Expected Colorado got ' + result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_get_state_for_region()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_equal(a, b):\n",
    "    return np.array_equiv(np.nan_to_num(a),np.nan_to_num(b))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_xr_batch():\n",
    "    interpolate = 1 \n",
    "    data_root = '../TestData/'\n",
    "    pml = PrepML(data_root, interpolate=interpolate, date_start=test_data_start_date, date_end=test_data_end_date, date_train_test_cutoff=test_data_train_cutoff)\n",
    "    pml.regions = test_data_regions\n",
    "    train, test = pml.prep_labels()\n",
    "    lookback = 7\n",
    "    X, y, l  = pml.get_xr_batch(train,                     \n",
    "                             lookback_days=lookback, \n",
    "                             batch_size=18)\n",
    "\n",
    "    #validate shape\n",
    "    assert X.vars.shape == (20, 981, 57), 'Have X.vars.shape of ' + str(X.vars.shape)\n",
    "    assert y.shape[0] == 20\n",
    "    \n",
    "    #check the data returned\n",
    "    for s in range(0, 20, 3):\n",
    "        \n",
    "        s_df = X.isel(sample=s)\n",
    "        \n",
    "        sample = s_df.sample.values.item().split(' ', 1) #only split first to keep regions with space togeter\n",
    "        test_region = sample[1]\n",
    "        test_lat = s_df.latitude.values.item()\n",
    "        test_lon = s_df.longitude.values.item()\n",
    "        test_lookback = lookback\n",
    "        test_date = np.datetime64(pd.to_datetime(sample[0], format='%Y%m%d'))\n",
    "        \n",
    "        data = pml.get_data_zarr(test_region, lat=test_lat, lon=test_lon, lookback_days=test_lookback, date=test_date)\n",
    "        \n",
    "        validate_zarr(data, test_lat, test_lon, test_lookback)\n",
    "\n",
    "        y2 = y[s:s+1]\n",
    "        \n",
    "        data2 = pml.get_data_zarr_batch(y2.iloc[0]['UnifiedRegion'], season='15-16', df=y2, lookback_days=lookback)\n",
    "\n",
    "        assert is_equal(data.vars.values, s_df.vars.values)\n",
    "        \n",
    "        assert is_equal(data.vars.values, data2[0].vars.values[0])          \n",
    "          \n",
    "                     #y_column='Day1DangerAboveTreeline', \n",
    "                     #label_values=['Low', 'Moderate', 'Considerable', 'High'], \n",
    "                     #oversample={'Low':True, 'Moderate':False, 'Considerable':False, 'High':True}, \n",
    "                     #random_state=1,\n",
    "                     #variables = None,\n",
    "                     #n_jobs=-1):\n",
    "    return X, y\n",
    "\n",
    "def test_get_xr_batch_y_col():\n",
    "    interpolate = 1 \n",
    "    data_root = '../TestData/'\n",
    "    pml = PrepML(data_root, interpolate=interpolate, date_start=test_data_start_date, date_end=test_data_end_date, date_train_test_cutoff=test_data_train_cutoff)\n",
    "    pml.regions = test_data_regions\n",
    "    train, test = pml.prep_labels()\n",
    "    lookback = 7\n",
    "    col = 'WindSlab_Likelihood'\n",
    "    ws_label_values = ['no-data', '0-unlikely', '1-possible', '2-likely', '3-very likely']\n",
    "    ws_oversample = {'no-data':False,  '0-unlikely':True, '1-possible':False, '2-likely':False, '3-very likely':True}\n",
    "    X, y, l  = pml.get_xr_batch(train,                     \n",
    "                                lookback_days=lookback, \n",
    "                                batch_size=18,\n",
    "                                y_column=col,\n",
    "                                label_values=ws_label_values,\n",
    "                                oversample=ws_oversample)\n",
    "    \n",
    "    #validate shape\n",
    "    assert X.vars.shape == (20, 981, 57), \"Expected X.vars.shape of (20, 981, 57), got \" + str(X.vars.shape) + \" instead\"\n",
    "    assert y.shape[0] == 20\n",
    "    \n",
    "    #check the data returned\n",
    "    for s in range(0, 20, 3):\n",
    "        \n",
    "        s_df = X.isel(sample=s)\n",
    "        \n",
    "        sample = s_df.sample.values.item().split(' ', 1) #only split first to keep regions with space togeter\n",
    "        test_region = sample[1]\n",
    "        test_lat = s_df.latitude.values.item()\n",
    "        test_lon = s_df.longitude.values.item()\n",
    "        test_lookback = lookback\n",
    "        test_date = np.datetime64(pd.to_datetime(sample[0], format='%Y%m%d'))\n",
    "        \n",
    "        data = pml.get_data_zarr(test_region, lat=test_lat, lon=test_lon, lookback_days=test_lookback, date=test_date)\n",
    "        \n",
    "        validate_zarr(data, test_lat, test_lon, test_lookback)\n",
    "\n",
    "        y2 = y[s:s+1]\n",
    "        \n",
    "        data2 = pml.get_data_zarr_batch(y2.iloc[0]['UnifiedRegion'], season='15-16', df=y2, lookback_days=lookback)\n",
    "\n",
    "        assert is_equal(data.vars.values, s_df.vars.values)\n",
    "        assert is_equal(data.vars.values, data2[0].vars.values[0]) \n",
    "        assert y[col].value_counts()[0] == 5, 'Expected 5 got ' + str(y[col].value_counts()[0]) \n",
    "        assert y[col].value_counts()[1] == 5, 'Expected 5 got ' + str(y[col].value_counts()[1]) \n",
    "        assert y[col].value_counts()[2] == 5, 'Expected 5 got ' + str(y[col].value_counts()[2]) \n",
    "    return X, y\n",
    "        \n",
    "def test_get_xr_batch_filter():\n",
    "    interpolate = 1 \n",
    "    data_root = '../TestData/'\n",
    "    pml = PrepML(data_root, interpolate=interpolate, date_start=test_data_start_date, date_end=test_data_end_date, date_train_test_cutoff=test_data_train_cutoff)\n",
    "    pml.regions = test_data_regions\n",
    "    train, test = pml.prep_labels()\n",
    "    lookback = 7\n",
    "    v =  ['ABSV_150mb_max', 'ABSV_1000mb_avg', 'WILT_surface_min']\n",
    "    X, y, l  = pml.get_xr_batch(train,                     \n",
    "                                lookback_days=lookback, \n",
    "                                batch_size=18,\n",
    "                                variables=v)\n",
    "\n",
    "    #validate shape\n",
    "    assert X.vars.shape == (20, 3, 57)\n",
    "    assert y.shape[0] == 20\n",
    "    \n",
    "    #check the data returned\n",
    "    for s in range(0, 20, 3):\n",
    "        \n",
    "        s_df = X.isel(sample=s)\n",
    "        \n",
    "        sample = s_df.sample.values.item().split(' ', 1) #only split first to keep regions with space togeter\n",
    "        test_region = sample[1]\n",
    "        test_lat = s_df.latitude.values.item()\n",
    "        test_lon = s_df.longitude.values.item()\n",
    "        test_lookback = lookback\n",
    "        test_date = np.datetime64(pd.to_datetime(sample[0], format='%Y%m%d'))\n",
    "        \n",
    "        data = pml.get_data_zarr(test_region, \n",
    "                                 lat=test_lat, \n",
    "                                 lon=test_lon, \n",
    "                                 lookback_days=test_lookback, \n",
    "                                 date=test_date,\n",
    "                                 variables=v)\n",
    "                                        \n",
    "\n",
    "        y2 = y[s:s+1]\n",
    "        \n",
    "        data2 = pml.get_data_zarr_batch(y2.iloc[0]['UnifiedRegion'], \n",
    "                                        season='15-16', \n",
    "                                        df=y2, \n",
    "                                        lookback_days=lookback,\n",
    "                                        variables=v)\n",
    "\n",
    "        assert is_equal(data.vars.values, s_df.vars.values)\n",
    "        assert is_equal(data.vars.values, data2[0].vars.values[0]) \n",
    "      \n",
    "    return X, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grand Mesa Zone\n",
      "Sangre de Cristo Range\n",
      "***variables is none\n",
      "***variables is none\n",
      "***variables is none\n",
      "***variables is none\n",
      "***variables is none\n",
      "***variables is none\n",
      "***variables is none\n",
      "***variables is none\n",
      "***variables is none\n"
     ]
    }
   ],
   "source": [
    "X, y = test_get_xr_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grand Mesa Zone\n",
      "Sangre de Cristo Range\n",
      "***variables is none\n",
      "***variables is none\n",
      "***variables is none\n",
      "***variables is none\n",
      "***variables is none\n",
      "***variables is none\n",
      "***variables is none\n",
      "***variables is none\n",
      "***variables is none\n",
      "***variables is none\n",
      "***variables is none\n",
      "***variables is none\n"
     ]
    }
   ],
   "source": [
    "X, y = test_get_xr_batch_y_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grand Mesa Zone\n",
      "Sangre de Cristo Range\n"
     ]
    }
   ],
   "source": [
    "X, y = test_get_xr_batch_filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generate_train_test_local():\n",
    "    interpolate = 1 \n",
    "    data_root = '../TestData/'\n",
    "    pml = PrepML(data_root, interpolate=interpolate, date_start=test_data_start_date, date_end=test_data_end_date, date_train_test_cutoff=test_data_train_cutoff)\n",
    "    pml.regions = test_data_regions\n",
    "    train, test = pml.prep_labels()\n",
    "    lookback = 7\n",
    "    ret_values = pml.generate_train_test_local(train, test, num_train_files=1, num_test_files=1, batch_size=25, num_train_rows_per_file=100, num_test_rows_per_file=25, n_jobs=8, \n",
    "                                                  file_label='unit_test', lookback_days=lookback, filter_vars=False, compressor='lz4', clevel=9)\n",
    "\n",
    "    \n",
    "    #data = np.load(pml.ml_path + '/Xtrain_batch_0__on_disk.npy', mmap_mode='r')\n",
    "    #assert data.shape == (100, 981, 57)\n",
    "    #assert np.isnan(data).all() == False\n",
    "    #TODO rewrite validation for zarr file type\n",
    "    #TODO investigate deprectating the memmapped files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grand Mesa Zone\n",
      "Sangre de Cristo Range\n",
      "Train: on file 1 of 1\n",
      "Test: on file 1 of 1\n"
     ]
    }
   ],
   "source": [
    "d = test_generate_train_test_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grand Mesa Zone\n",
      "Sangre de Cristo Range\n",
      "Steamboat Zone\n",
      "Front Range Zone\n",
      "Vail Summit Zone\n",
      "Sawatch Zone\n",
      "Aspen Zone\n",
      "North San Juan Mountains\n",
      "South San Juan Mountains\n",
      "Gunnison Zone\n",
      "CPU times: total: 14 s\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#test_ignore\n",
    "\n",
    "interpolate = 1 \n",
    "data_root = 'D:/OAPMLData/'\n",
    "output_root = 'D:/OAPMLData/'\n",
    "nc_root = 'D:/OAPMLData/'\n",
    "pml = PrepML(data_root, output_root, nc_root = nc_root, interpolate=interpolate, resample_length='3H', date_start='2015-11-01', date_end='2016-04-30', date_train_test_cutoff='2016-03-31')\n",
    "\n",
    "pml.regions = {'Colorado': ['Grand Mesa Zone', 'Sangre de Cristo Range', 'Steamboat Zone', 'Front Range Zone',\n",
    "           'Vail Summit Zone', 'Sawatch Zone', 'Aspen Zone', \n",
    "           'North San Juan Mountains', 'South San Juan Mountains', 'Gunnison Zone']}\n",
    "            # 'Canada': [\"Northwest Coastal\", \"Northwest Inland\", \"Sea To Sky\", \n",
    "            # \"South Coast Inland\", \"South Coast\", \"North Rockies\", \n",
    "            # \"Cariboos\", \"North Columbia\", \"South Columbia\", \"Purcells\", \n",
    "            # \"Kootenay Boundary\", \"South Rockies\", \"Lizard Range and Flathead\", \n",
    "            # \"Vancouver Island\", \"Kananaskis Country, Alberta Parks\", \"Chic Chocs, Avalanche Quebec\",\n",
    "            # \"Little Yoho\", \"Banff, Yoho and Kootenay National Parks\", \"Glacier National Park\",\n",
    "            # \"Waterton Lakes National Park\", \"Jasper National Park\"]}\n",
    "\n",
    "\n",
    "train, test = pml.prep_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.iloc[train[['UnifiedRegion', 'parsed_date']].drop_duplicates().index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.iloc[test[['UnifiedRegion', 'parsed_date']].drop_duplicates().index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Day1DangerAboveTreeline\n",
       "Moderate        650\n",
       "Considerable    199\n",
       "no-data         163\n",
       "Low             155\n",
       "High             22\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Day1DangerAboveTreeline'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Day1DangerAboveTreeline\n",
       "Low             77\n",
       "Moderate        66\n",
       "no-data         20\n",
       "Considerable    13\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Day1DangerAboveTreeline'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: on file 1 of 4\n",
      "Train: on file 2 of 4\n",
      "Train: on file 3 of 4\n",
      "Train: on file 4 of 4\n",
      "Test: on file 1 of 1\n",
      "CPU times: total: 20.3 s\n",
      "Wall time: 37.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "label_values=['Low', 'Moderate', 'Considerable', 'High']\n",
    "oversample={'Low':True, 'Moderate':False, 'Considerable':False, 'High':True}\n",
    "filenames, train2, test2 = pml.generate_train_test_local(train, \n",
    "                                              test, \n",
    "                                              num_train_files=4, \n",
    "                                              num_test_files=1, \n",
    "                                              num_train_rows_per_file=100, \n",
    "                                              num_test_rows_per_file=100, \n",
    "                                              batch_size=100,\n",
    "                                              file_label='co_day1above_3h_tutorial', \n",
    "                                              label_values=label_values, \n",
    "                                              oversample=oversample,\n",
    "                                              temp_destination_path= 'D:OAPMLData/Temp/', \n",
    "                                              file_type='zarr',\n",
    "                                              lookback_days = 180,\n",
    "                                              filter_vars = False,  #True doesn't seem to work.\n",
    "                                              n_jobs=24,\n",
    "                                              compressor='lz4',\n",
    "                                              clevel=9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oap_datapipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
