# AUTOGENERATED! DO NOT EDIT! File to edit: ../DataPipelineNotebooks/3.PrepMLData.ipynb.

# %% auto 0
__all__ = ['debug', 'PrepML']

# %% ../DataPipelineNotebooks/3.PrepMLData.ipynb 3
import xarray as xr
import numpy as np
import pandas as pd
from joblib import Parallel, delayed
from functools import partial
import zarr
from datetime import datetime
import os
import numcodecs
from numcodecs import Blosc
import dask
dask.config.set(**{'array.slicing.split_large_chunks': False})
debug = False
numcodecs.blosc.use_threads = False
from .tsai_utilities import *

# %% ../DataPipelineNotebooks/3.PrepMLData.ipynb 5
class PrepML:
    def __init__(self, data_root, output_root = None, nc_root = None, interpolate=1, resample_length='3H', date_start='2015-11-01', date_end='2021-04-30', date_train_test_cutoff='2020-11-01'):
        """
        Initialize the class
        
        Keyword Arguments
        data_root: the root path of the data folders which contains the 4.GFSFiltered1xInterpolationZarr
        output_root: optional path to a seperate output base path, if None then data_root is used as output_root
        interpolate: the amount of interpolation applied in in the previous ParseGFS notebook (used for finding the correct input/output paths)        
        resample_length: the grain the data was resampled to in 1.ParseGFS
        date_start: Earlist date to include in label set (default: '2015-11-01')
        date_end: Latest date to include in label set (default: '2020-04-30')
        date_train_test_cutoff: Date to use as a cutoff between the train and test labels (default: '2019-11-01')
        """
        self.data_root = data_root
        if output_root is None:
            self.output_root = data_root
        else:
            self.output_root = output_root
            
        if nc_root is None:
            self.nc_root = data_root
        else:
            self.nc_root = nc_root
        self.interpolation = interpolate
        self.resample_length = resample_length
        self.date_start = date_start
        self.date_end = date_end
        self.date_train_test_cutoff = date_train_test_cutoff
        self.nc_path = self.nc_root + '/2.GFSFiltered'+ str(self.interpolation) + 'xInterpolation' + resample_length + '/'
        self.processed_path = data_root + '/3.GFSFiltered'+ str(self.interpolation) + 'xInterpolationZarr' + resample_length + '/'
        self.path_to_labels = data_root + 'CleanedForecastsNWAC_CAIC_UAC_CAC.V1.2013-2021.csv'
        self.ml_path = self.output_root + '/4.MLData'
        self.date_col = 'Day1Date'
        self.region_col = 'UnifiedRegion'
        self.parsed_date_col = 'parsed_date'
        if not os.path.exists(self.ml_path):
            os.makedirs(self.ml_path)
            
        #map states to regions for purposes of data lookup
        self.regions = {
            'Utah': ['Abajos', 'Logan', 'Moab', 'Ogden', 'Provo', 
                     'Salt Lake', 'Skyline', 'Uintas'],  
            'Colorado': ['Grand Mesa Zone', 'Sangre de Cristo Range', 'Steamboat Zone', 'Front Range Zone',
                         'Vail Summit Zone', 'Sawatch Zone', 'Aspen Zone', 
                         'North San Juan Mountains', 'South San Juan Mountains', 'Gunnison Zone'],
            'Washington': ['Mt Hood', 'Olympics', 'Snoqualmie Pass', 'Stevens Pass',
                           'WA Cascades East, Central', 'WA Cascades East, North', 'WA Cascades East, South',
                           'WA Cascades West, Central', 'WA Cascades West, Mt Baker', 'WA Cascades West, South'],
            'Canada': ["Northwest Coastal", "Northwest Inland", "Sea To Sky", 
                       "South Coast Inland", "South Coast", "North Rockies", 
                       "Cariboos", "North Columbia", "South Columbia", "Purcells", 
                       "Kootenay Boundary", "South Rockies", "Lizard Range and Flathead", 
                       "Vancouver Island", "Kananaskis Country, Alberta Parks", "Chic Chocs, Avalanche Quebec",
                       "Little Yoho", "Banff, Yoho and Kootenay National Parks", "Glacier National Park",
                       "Waterton Lakes National Park", "Jasper National Park"]
        }
    
        
    @staticmethod
    def lookup_forecast_region(label_region):
        """
        mapping between region names as the labels and the forecasts have slightly different standards
        TODO: could add a unified mapping upstream in parseGFS files or in the label generation

        Keyword Arguments:
        label_region: region as defined in the labels file

        returns the region as defined in the features
        """
        if label_region == 'Mt Hood':
            return 'Mt Hood'
        elif label_region == 'Olympics':
            return 'Olympics'
        elif label_region == 'Cascade Pass - Snoq. Pass':
            return 'Snoqualmie Pass'
        elif label_region == 'Cascade Pass - Stevens Pass':
            return 'Stevens Pass'
        elif label_region == 'Cascade East - Central':
            return 'WA Cascades East, Central'
        elif label_region == 'Cascade East - North':
            return 'WA Cascades East, North'
        elif label_region == 'Cascade East - South':
            return 'WA Cascades East, South'
        elif label_region == 'Cascade West - Central':
            return 'WA Cascades West, Central'
        elif label_region == 'Cascade West - North':
            return 'WA Cascades West, Mt Baker'
        elif label_region == 'Cascade West - South':
            return 'WA Cascades West, South'
        elif label_region == 'Abajo':
            return 'Abajos'
        elif label_region == 'Logan':
            return 'Logan'
        elif label_region == 'Moab':
            return 'Moab'
        elif label_region == 'Ogden':
            return 'Ogden'
        elif label_region == 'Provo':
            return 'Provo'
        elif label_region == 'Salt Lake':
            return 'Salt Lake'
        elif label_region == 'Skyline':
            return 'Skyline'
        elif label_region == 'Uintas':
            return 'Uintas'
        elif label_region == 'Grand Mesa':
            return 'Grand Mesa Zone'
        elif label_region == 'Sangre de Cristo' or label_region == 'Sangre De Cristo':
            return 'Sangre de Cristo Range'
        elif label_region == 'Steamboat & Flat Tops':
            return 'Steamboat Zone'
        elif label_region == 'Front Range':
            return 'Front Range Zone'
        elif label_region == 'Vail & Summit County':
            return 'Vail Summit Zone'
        elif label_region == 'Sawatch Range':
            return 'Sawatch Zone'
        elif label_region == 'Aspen':
            return 'Aspen Zone'
        elif label_region == 'Northern San Juan':
            return 'North San Juan Mountains'
        elif label_region == 'Southern San Juan':
            return 'South San Juan Mountains'
        elif label_region == 'Gunnison':
            return 'Gunnison Zone'
        elif label_region == 'kananaskis':
            return 'Kananaskis Country, Alberta Parks'
        elif label_region == 'Banff  Yoho and Kootenay':
            return 'Banff, Yoho and Kootenay National Parks'
        elif label_region == 'Jasper':
            return 'Jasper National Park'
        elif label_region == 'Glacier':
            return 'Glacier National Park'
        elif label_region == 'Waterton Lakes':
            return 'Waterton Lakes National Park'
        elif label_region == 'Little Yoho':
            return 'Little Yoho'
        elif label_region == 'northwest-coastal':
            return 'Northwest Coastal'
        elif label_region == 'northwest-inland':
            return 'Northwest Inland'
        elif label_region == 'sea-to-sky':
            return 'Sea To Sky'
        elif label_region == 'south-coast-inland':
            return 'South Coast Inland'
        elif label_region == 'south-coast':
            return 'South Coast'
        elif label_region == 'north-rockies':
            return 'North Rockies'
        elif label_region == 'north-columbia':
            return 'North Columbia'
        elif label_region == 'south-columbia':
            return 'South Columbia'
        elif label_region == 'purcells':
            return 'Purcells'
        elif label_region == 'kootenay-boundary':
            return 'Kootenay Boundary'
        elif label_region == 'south-rockies':
            return 'South Rockies'
        elif label_region == 'lizard-range':
            return 'Lizard Range and Flathead'
        elif label_region == 'yukon':
            return 'Yukon'
        elif label_region == 'cariboos':
            return 'Cariboos'        
        else:
            #print('Got region ' + label_region + ' but its not being looked up.')
            return label_region
    

    @staticmethod
    def date_to_season(d):
        """
        mapping of date to season
        
        Keyword Arguments
        d: datetime64
        
        returns season indicator
        """
        #if type of pandas timestamp convert to datetime64
        if isinstance(d, pd.Timestamp):
            d = d.to_datetime64()

        if d >= np.datetime64('2014-11-01') and d <= np.datetime64('2015-04-30'):
            return (np.datetime64('2014-11-01'), '14-15')
        elif d >= np.datetime64('2015-11-01') and d <= np.datetime64('2016-04-30'):
            return (np.datetime64('2015-11-01'), '15-16')
        elif d >= np.datetime64('2016-11-01') and d <= np.datetime64('2017-04-30'):
            return (np.datetime64('2016-11-01'), '16-17')
        elif d >= np.datetime64('2017-11-01') and d <= np.datetime64('2018-04-30'):
            return (np.datetime64('2017-11-01'), '17-18')        
        elif d >= np.datetime64('2018-11-01') and d <= np.datetime64('2019-04-30'):
            return (np.datetime64('2018-11-01'), '18-19')        
        elif d >= np.datetime64('2019-11-01') and d <= np.datetime64('2020-04-30'):            
            return (np.datetime64('2019-11-01'), '19-20')
        elif d >= np.datetime64('2020-11-01') and d <= np.datetime64('2021-04-30'):            
            return (np.datetime64('2020-11-01'), '20-21')
        else:
            #print('Unknown season ' + str(d))
            return (None,'Unknown')
    
   
    def get_state_for_region(self, region):
        """
        Returns the state for a given region
        
        Keywork Arguments
        region: region we want to lookup the state for
        """
        for k in self.regions.keys():
            if region in self.regions[k]:
                return k

        raise Exception('No region with name ' + region)

    def remove_overlap(self, df, label_column='Day1DangerAboveTreeline', lookback_days=14, overlap_percent=1.0): 
        """
        Removes overlap_percent of labels from the lat/lon/date/danger tuple based on the lookback_days 
        """
        #give df an index so we can use it to remove rows
        df.reset_index(inplace=True)
        #for each lat/lon/season tuple select only overlap_percent of those dates
        #this is to minimized the data leakage where timeseries overlap
        #find a list of lat/lon/season from df
        lat_lon_season = df[['latitude', 'longitude', 'season', label_column]].drop_duplicates().copy()
        #for each lat/lon/season tuple select only overlap_percent of those dates
        for index, row in lat_lon_season.iterrows():
            #find the rows in df corresponding to this lat/lon/season/label_column
            overlap_df = df[(df['season']==row['season']) & (df['latitude']==row['latitude']) & (df['longitude']==row['longitude']) & (df[label_column]==row[label_column])]
            #find the number of rows to save 
            num_to_drop = int(len(overlap_df) * (1-overlap_percent))
            #remove the rows
            df.drop(overlap_df.sample(num_to_drop, random_state=42).index, inplace=True)
        return df
        
    def prep_labels(self, overwrite_cache=True, lookback_days=14, overlap_percent=1.0):
        """
        Preps the data and lable sets in to two sets, train & test
        
        Keyword Arguments
        overwrite_cache: True indicates we want to recalculate the lat/lon combos, False indicates use the values if they exist in the cache file (otherwise calcualte and cache it)
        
        returns the train & test sets
        """
        
        #find the season
        nc_date = np.datetime64(self.date_start)
        nc_season = PrepML.date_to_season(nc_date)[1]
        
        #maintaining this as a dict since the arrays are ragged and its more efficient this way
        #storing one sample for each region to get the lat/lon layout
        region_zones = []
        region_data = {}
        for region in self.regions.keys():
            for r in self.regions[region]:               
                region_zones.append(r)
                region_data[r] = xr.open_dataset(self.nc_path + nc_season + '/Region_' + r + '_' + pd.to_datetime(nc_date).strftime('%Y%m%d') + '.nc')
        
        #Read in all the label data
        self.labels = pd.read_csv(self.path_to_labels, low_memory=False,
                dtype={'Day1Danger_OctagonAboveTreelineEast': 'object',
                       'Day1Danger_OctagonAboveTreelineNorth': 'object',
                       'Day1Danger_OctagonAboveTreelineNorthEast': 'object',
                       'Day1Danger_OctagonAboveTreelineNorthWest': 'object',
                       'Day1Danger_OctagonAboveTreelineSouth': 'object',
                       'Day1Danger_OctagonAboveTreelineSouthEast': 'object',
                       'Day1Danger_OctagonAboveTreelineSouthWest': 'object',
                       'Day1Danger_OctagonAboveTreelineWest': 'object',
                       'Day1Danger_OctagonBelowTreelineEast': 'object',
                       'Day1Danger_OctagonBelowTreelineNorth': 'object',
                       'Day1Danger_OctagonBelowTreelineNorthEast': 'object',
                       'Day1Danger_OctagonBelowTreelineNorthWest': 'object',
                       'Day1Danger_OctagonBelowTreelineSouth': 'object',
                       'Day1Danger_OctagonBelowTreelineSouthEast': 'object',
                       'Day1Danger_OctagonBelowTreelineSouthWest': 'object',
                       'Day1Danger_OctagonBelowTreelineWest': 'object',
                       'Day1Danger_OctagonNearTreelineEast': 'object',
                       'Day1Danger_OctagonNearTreelineNorth': 'object',
                       'Day1Danger_OctagonNearTreelineNorthEast': 'object',
                       'Day1Danger_OctagonNearTreelineNorthWest': 'object',
                       'Day1Danger_OctagonNearTreelineSouth': 'object',
                       'Day1Danger_OctagonNearTreelineSouthEast': 'object',
                       'Day1Danger_OctagonNearTreelineSouthWest': 'object',
                       'Day1Danger_OctagonNearTreelineWest': 'object',
                       'SpecialStatement': 'object',
                       'image_paths': 'object',
                       'image_types': 'object',
                       'image_urls': 'object'})
     
        self.labels['parsed_date'] = pd.to_datetime(self.labels[self.date_col], format='%Y%m%d')
        
        metadata_cols = [self.date_col, self.region_col]
        #ensure we are only using label data for regions we are looking at
        #return region_zones
        self.labels[self.region_col] = self.labels.apply(lambda x : PrepML.lookup_forecast_region(x[self.region_col]), axis=1)        
        
        self.labels = self.labels[self.labels[self.region_col].isin(region_zones)]                        
        self.labels = self.labels[self.labels[self.region_col]!='Unknown region']
        
        #add a season column
        tmp = pd.DataFrame.from_records(self.labels[self.parsed_date_col].apply(PrepML.date_to_season).reset_index(drop=True))
        self.labels.reset_index(drop=True, inplace=True)
        self.labels['season'] = tmp[1]
        #some region/seasons have excessive errors in the data, remove those
        self.labels = self.labels[self.labels['season'].isin(['15-16', '16-17', '17-18', '18-19', '19-20', '20-21'])]
        self.labels = self.labels[~self.labels.index.isin(self.labels[(self.labels['season']=='15-16') & (self.labels[self.region_col]=='Steamboat Zone')].index)]
        self.labels = self.labels[~self.labels.index.isin(self.labels[(self.labels['season']=='16-17') & (self.labels[self.region_col]=='Front Range Zone')].index)]               
        lat_lon_union = pd.DataFrame()
        lat_lon_path = self.processed_path + 'lat_lon_union.csv'
        if overwrite_cache or not os.path.exists(lat_lon_path):   
            #find union of all lat/lon/region to just grids with values
            #the process to filter the lat/lon is expensive but we need to do it here (1-5 seconds per region)
            #as the helps the batch process select relevant data
            for r in region_data.keys():
                print(r)
                region_df = region_data[r].stack(lat_lon = ('latitude', 'longitude')).lat_lon.to_dataframe()            
                tmp_df = pd.DataFrame.from_records(region_df['lat_lon'].values, columns=['latitude', 'longitude'])
                indexes_to_drop = []
                for index, row in tmp_df.iterrows():
                    #TODO: there might be a more efficient way than doing this one by one?
                    if 0 == np.count_nonzero(region_data[r].to_array().sel(latitude=row['latitude'], longitude=row['longitude']).stack(time_var = ('time', 'variable')).dropna(dim='time_var', how='all').values):
                        indexes_to_drop.append(index)
                tmp_df.drop(indexes_to_drop, axis=0, inplace=True)
                tmp_df[self.region_col] = r
                lat_lon_union = pd.concat([lat_lon_union, tmp_df])        
        
                #cache the data
                lat_lon_union.to_csv(lat_lon_path)
        else:
            #load the cached data
            lat_lon_union = pd.read_csv(lat_lon_path,float_precision='round_trip')
        #join in with the labels so we have a label per lat/lon pair
        lat_lon_union = lat_lon_union.drop(columns=['Unnamed: 0'], inplace=False).set_index(self.region_col, drop=False).join(self.labels.drop(columns=['Unnamed: 0'], inplace=False).set_index(self.region_col, drop=False), how='left', lsuffix='left', rsuffix='right')
        lat_lon_union.reset_index(inplace=True, drop=False)

        if overlap_percent < 1.0:
            lat_lon_union = self.remove_overlap(lat_lon_union, lookback_days=lookback_days, overlap_percent=overlap_percent)

        #define the split between train and test
        date_min = np.datetime64(self.date_start)
        date_max = np.datetime64(self.date_end)
        train_date_cutoff = np.datetime64(self.date_train_test_cutoff)

        #split the train/test data
        labels_data_union = lat_lon_union[lat_lon_union[self.parsed_date_col] >= date_min]
        labels_data_union = labels_data_union[labels_data_union[self.parsed_date_col] <= date_max]
        #copy so we can delete the overall data and only keep the filtered
        labels_data_train = labels_data_union[labels_data_union[self.parsed_date_col] <= train_date_cutoff].copy()
        labels_data_test = labels_data_union[labels_data_union[self.parsed_date_col] > train_date_cutoff].copy()
        labels_data_train.reset_index(inplace=True, drop=True)
        labels_data_test.reset_index(inplace=True, drop=True)

        #verify we aren't missing anything    
        tmp = [self.regions[x] for x in self.regions.keys()]
        #transform temp in to a flat list from a list of lists
        regions = [item for sublist in tmp for item in sublist]
        #check that all regions are in the train set (2 are usually missing so only assert is more are missing)
        assert len(set(labels_data_train['UnifiedRegion'].unique()) ^ set(regions)) < 3, 'Expected all regions to be in train set, got ' + str(set(labels_data_train['UnifiedRegion'].unique()) ^ set(regions))
        #check that all regions are in the test set
        assert len(set(labels_data_test['UnifiedRegion'].unique()) ^ set(regions)) < 3, 'Expected all regions to be in test set, got ' + str(set(labels_data_test['UnifiedRegion'].unique()) ^ set(regions))

        return labels_data_train, labels_data_test
    
    
    def augment_labels_with_trends(self, label_to_add_trend_info='Day1DangerAboveTreelineValue'):
        raise NotImplementedError('Method is not fully implemented or tested')
        #add extra labels which also allow us to have labels which indicate the trend in the avy direction
        #the thought here is that predicting a rise or flat danger is usually easier than predicting when 
        #to lower the danger so seperating these in to seperate clases
        #TODO: this should be dynamic based on label passed in, not hard coded to above treeline
        labels_trends = pd.DataFrame()
        for r in self.labels[self.region_col].unique():
            for s in self.labels['season'].unique():
                region_season_df = self.labels[self.labels['season']==s]
                region_season_df = region_season_df[region_season_df[self.region_col]==r]
                if(len(region_season_df) == 0):
                    continue
                region_season_df.sort_values(by='parsed_date', inplace=True)
                region_season_df.reset_index(inplace=True, drop=True)
                region_season_df[label_to_add_trend_info] = region_season_df['Day1DangerAboveTreeline'].map({'Low':0, 'Moderate':1, 'Considerable':2, 'High':3})
                region_season_df.loc[0,'Day1DangerAboveTreelineWithTrend'] = region_season_df.iloc[0]['Day1DangerAboveTreeline'] + '_Initial'

                for i in range(1,len(region_season_df)):
                    prev = region_season_df.iloc[i-1]['Day1DangerAboveTreelineValue']
                    cur = region_season_df.loc[i,'Day1DangerAboveTreelineValue']
                    trend = '_Unknown'
                    if prev == cur:
                        trend = '_Flat'
                    elif prev < cur:
                        trend = '_Rising'
                    elif prev >  cur:
                        trend = '_Falling'

                    region_season_df.loc[i,'Day1DangerAboveTreelineWithTrend'] = region_season_df.iloc[i]['Day1DangerAboveTreeline'] + trend
                labels_trends = pd.concat([labels_trends,region_season_df])
        assert(len(labels_trends)==len(self.labels))
        self.labels = labels_trends

    def get_data_zarr(self, region, lat, lon, lookback_days, date, variables=None):
        """
        utility to get data for a specific point
        
        Keyword Arguments
        region: the region the point exists in
        lat: the latitude of the point to lookup
        lon: the longitude of the point to lookup
        lookback_days: the number of days prior to the date to also return
        date: the date which marks the end of the dataset (same date as the desired label)
        variables: filter to just these variables (default: None indicates return all variables)
        """
        #print(region + ' ' + str(lat) + ', ' + str(lon) + ' ' + str(date))
        state = self.get_state_for_region(region)
        earliest_data, season = PrepML.date_to_season(date)

        path = self.processed_path + '/' + season + '/' + state + '/Region_' + region + '.zarr'
        #print('Opening file ' + path)

        tmp_ds = xr.open_zarr(path, consolidated=True)
        
        #filter to just the variables we want
        #TODO: this may be more efficient if we use the open_zarr drop to not even read the variables
        if variables is not None:
            tmp_ds = tmp_ds.sel(variable=tmp_ds.variable.isin(variables))
        
        start_day = date - np.timedelta64(lookback_days-1, 'D')
        #print('start day ' + str(start_day))
        tmp_ds = tmp_ds.sel(latitude=lat, longitude=lon, method='nearest').sel(time=slice(start_day, date + np.timedelta64(24, 'h')))        
        

        date_values_pd = pd.date_range(start=start_day, end=date + np.timedelta64(24, 'h'), freq=self.resample_length)
        #date_values_pd = pd.date_range(start_day, periods=lookback_days, freq='D')
        #reindex should fill missing values with NA
        tmp_ds = tmp_ds.reindex({'time': date_values_pd})

        tmp_ds = tmp_ds.reset_index(dims_or_levels='time', drop=True).load()
        return tmp_ds
    
    def get_data_zarr_batch(self, region, season, df, lookback_days, variables=None):
        """
        utility to get data for a set of points
        
        Keyword Arguments
        region: the region the point exists in
        season: the season the data is in
        df: DataFrame of label rows to pull the data for (should all be from the same region and season)
        lookback_days: the number of days prior to the date to also return        
        variables: filter to just these variables (default: None indicates return all variables)
        """
        
        state = self.get_state_for_region(region)
        
        path = self.processed_path + '/' + season + '/' + state + '/Region_' + region + '.zarr'
        
        #finds the minimal set of values for the single zarr collection and then appends
        #the individaul data to results
        results = []
       
        lats = df['latitude'].unique()
        lons = df['longitude'].unique()
       
        tmp_ds = xr.open_zarr(path, consolidated=True)        
        min_ds = tmp_ds.sel(latitude=lats, longitude=lons)
      
        #filter to just the variables we want
        #TODO: this may be more efficient if we use the open_zarr drop to not even read the variables
        if variables is not None:
            #print('***in var filter with var length ' + str(len(variables)))
            min_ds = min_ds.sel(variable=min_ds.variable.isin(variables))
        else:
            print('***variables is none')

        loaded_df = min_ds.sel(latitude=df['latitude'].unique(), longitude=df['longitude'].unique()).load()    
        for d in df.iterrows():
            d = d[1]
            date = d['parsed_date']
            start_day = date - np.timedelta64(lookback_days-1, 'D')
            
            result_df = loaded_df.sel(latitude=d['latitude'], longitude=d['longitude']).sel(time=slice(start_day, date + np.timedelta64(24, 'h')))
            date_values_pd = pd.date_range(start=start_day, end=date + np.timedelta64(24, 'h'), freq=self.resample_length)
            #reindex should fill missing values with NA
            result_df = result_df.reindex({'time': date_values_pd})
            result_df = result_df.assign_coords({'sample': date.strftime('%Y%m%d') + ' ' + region}).expand_dims('sample')
            results.append(result_df.reset_index(dims_or_levels='time', drop=True))        
     
        return results
    


    def process_sample2(self, iter_tuple, df, lookback_days, variables):
        region = iter_tuple[1]['UnifiedRegion']
        season = iter_tuple[1]['season']
        df_r = df[df['UnifiedRegion']==region]               
        df_r_s = df_r[df_r['season']==season]
        #print('in process sample2')
        return self.get_data_zarr_batch(region=region, 
                                        season=season, 
                                        df=df_r_s, 
                                        lookback_days=lookback_days,
                                        variables=variables)
    
    def get_xr_batch(self, 
                     labels, 
                     lookback_days=14, 
                     batch_size=64, 
                     y_column='Day1DangerAboveTreeline', 
                     label_values=['Low', 'Moderate', 'Considerable', 'High'], 
                     oversample={'Low':True, 'Moderate':False, 'Considerable':False, 'High':True}, 
                     random_state=None,
                     variables = None,
                     n_jobs=1):
        """
        Primary method to take a set of labels and pull the data for it
        the data is large so generally this needs to be done it batches
        and then stored on disk
        For a set of labels and a target column from the labels set create the ML data

        Keyword Arguments
        labels: the set of labels we will randomly choose from
        lookback_days: the number of days prior to the date in the label to also return which defines the timeseries (default: 14)
        batch_size: the size of the data batch to return (default: 64)
        y_column: the column in the label set to use as the label (default: Day1DangerAboveTreeline)
        label_values: possible values for the y label (default: ['Low', 'Moderate', 'Considerable', 'High'])
        oversample: dictionary defining which labels from the label_values set to apply naive oversampling to (default: {'Low':True, 'Moderate':False, 'Considerable':False, 'High':True})
        random_state: define a state to force datasets to be returned in a reproducable fashion (default: None)
                      if using oversample this needs to be None or you will get the same set of values every time
        varaibles: variables to include (default: None which indicates include all variables)
        n_jobs: number of processes to use (default: -1)
        """
        if debug:
            print('Getting a batch for label ' + y_column)
        labels_data = labels

        X = None     
        y = None 

        first = True   
        first_y = True
        num_in_place = 0
        error_files = []
        while num_in_place < batch_size:
            if not first:
                #if we didn't meet the full batch size 
                #continue appending until its full
                #if num_in_place % 5 == 0:
                if(debug):
                    print('Filling remaining have ' + str(num_in_place))
                sample_size = batch_size-num_in_place
                if sample_size < len(label_values):
                    sample_size = len(label_values)
            else: 
                sample_size = batch_size

            batch_lookups = []
            size = int(sample_size/len(label_values))
            #copy this so we can modify label_values during the loop without affecting the iteration
            label_iter = label_values.copy()
            if debug:
                print('have label_iter ' + str(label_iter))
            for l in label_iter:
                if debug:
                    print('    on label: ' + l + ' with samplesize: ' + str(size))
                label_len = len(labels_data[labels_data[y_column]==l])
                if debug: print('    len: ' + str(label_len))
                if label_len == 0:
                    #we don't have any more of this label, remove it from the list so we can continue efficiently
                    if debug: print('    No more values for label: ' + l)
                    label_values.remove(l)
                    continue
                    
                label_slice = labels_data[labels_data[y_column]==l]
                
                #ensure the proposed sample is larger than the available values
                pick_size = size
                if len(label_slice) < pick_size:
                    pick_size = len(label_slice)
                    
                if pick_size > 0:
                    batch_lookups.append(label_slice.sample(pick_size, random_state=random_state))

                    if not oversample[l]:
                        labels_data = labels_data.drop(batch_lookups[-1].index, axis=0)



            if len(batch_lookups) == 0:
                #no more data left
                break
                
            batch_lookup = pd.concat(batch_lookups).sample(frac=1, random_state=random_state)
            
            batch_lookup.reset_index(inplace=True, drop=True)
            if debug: print('have n_jobs ' + str(n_jobs))
       
            tuples = batch_lookup[['UnifiedRegion', 'season']].drop_duplicates()
            func = partial(self.process_sample2, df=batch_lookup, lookback_days=lookback_days, variables=variables)        
            data2 = Parallel(n_jobs=n_jobs, backend="loky")(map(delayed(func), tuples.iterrows())) 
            
            data = [item for sublist in data2 for item in sublist]
            
            if first and len(data) > 0:                            
                X = xr.concat(data, dim='sample') 
                y = batch_lookup
                first = False            
            elif not first and len(data) > 0:    
                X_t = xr.concat(data, dim='sample')
                X = xr.concat([X, X_t], dim='sample')
                y = pd.concat([y, batch_lookup], axis=0)

            num_in_place = y.shape[0]
        

        X = X.sortby(['sample', 'latitude', 'longitude'])
        y['sample'] = y['parsed_date'].dt.strftime('%Y%m%d') + ': ' + y['UnifiedRegion']
        y = y.sort_values(['sample', 'latitude', 'longitude']).reset_index(drop=True)

        
        return X, y, labels_data

    @staticmethod
    def prepare_batch_simple(X, y):
        """
        ensure, X and y indexes are aligned

        Keyword Arguments
        X: The X dataframe
        y: the y dataframe
        """

        X = X.sortby(['sample', 'latitude', 'longitude'])

        sample = y.apply(lambda row: '{}: {}'.format(row['parsed_date'], row['UnifiedRegion']), axis=1)
        y['sample'] = sample
        y = y.set_index(['sample', 'latitude', 'longitude'])
        y.sort_index(inplace=True)    
        y.reset_index(drop=False, inplace=True)
        return X, y

 
    def create_zarr(self,                     
                    remaining_labels, 
                    zarr_file,
                    variables,
                    train_or_test = 'train',                          
                    num_rows = 10000, 
                    lookback_days=180, 
                    batch=0, 
                    batch_size=500,
                    y_column='Day1DangerAboveTreeline', 
                    label_values=['Low', 'Moderate', 'Considerable', 'High'], 
                    oversample={'Low':True, 'Moderate':False, 'Considerable':False, 'High':True},  
                    file_label = '',
                    n_jobs=1):
        
        num_variables = len(variables)            
        
        # We are going to create a loop to fill in the file
        start = 0
        y = pd.DataFrame()
        for i in range(0, num_rows, batch_size):
            if debug: print('On ' + str(i) + ' of ' + str(num_rows))
            X_df, y_df, remaining_labels = self.get_xr_batch(remaining_labels, 
                                                       lookback_days=lookback_days, 
                                                       batch_size=batch_size, 
                                                       y_column=y_column, 
                                                       label_values=label_values,
                                                       oversample=oversample,
                                                       variables=variables,
                                                       n_jobs=n_jobs)         
        
            
            X_df, y_df = PrepML.prepare_batch_simple(X_df, y_df)
            
            #need to make sure all the variables are in the same order (there was an issue that they weren't between train and test sets)
            X_df = X_df.sortby('variable')
            
            if batch_size > X_df.vars.values.shape[0]:
                end = start + X_df.vars.values.shape[0]
            else:
                end = start + batch_size
            
            offset = batch * num_rows
            if debug: print('start: ' + str(start) + ' end: ' + str(end), ' offset: ' + str(offset))
            #print(str(X.shape))
            if debug: print(str(X_df.vars.values.shape))
            if debug: print(str(batch_size))
            if debug: print(str(offset) + ' ' + str(offset + end))
            # I now fill a slice of the zarr file            
            zarr_file[offset + start:offset + end] = X_df.vars.values[:batch_size] #sometimes the process will add a few extras, filter them

            y = pd.concat([y, y_df[:batch_size]])
            start = end
            del X_df, y_df
        
        y.to_parquet(self.ml_path + '/y_' + train_or_test + '_batch_' + str(batch) + '_' + file_label + '.parquet')
        return remaining_labels
    
    #TODO: derive lookback_days from the input set
    #TODO: only write out one y file per X file
    def create_memmapped(self, 
                         remaining_labels, 
                         variables,
                         train_or_test = 'train',                          
                         num_rows = 10000, 
                         lookback_days=180, 
                         batch=0, 
                         batch_size=500,
                         y_column='Day1DangerAboveTreeline', 
                         label_values=['Low', 'Moderate', 'Considerable', 'High'], 
                         oversample={'Low':True, 'Moderate':False, 'Considerable':False, 'High':True},
                         file_label='',
                         n_jobs=1):
        """
        Generate a set of batches and store them in a memmapped numpy array
        this is the technique used to prep data for timeseriesai notebook
        Will store a single numpy X file in the ML directory as well as several y parquet files (one per batch size)
        
        Keyword Arguments
        remaining_labels: the set of labels to draw from
        variables: the variables to include, required
        train_or_test: is this a train or test set--used in the file label (default: train)
        num_variables: number of variables in the X set (default: 1131) 
        num_rows: total number of rows to store in the file (deafult: 10000)
        lookback_days: number of days before the label date to include in the timeseries (default: 180)
        batch: batch number to start in (default: 0) used in case you are generating multiple files
        batch_size: number of rows to process at once to accomodate memory limitations (default: 500)
        y_column: the column in the label set to use as the label (default: Day1DangerAboveTreeline)
        label_values: possible values for the y label (default: ['Low', 'Moderate', 'Considerable', 'High'])
        oversample: dictionary defining which labels from the label_values set to apply naive oversampling to (default: {'Low':True, 'Moderate':False, 'Considerable':False, 'High':True})    
        file_label: optional label for files to distinguish different datasets
        n_jobs: number of parallel jobs to run        
        """

        num_variables = len(variables)
        
        # Save a small empty array
        X_temp_fn = self.ml_path + '/temp_X.npy'
        np.save(X_temp_fn, np.empty(1))

        # Create a np.memmap with desired dtypes and shape of the large array you want to save.
        # It's just a placeholder that doesn't contain any data        
        X_fn = self.ml_path + '/X' + train_or_test + '_batch_' + str(batch) + '_' + file_label + '_on_disk.npy'

        X = None
        

        # We are going to create a loop to fill in the np.memmap
        start = 0
        y = pd.DataFrame()
        for i in range(0, num_rows, batch_size):
            print('On ' + str(i) + ' of ' + str(num_rows))
            # You now grab a chunk of your data that fits in memory
            # This could come from a pandas dataframe for example        
            X_df, y_df, remaining_labels = self.get_xr_batch(remaining_labels, 
                                                       lookback_days=lookback_days, 
                                                       batch_size=batch_size, 
                                                       y_column=y_column, 
                                                       label_values=label_values,
                                                       oversample=oversample,
                                                       variables=variables,
                                                       n_jobs=n_jobs)         
        
            
            X_df, y_df = PrepML.prepare_batch_simple(X_df, y_df)
            
            #need to make sure all the variables are in the same order (there was an issue that they weren't between train and test sets)
            X_df = X_df.sortby('variable')
            
            if batch_size > X_df.vars.values.shape[0]:
                end = start + X_df.vars.values.shape[0]
            else:
                end = start + batch_size

            if debug: print('start: ' + str(start) + ' end: ' + str(end))
            #print(str(X.shape))
            if debug: print(str(X_df.vars.values.shape))
            if debug: print(str(batch_size))
            # I now fill a slice of the np.memmap  
            if X is None:
                X = np.memmap(X_temp_fn, dtype='float32', shape=(num_rows, num_variables, X_df.vars.values.shape[2]))
            
            X[start:end] = X_df.vars.values[:batch_size] #sometimes the process will add a few extras, filter them

            
            #y_df[:batch_size].to_parquet(self.ml_path + '/y_' + train_or_test + '_batch_' + str(batch) + '_' + file_label + '_' + str(i/batch_size) + '.parquet')
            y = pd.concat([y, y_df[:batch_size]])
            start = end
            del X_df, y_df

        #I can now remove the temp file I created
        os.remove(X_temp_fn)

        # Once the data is loaded on the np.memmap, I save it as a normal np.array
        np.save(X_fn, X)
        y.to_parquet(self.ml_path + '/y_' + train_or_test + '_batch_' + str(batch) + '_' + file_label + '.parquet')
        return remaining_labels, X_fn


    def concat_memapped(self, to_concat_filenames, file_label='', dim_1_size=1131, dim_2_size=180, temp_destination_path=None):
        """
        concat multiple numpy files on disk in to a single file
        required for timeseriesai notebook as input to that is a single memmapped file containing X train and test data

        Keyword Arguments:
        to_concat_filenames: the files to concat
        dim_1_size: number of variables in the files (default: 1131)
        dim_2_size: number of lookback dates in the files (length of timeseries) (default: 180)
        destination_path: alternate path to put the concat file 
        """
        
        if temp_destination_path is None:
            temp_destination_path = self.ml_path
            
        to_concat = []
        for i in range(len(to_concat_filenames)):
            to_concat.append(np.load(to_concat_filenames[i], mmap_mode='r'))

        dim_0_size = 0

        for i in range(len(to_concat)):
            dim_0_size += to_concat[i].shape[0]
            assert to_concat[i].shape[1] == dim_1_size, 'Expected dim1 shape as ' + str(dim_1_size) + ' but got ' + str(to_concat[i].shape[1])
            assert to_concat[i].shape[2] == dim_2_size, 'Expected dim2 shape as ' + str(dim_2_size) + ' but got ' + str(to_concat[i].shape[2])

        X_temp_fn = temp_destination_path + '/temp_X.npy'
        np.save(X_temp_fn, np.empty(1))
        X_fn = self.ml_path + '/X_all' + '_' + file_label + '.npy'
        X = np.memmap(X_temp_fn, dtype='float32', shape=(dim_0_size, dim_1_size, dim_2_size))
        dim_0_start = 0
        for i in range(len(to_concat)):
            print('On file ' + str(i) + ' of ' + str(len(to_concat)))
            dim_0 = to_concat[i].shape[0]
            X[dim_0_start:dim_0_start+dim_0] = to_concat[i]
            dim_0_start += dim_0


        #I can now remove the temp file I created
        os.remove(X_temp_fn)

        # Once the data is loaded on the np.memmap, I save it as a normal np.array
        np.save(X_fn, X)
        del to_concat
    

    def filter_features(self, feature_list):
        """
        Filter list of variable names to a smaller set based on name heuristics
        """
        l = TSAIUtilities.filter_features(set(feature_list), only_var=False)
        return l

    #TODO: add the ability to restart from a cached label file
    def generate_train_test_local(self,
                                  train_labels, 
                                  test_labels, 
                                  num_train_files=1, 
                                  num_test_files=1, 
                                  num_train_rows_per_file=1000, 
                                  num_test_rows_per_file=500,
                                  batch_size=500,
                                  lookback_days = 180,
                                  y_column='Day1DangerAboveTreeline', 
                                  label_values=['Low', 'Moderate', 'Considerable', 'High'], 
                                  oversample={'Low':True, 'Moderate':False, 'Considerable':False, 'High':True},
                                  file_label = '',
                                  temp_destination_path=None,
                                  file_type = 'zarr',
                                  filter_vars = True,
                                  n_jobs=1,
                                  compressor='lz4',
                                  clevel='6'):
        """        
        create several memapped files
        we do this as the technique to create one has some memory limitations
        also due to the memory limitations sometimes this process runs out of memory and crashes
        which is why we cache the label state after every iteration so we can restart at that state
        15 mins for 10000 rows using all 16 cores on my machine
        I can generate a max of ~50000 rows per batch with 48 gb of ram before running out of memory
        
        Keyword Arguments:
        y_column: the column in the label set to use as the label (default: Day1DangerAboveTreeline)
        label_values: possible values for the y label (default: ['Low', 'Moderate', 'Considerable', 'High'])
        oversample: dictionary defining which labels from the label_values set to apply naive oversampling to (default: {'Low':True, 'Moderate':False, 'Considerable':False, 'High':True})    
        file_label: optional file label to add to the on disk files to distinguish between data sets
        temp_destination_path: alternate path to put the concat file temporarily (improves disk throughput)
        """
        assert num_train_rows_per_file % batch_size == 0, 'num_train_rows_per_file needs to be a multiple of batch_size'
        assert batch_size <= num_train_rows_per_file, 'num_train_rows_per_file needs to be greater than batch_size'
        assert num_test_rows_per_file % batch_size == 0, 'num_test_rows_per_file needs to be a multiple of batch_size'        
        assert batch_size <= num_test_rows_per_file, 'num_test_rows_per_file needs to be greater than batch_size'
        #not all seasons have the same # of variables so find the common subset first
        train_seasons = train_labels['season'].unique()
        test_seasons = test_labels['season'].unique()
        #find the common vars for each season
        #pull one sample of data for each season
        data = {}
        for s in train_seasons:
            label = train_labels[train_labels['season'] == s].sample(n = 1)
            assert(len(label==1))
            data[s] = self.get_data_zarr(label.iloc[0]['UnifiedRegion'], label.iloc[0]['latitude'], label.iloc[0]['longitude'], lookback_days, label.iloc[0]['parsed_date'])

        for s in test_seasons:
            label = test_labels[test_labels['season'] == s].sample(n = 1)
            assert(len(label==1))
            data[s] = self.get_data_zarr(label.iloc[0]['UnifiedRegion'], label.iloc[0]['latitude'], label.iloc[0]['longitude'],  lookback_days, label.iloc[0]['parsed_date'])
        
        v = []
        for d in data.keys():
            v.append(set(data[d].variable.values))
        final_vars = list(set.intersection(*v))
        if debug: print("Len of vars: " + str(len(final_vars))) 
        if filter_vars:
            final_vars = self.filter_features(final_vars)
        
        #get a sample so we can dump the feature labels
        X, _, _ = self.get_xr_batch(train_labels, variables=final_vars, lookback_days=lookback_days, batch_size=4, n_jobs=n_jobs)   
        if debug: print("Shape of X.time is: " + str(len(X.time.data)))
        pd.Series(X.variable.data).to_csv(self.ml_path + '/FeatureLabels_' + file_label + '.csv')
        label_values_copy = label_values.copy()
        if file_type == 'zarr':
            file_path = self.ml_path + '/X' + '_' + file_label + '.zarr'
            c = None
            if compressor is not None:
                c = Blosc(cname=compressor, clevel=clevel, shuffle=Blosc.BITSHUFFLE)            
            zarr_arr = zarr.open(file_path, mode='a', chunks=(1,-1, -1), 
                                 compressor=c,
                                 shape=(num_train_files * num_train_rows_per_file + 
                                        num_test_files * num_test_rows_per_file, 
                                        len(final_vars), len(X.time.data)), 
                                 dtype=np.float32)
            for i in range(0, num_train_files):
                print('Train: on file ' + str(i+1) + ' of ' + str(num_train_files))
                train_labels = self.create_zarr(train_labels, 
                                                zarr_file=zarr_arr,
                                                variables = final_vars,
                                                train_or_test = 'train',                                                            
                                                num_rows=num_train_rows_per_file, 
                                                batch=i,
                                                batch_size=batch_size,
                                                y_column=y_column,
                                                lookback_days=lookback_days,
                                                label_values=label_values,
                                                oversample=oversample,    
                                                file_label=file_label,
                                                n_jobs=n_jobs)
            oversample_test = {}
            for k in oversample.keys():
                oversample_test[k] = False

            #same process for test
            for i in range(0, num_test_files):
                print('Test: on file ' + str(i+1) + ' of ' + str(num_test_files))
                test_labels = self.create_zarr(test_labels, 
                                               zarr_file=zarr_arr,
                                               variables = final_vars,
                                               train_or_test = 'test',                                                           
                                               num_rows=num_test_rows_per_file, 
                                               batch=i + num_train_files,
                                               batch_size=batch_size,
                                               y_column=y_column,
                                               lookback_days=lookback_days,
                                               label_values=label_values_copy,
                                               oversample=oversample_test,
                                               file_label=file_label,
                                               n_jobs=n_jobs)
            return file_path, train_labels, test_labels
                
        else:        
            filenames = []
            #keep a copy so that we can ensure test gets the full set
            
            for i in range(0, num_train_files):
                train_labels, files = self.create_memmapped(train_labels, 
                                                            variables = final_vars,
                                                            train_or_test = 'train',                                                            
                                                            num_rows=num_train_rows_per_file, 
                                                            batch=i,
                                                            batch_size=batch_size,
                                                            y_column=y_column,
                                                            label_values=label_values,
                                                            oversample=oversample,
                                                            file_label=file_label,
                                                            n_jobs=n_jobs)
                filenames.append(files)

            #test files shouldn't use oversampling
            oversample_test = {}
            for k in oversample.keys():
                oversample_test[k] = False

            #same process for test
            for i in range(0, num_test_files):
                test_labels, files = self.create_memmapped(test_labels, 
                                                           variables = final_vars,
                                                           train_or_test = 'test',                                                           
                                                           num_rows=num_test_rows_per_file, 
                                                           batch=i,
                                                           batch_size=batch_size,
                                                           y_column=y_column,
                                                           label_values=label_values_copy,
                                                           oversample=oversample_test,
                                                           file_label=file_label,
                                                           n_jobs=n_jobs)
                filenames.append(files)
        
            return filenames, train_labels, test_labels
    
