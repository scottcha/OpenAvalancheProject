---

title: Open Avalanche Project


keywords: fastai
sidebar: home_sidebar



nb_path: "DataPipelineNotebooks/index.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: DataPipelineNotebooks/index.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Open source project to bring data and ml to avalanche forecasting</p>
<p>Webpage is <a href="https://openavalancheproject.org">https://openavalancheproject.org</a>
Docs are located at <a href="https://scottcha.github.io/OpenAvalancheProject/">https://scottcha.github.io/OpenAvalancheProject/</a></p>
<p>We are starting to try and improve communications on the group here <a href="https://groups.google.com/g/openavalancheproject">https://groups.google.com/g/openavalancheproject</a></p>
<p>Directories are organized as follows:</p>
<ul>
<li><p>Data</p>
<p>Contains files associated with data inputs, such as geojson definitions of avalanche regions.  Training and label data are linked in the README there as they are too large to host in git.</p>
</li>
<li><p>Environments</p>
<p>Conda environment yml files</p>
</li>
<li><p>ML</p>
<p>In the current state this is empty but will host the production pipline files once the notebook effort demonstrates a sufficient architecture and accuracy</p>
</li>
<li><p>WebApp is the bulk of the operational code</p>
<ul>
<li>OpenAvalancheProjectWebApp Contains the code for the website</li>
<li>OpenAvalancheProjectWebApp.Tests Tests for the website (unfortunately empty at the moment)</li>
</ul>
</li>
</ul>
<h2 id="Tutorial">Tutorial<a class="anchor-link" href="#Tutorial"> </a></h2><h3 id="1.-Getting-new-input-data">1. Getting new input data<a class="anchor-link" href="#1.-Getting-new-input-data"> </a></h3><p>This aspect of the tutorial will cover how you can obtain new weather input data for a new date range or region.  This part assumes you have avalanche forecast labels for the dates and region (OAP currently has historical forecast labels for three avalanche centers in the US from the 15-16 season through the 19-20 season and is working on expanding that).</p>
<p>Due to the large size of the input GFS data and the fact that its already hosted by NCAR OAP isn't currently providing copies of this data.  If you want to start a data processing pipeline from the original data you can start with this process here.  If you aren't interested in the data processing steps and only in the ML steps you can download the labels here: <a href="https://oapstorageprod.blob.core.windows.net/oap-training-data/Data/V1.1FeaturesWithLabels2013-2020.zip">https://oapstorageprod.blob.core.windows.net/oap-training-data/Data/V1.1FeaturesWithLabels2013-2020.zip</a> and a subset of training data here: <a href="https://oapstorageprod.blob.core.windows.net/oap-training-data/Data/MLDataWashington.zip">https://oapstorageprod.blob.core.windows.net/oap-training-data/Data/MLDataWashington.zip</a> and skip to the fourth notebook 4.TimeseriesAi</p>
<p>The input data is derived from the .25 degree GFS model hosted by NCAR hosted at this site: <a href="https://rda.ucar.edu/datasets/ds084.1/">https://rda.ucar.edu/datasets/ds084.1/</a>
You'll need to create an account and once you are logged in you can visit the above link and then click on the Data Access tab.  One note is that I've found that chromium based browsers don't work well on this site so I recommend you use Firefox for at least downloading the data.</p>
<p>Due to the size of the files we are downloading I only recommend downloading one season and for a regional subset at a time.  In this example I'm going to download the data for Washington.</p>
<p><img src="/openavalancheproject/images/NCAR_GetData.png" alt="NCAR Get Data"></p>
<p>Click on the "Get a Subset" link.</p>
<p>The next page allows us to select both the dates and parameters we are interested in.  Currently we read all parameters so leave the parameters unchecked.  For dates choose one winter season.  In the below screenshot I've selected dates Nov 1, 2015 thorough April 30, 2016 for the 15-16 season.  The models assume the season starts Nov 1 and ends April 30 (it wouldn't be too difficult to update the data pipeline for a southern hemisphere winter but its not something I've done yet).</p>
<p><img src="/openavalancheproject/images/NCAR_DateSelection.png" alt="NCAR Date Selection"></p>
<p>Click Continue and wait for it to validate your selections.</p>
<p>The next page allows you to further subset your data.  There are a few important things here.</p>

<pre><code>1. Verify that the dates are correct.  
2. We want the output as grib (same as input) 
3. Download all vertical levels.  
4. Select only the 3-24 hour forecasts in the gridded products as currently OAP doesn't use more than this.  
5. You can also then select the bounding box for the area you want to download. Once you have a bounding box you like write down the lat/lon values so its easier to input when we come back for other date ranges.

</code></pre>
<p><img src="/openavalancheproject/images/NCAR_Subset2.png" alt="NCAR Subset Selection"></p>
<p>Once the selections are correct and you can eventually click through to submit your request.  You should get a confirmation page of your selections and the system will start to retrieve your data.  This usually takes a few hours and you will get an email when its ready for download.  At this point if you want additional date/time ranges you can submit the requests and they will get queued and made avalable for download when they are ready.  In this example the downloaded files were 1.1 GB.</p>
<p>Extract and decompress all the files until you have a per forecast grib file and ensure all the files have been moved in to a single directory (per season per location). If you are using Linux this stackoverflow post may help <a href="https://askubuntu.com/questions/146634/shell-script-to-move-all-files-from-subfolders-to-parent-folder">https://askubuntu.com/questions/146634/shell-script-to-move-all-files-from-subfolders-to-parent-folder</a>.</p>
<p>Once you have all the files as grib files in a single directory for that date and location (i.e., 15-16/Washington/) there are a couple final cleaning steps.  Due to the download process sometimes some files earlier than 11/1 are included.  You can just delete those files (the file date is)</p>
<p>_Its worth a brief interlude in to understanding how these files are encoded.  Here is a typical file name gfs.0p25.2015110100.f003.grib2.chamberlin455705.  Lets break that down gfs: is the model we are using.  0p25 I beleive is the resolution at .25 degress but I haven't seen this documented.  2015110100 is the encoded date of the model runtime.  You will see in your dataset that there are four models run per day: 00, 06, 12, 18.  Currently we are only using the 00 model (the first of the day).  The next component is .f003 which is the forecast for 3 hours from the model runtime.  grib2 is the input file format.  chamberlin455705 is the enocded download request.</p>
<p>Next delete all files which have a model run hour other than 00 (i.e., 06, 12, 18).  Check that you have 1456 files at this point (8 files per day for 182 days, my download is missing the last 4 files which isn't a big deal as a subsequent steps averages these hourly forecasts to daily forecasts).  The total size of the input files at this point is ~900MB.</p>
<p>Now remove the download request label in the filename which is easily accomplished using the rename command.</p>

<pre><code>rename 's/(.*)\.grib2.chamberlin455705/$1.grib2/g' *

</code></pre>
<p><img src="/openavalancheproject/images/files_example2.png" alt="File List Example"></p>
<p>The final step is to ensure the input data is in the correct folder structure.  All data for this project will sit off a path you define as the base path.  The GFS input data then needs to be in subfolders of that path delineated by season and state (or country).
For example if our past path in this example is:</p>

<pre><code>/media/scottcha/E1/Data/OAPMLData/

</code></pre>
<p>The place this data in</p>

<pre><code>/media/scottcha/E1/Data/OAPMLData/1.RawWeatherData/15-16/Washington/

</code></pre>
<p>Notes:</p>
<ul>
<li>There is an option to covert the file to NetCDF in the NCAR/UCAR UI.  Don't use this as it will result in a .nc file which isn't in the same format as the one we are going to use.</li>
</ul>
<h3 id="2.-Transform-and-Filtering-the-Data">2. Transform and Filtering the Data<a class="anchor-link" href="#2.-Transform-and-Filtering-the-Data"> </a></h3><p>Now that we have the input file set we can start to go through the initial data pipeline steps to transform and filter the data. Today this is done in a series of Jupyter notebooks.  This format makes it easy to incrementally process and check the outputs while the project is in a development phase (once we have a model which seems to have a resonable output these steps will be encoded in a set of python modules and implemented as a processing pipeline).</p>
<p>Assuming you have Anaconda and Jupyter installed first change directory to the Environments directory at the root level of the repo.  This contains two conda envrionment definitions, one for the processing steps, pangeo_small.yml, and one for the deep learning step, timeseriesai.yml.</p>

<pre><code>conda env create -f pangeo_small.yml

</code></pre>
<p><em>This environment file was adopted from the (Pangeo project)[<a href="https://pangeo.io/">https://pangeo.io/</a>] but has been slimmed down a bit.</em></p>
<p>Once the environment has been created you can activate it with</p>

<pre><code>conda activate pangeo_small

</code></pre>
<p>There is one step we need to take before going through the notebooks and that is converting the grib2 files to NetCDF.  We do this for a couple of reasons but primarily that using this tool efficiently collapses the vertical dimensions (called level) in to the variable definitions so we can more easily get it to the ML format we need.  The utility to do this is called wgrib2 and should have been installed in the pangeo_small environment.</p>
<p>Using a terminal prompt change directory to the folder where you downloaded and unpacked the weather model files.</p>

<pre><code>/media/scottcha/E1/Data/OAPMLData/1.RawWeatherData/15-16/Washington/

</code></pre>
<p>In that directory you can execute this command to iterate through all the files and tranform them:</p>

<pre><code>for i in *.grib2; do wgrib2 $i -netcdf $i.nc; done

</code></pre>
<p>Rexecute a rename command to remove the grib extension:</p>

<pre><code>rename 's/(.*)\.grib2/$1.nc/g' *

</code></pre>
<p><em>There are ways of improving the efficiency by doing this in parallel so feel free to improve on this.</em></p>
<p>To start a new notebook launch jupyter</p>

<pre><code>jupyter notebook

</code></pre>
<h3 id="3.-ParseGFS">3. ParseGFS<a class="anchor-link" href="#3.-ParseGFS"> </a></h3><h4 id="Parsing-and-filtering-the-input-files">Parsing and filtering the input files<a class="anchor-link" href="#Parsing-and-filtering-the-input-files"> </a></h4><p>Completing these next few steps bascially takes the raw input weather data and leaves us with data slightly transformed but filtered to only the coordinates in the avalanche regions for that location.  For example here is what a regional view of one of the parameters (U component of wind vector) looks like when both interpolated 4x and viewed across the entire Washington region:</p>
<p><img src="/openavalancheproject/images/Wind_Example.png" alt="Washington Wind Component"></p>
<p>We've used this geojson definition of the avalanche regions to subset that view in to much smaller views focused on the avalanche forecast regions.  Here are all the US regions.</p>
<p><img src="/openavalancheproject/images/US_Avy_Regions.png" alt="US Avalanche Regions"></p>
<p>And then this is what it looks like when filtered to only the Olympics avalanche region (the small one in the top left of the US regions):</p>
<p><img src="/openavalancheproject/images/Wind_Region_Example.png" alt="Olympics Wind Component"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Building-the-project">Building the project<a class="anchor-link" href="#Building-the-project"> </a></h3><p>To build and install the project first clone locally.
Then you can build the project by executing</p>

<pre><code>nbdev_build_lib

</code></pre>
<p>In the project root.  To install the modules in to your current conda environment you can then execute</p>

<pre><code>pip install -e .</code></pre>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">openavalancheproject.parse_gfs</span> <span class="kn">import</span> <span class="n">ParseGFS</span>
<span class="kn">from</span> <span class="nn">openavalancheproject.convert_to_zarr</span> <span class="kn">import</span> <span class="n">ConvertToZarr</span>
<span class="kn">from</span> <span class="nn">openavalancheproject.prep_ml</span> <span class="kn">import</span> <span class="n">PrepML</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Files-on-disk-structure">Files on disk structure<a class="anchor-link" href="#Files-on-disk-structure"> </a></h1><p>OAPMLData\
    CleanedForecastsNWAC_CAIC_UAC.V1.2013-2020.csv #labels downloaded from <a href="https://oapstorageprod.blob.core.windows.net/oap-training-data/Data/V1.1FeaturesWithLabels2013-2020.zip">https://oapstorageprod.blob.core.windows.net/oap-training-data/Data/V1.1FeaturesWithLabels2013-2020.zip</a></p>

<pre><code>1.RawWeatherData/
    gfs/
        &lt;season&gt;/
            /&lt;state or country&gt;/
2.GFSDaily(x)Interpolation/
3.GFSFiltered(x)Interpolation/
4.GFSFiltered(x)InterpolationZarr/
5.MLData</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="These-parameters-need-to-be-set">These parameters need to be set<a class="anchor-link" href="#These-parameters-need-to-be-set"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">season</span> <span class="o">=</span> <span class="s1">&#39;15-16&#39;</span>
<span class="n">state</span> <span class="o">=</span> <span class="s1">&#39;Washington&#39;</span>

<span class="n">interpolate</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1">#interpolation factor: whether we can to augment the data through lat/lon interpolation; 1 no interpolation, 4 is 4x interpolation</span>

<span class="n">data_root</span> <span class="o">=</span> <span class="s1">&#39;/media/scottcha/E1/Data/OAPMLData/&#39;</span>

<span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1">#number of parallel processes, this processing is IO bound so don&#39;t set this too high</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pgfs</span> <span class="o">=</span> <span class="n">ParseGFS</span><span class="p">(</span><span class="n">season</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">data_root</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>/media/scottcha/E1/Data/OAPMLData//1.RawWeatherData/gfs/15-16/Washington/ Is Input Directory
/media/scottcha/E1/Data/OAPMLData/2.GFSDaily1xInterpolation/15-16/ Is output directory and input to filtering
/media/scottcha/E1/Data/OAPMLData/3.GFSFiltered1xInterpolation/15-16/ Is output directory of filtering
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-first-step-is-to-resample-the-GFS-files">The first step is to resample the GFS files<a class="anchor-link" href="#The-first-step-is-to-resample-the-GFS-files"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#n_jobs=4</span>
<span class="c1">#CPU times: user 1.11 s, sys: 551 ms, total: 1.66 s</span>
<span class="c1">#Wall time: 12min 22s</span>
<span class="o">%</span><span class="k">time</span> results = pgfs.resample_local()
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Then-interpolate-and-filter-those-files">Then interpolate and filter those files<a class="anchor-link" href="#Then-interpolate-and-filter-those-files"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#n_jobs = 6</span>
<span class="c1">#CPU times: user 1.83 s, sys: 830 ms, total: 2.66 s</span>
<span class="c1">#Wall time: 45min 18s</span>
<span class="o">%</span><span class="k">time</span> results = pgfs.interpolate_and_write_local()
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="If-interpolate-and-write-returns-errors-you-can-retry-them-individually-like:">If interpolate and write returns errors you can retry them individually like:<a class="anchor-link" href="#If-interpolate-and-write-returns-errors-you-can-retry-them-individually-like:"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">redo</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;20151103&#39;</span><span class="p">,</span> <span class="s1">&#39;20151105&#39;</span><span class="p">]</span>
<span class="c1">#fix any errors</span>
<span class="n">redo2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">redo</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;on &#39;</span> <span class="o">+</span> <span class="n">r</span><span class="p">)</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">interpolate_and_write</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">redo2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Once-the-converstion-is-complete-for-a-set-of-seasons-and-states-we-need-to-convert-the-batch-to-Zarr">Once the converstion is complete for a set of seasons and states we need to convert the batch to Zarr<a class="anchor-link" href="#Once-the-converstion-is-complete-for-a-set-of-seasons-and-states-we-need-to-convert-the-batch-to-Zarr"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#uncomment regions and seasons if doing a larger transform</span>
<span class="n">regions</span> <span class="o">=</span> <span class="p">{</span>
           <span class="s1">&#39;Washington&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Mt Hood&#39;</span><span class="p">,</span> <span class="s1">&#39;Olympics&#39;</span><span class="p">,</span> <span class="s1">&#39;Snoqualmie Pass&#39;</span><span class="p">,</span> <span class="s1">&#39;Stevens Pass&#39;</span><span class="p">,</span>
           <span class="s1">&#39;WA Cascades East, Central&#39;</span><span class="p">,</span> <span class="s1">&#39;WA Cascades East, North&#39;</span><span class="p">,</span> <span class="s1">&#39;WA Cascades East, South&#39;</span><span class="p">,</span>
           <span class="s1">&#39;WA Cascades West, Central&#39;</span><span class="p">,</span> <span class="s1">&#39;WA Cascades West, Mt Baker&#39;</span><span class="p">,</span> <span class="s1">&#39;WA Cascades West, South&#39;</span><span class="p">]</span>
           <span class="p">}</span>
<span class="n">seasons</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;15-16&#39;</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="4.-ConvertToZarr">4. ConvertToZarr<a class="anchor-link" href="#4.-ConvertToZarr"> </a></h3><h4 id="Reformat-data-in-to-efficient-Zarr-format">Reformat data in to efficient Zarr format<a class="anchor-link" href="#Reformat-data-in-to-efficient-Zarr-format"> </a></h4><p>The next step in our data transformation pipeline is to transform the NetCDF files to Zarr files which are indexed in such a way to make access to specific dates and lat/lon pairs as efficient as possible. This process can be run entirely end to end once you are sure the parameters are set correctly.  It does take about 6 hours on my workstation using all cores.  The imporant item about this notebook is that we are essentially indexing the data to be accessed efficiently when we create our ML datasets.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ctz</span> <span class="o">=</span> <span class="n">ConvertToZarr</span><span class="p">(</span><span class="n">seasons</span><span class="p">,</span> <span class="n">regions</span><span class="p">,</span> <span class="n">data_root</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ctz</span><span class="o">.</span><span class="n">convert_local</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="5.-PrepMLData">5. PrepMLData<a class="anchor-link" href="#5.-PrepMLData"> </a></h3><h4 id="Converting-the-data-in-to-a-memmapped-numpy-timeseries-(samples,-feature,-timestep)">Converting the data in to a memmapped numpy timeseries (samples, feature, timestep)<a class="anchor-link" href="#Converting-the-data-in-to-a-memmapped-numpy-timeseries-(samples,-feature,-timestep)"> </a></h4><p>This step needs to be run once to create a dataset to be used in a subsequent ML step.  The way to think about these methods is that we use the set of valid labels + the valid lat/lon pairs as an index in to the data.  Its important to understand the regions are geographically large and usually cover many lat/lon pairs in our gridded dataset while the labels apply to an entire region (multiple lat/lon pairs).  For example the <em>WA Cascades East, Central</em> region coveres 24 lat/lon pairs so if on Jan 1 there was a label we wanted to predict our dataset would have 24 lat/lon pairs in that region associated with that label.  There are pros and cons for this approach.</p>
<p>Pros:</p>
<ol>
<li>Reasonable data augmentation approach</li>
<li>Aligns with how we utltimatly want to provide predictions--more granular, not restricted to established regions</li>
</ol>
<p>Cons:</p>
<ol>
<li>Could be contributing to overfitting</li>
<li>The data becomes very large</li>
</ol>
<p>That being said the methods will calculate this index for every label/lat/lon point and then we'll split this in to train and test sets.  Its important to ensure that the train test split is done in time (i.e., I usually use 15-16 through 18-19 as the training set and then 19-20 as the test set) as if you don't there will be data leakage.</p>
<p>Once the train test split is done on the labels there is a process to build up the dataset.  This is still a slow process even when doing it in parallel and agains the indexed Zarr data.  I've spent a lot of time trying various ways of optimizing this but I'm sure this could use more work.  The primary method internal method for doign this is called _get_xr<em>batch</em> and takes several parameters:</p>
<ol>
<li>labels: list of the train or test set labels</li>
<li>lookback_days: the number of previous days to include in the dataset.  For example if the label is for Jan 1, then a lookback_days of 14 will also include the previous 14 days.  I've been typically using 180 days as lookback (if a lookback extends prior to Nov 1 then we just fill in NaN as the data is likly irrelevant) but its possible that a lower value might give better results.</li>
<li>batch_size: the size of the batch you want returned</li>
<li>y_column: the label you want to use</li>
<li>label_values: the possible values of the label from y_column.  We include this as the method can implement oversampling to adjust for the imbalanced data.</li>
<li>oversample: dict which indicates which labels should be oversampled.  </li>
<li>random_state: random variable initilizer</li>
<li>n_jobs: number of processes to use</li>
</ol>
<p>In the tutorial the notebook produced one train batche of 1,000 rows and one test batch of 500 rows and then concats them in a single memapped file.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="At-this-point-we-can-generate-a-train-and-test-dataset-from-the-Zarr-data">At this point we can generate a train and test dataset from the Zarr data<a class="anchor-link" href="#At-this-point-we-can-generate-a-train-and-test-dataset-from-the-Zarr-data"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pml</span> <span class="o">=</span> <span class="n">PrepML</span><span class="p">(</span><span class="n">data_root</span><span class="p">,</span> <span class="n">interpolate</span><span class="p">,</span> <span class="n">date_start</span><span class="o">=</span><span class="s1">&#39;2015-11-01&#39;</span><span class="p">,</span> <span class="n">date_end</span><span class="o">=</span><span class="s1">&#39;2016-04-30&#39;</span><span class="p">,</span> <span class="n">date_train_test_cutoff</span><span class="o">=</span><span class="s1">&#39;2016-04-01&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> train_labels, test_labels = pml.prep_labels()
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_labels</span> <span class="o">=</span> <span class="n">train_labels</span><span class="p">[</span><span class="n">train_labels</span><span class="p">[</span><span class="s1">&#39;UnifiedRegion&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">([</span><span class="s1">&#39;Mt Hood&#39;</span><span class="p">,</span> 
                                                              <span class="s1">&#39;Olympics&#39;</span><span class="p">,</span> 
                                                              <span class="s1">&#39;Snoqualmie Pass&#39;</span><span class="p">,</span>
                                                              <span class="s1">&#39;Stevens Pass&#39;</span><span class="p">,</span>
                                                              <span class="s1">&#39;WA Cascades East, Central&#39;</span><span class="p">,</span>
                                                              <span class="s1">&#39;WA Cascades East, North&#39;</span><span class="p">,</span>
                                                              <span class="s1">&#39;WA Cascades East, South&#39;</span><span class="p">,</span>
                                                              <span class="s1">&#39;WA Cascades West, Central&#39;</span><span class="p">,</span>
                                                              <span class="s1">&#39;WA Cascades West, Mt Baker&#39;</span><span class="p">,</span>
                                                              <span class="s1">&#39;WA Cascades West, South&#39;</span><span class="p">])]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test_labels</span> <span class="o">=</span> <span class="n">test_labels</span><span class="p">[</span><span class="n">test_labels</span><span class="p">[</span><span class="s1">&#39;UnifiedRegion&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">([</span><span class="s1">&#39;Mt Hood&#39;</span><span class="p">,</span> 
                                                              <span class="s1">&#39;Olympics&#39;</span><span class="p">,</span> 
                                                              <span class="s1">&#39;Snoqualmie Pass&#39;</span><span class="p">,</span>
                                                              <span class="s1">&#39;Stevens Pass&#39;</span><span class="p">,</span>
                                                              <span class="s1">&#39;WA Cascades East, Central&#39;</span><span class="p">,</span>
                                                              <span class="s1">&#39;WA Cascades East, North&#39;</span><span class="p">,</span>
                                                              <span class="s1">&#39;WA Cascades East, South&#39;</span><span class="p">,</span>
                                                              <span class="s1">&#39;WA Cascades West, Central&#39;</span><span class="p">,</span>
                                                              <span class="s1">&#39;WA Cascades West, Mt Baker&#39;</span><span class="p">,</span>
                                                              <span class="s1">&#39;WA Cascades West, South&#39;</span><span class="p">])]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_labels</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Note-the-class-imbalance-and-the-test-set-not-having-all-classes.--This-isn't-a-good-set-for-ML-(one-should-use-the-entire-2015-2020-dataset-but-you-need-to-ensure-you-have-all-the-data-from-those-dates-available)">Note the class imbalance and the test set not having all classes.  This isn't a good set for ML (one should use the entire 2015-2020 dataset but you need to ensure you have all the data from those dates available)<a class="anchor-link" href="#Note-the-class-imbalance-and-the-test-set-not-having-all-classes.--This-isn't-a-good-set-for-ML-(one-should-use-the-entire-2015-2020-dataset-but-you-need-to-ensure-you-have-all-the-data-from-those-dates-available)"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_labels</span><span class="p">[</span><span class="s1">&#39;Day1DangerAboveTreeline&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test_labels</span><span class="p">[</span><span class="s1">&#39;Day1DangerAboveTreeline&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="This-will-generate-local-files-sampling-from-the-datasets-(parameters-can-specify-exactly-the-amount-of-data-to-store)-in-the-ML-folder-which-can-be-used-for-the-next-ML-process">This will generate local files sampling from the datasets (parameters can specify exactly the amount of data to store) in the ML folder which can be used for the next ML process<a class="anchor-link" href="#This-will-generate-local-files-sampling-from-the-datasets-(parameters-can-specify-exactly-the-amount-of-data-to-store)-in-the-ML-folder-which-can-be-used-for-the-next-ML-process"> </a></h3><p>Modifying the parameters so you don't run out of memory is important as its designed to append to the on disk files so as to stay within memory contraits: num_train_rows_per_file maxes out at around 50000 on my 48gb local machine.  If you want more data than then then use num_train_files parameter which will create multiple files num_train_rows_per_file and will append them in to one file at the end of the process.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> train_labels_remaining, test_labels_remaining = pml.generate_train_test_local(train_labels, test_labels, num_train_rows_per_file=1000, num_test_rows_per_file=500, num_variables=978)
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="6.TimeseriesAI">6.TimeseriesAI<a class="anchor-link" href="#6.TimeseriesAI"> </a></h3><h4 id="Demonstrate-using-the-data-as-the-input-to-a-deep-learning-training-process">Demonstrate using the data as the input to a deep learning training process<a class="anchor-link" href="#Demonstrate-using-the-data-as-the-input-to-a-deep-learning-training-process"> </a></h4><p>Now that our data is in the right format we can try and do some machine learning on it.  The 4.TimeseriesAi notebook in the ML folder is only to demonstrate the process to do this as today the results are a proof of concept and not sophisticated at all.  This area has only had minimal investment to date and is where focus is now being applied.  The current issue is overfitting and that will need to be addresssed before both exapnding the dataset size or training for additional epochs.</p>
<p>The Notebook 4.TimeseriesAI leverages the Timeseries Deep Learning library <a href="https://github.com/timeseriesAI/tsai">https://github.com/timeseriesAI/tsai</a> based on FastAI <a href="https://github.com/fastai/fastai">https://github.com/fastai/fastai</a> and it realitvely straightforward to understand especially if you are familiar with FastAI.  As progress is made here this notbook will be updated to reflect the current state.</p>
<p>This notebook also depends on a different conda environment in the <em>Environments</em> folder.  Create and activate the environment from the timeseriesai.yml file to use this notebook.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Citations">Citations<a class="anchor-link" href="#Citations"> </a></h2><p>National Centers for Environmental Prediction/National Weather Service/NOAA/U.S. Department of Commerce. 2015, updated daily. NCEP GFS 0.25 Degree Global Forecast Grids Historical Archive. Research Data Archive at the National Center for Atmospheric Research, Computational and Information Systems Laboratory. <a href="https://doi.org/10.5065/D65D8PWK">https://doi.org/10.5065/D65D8PWK</a>. Accessed April, 2020</p>

</div>
</div>
</div>
</div>
 

